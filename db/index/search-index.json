[{"title":"Features of agile","indexable_content":"Here are some of the common features of the agile development methods we’ve used at GDS. Sprints are the rhythmic foundation of an agile process that uses Scrum. They usually last 1-2 weeks. Factors in deciding the length of your sprints include: Teams commonly start with a Sprint 0, which is used to get the development and working environments setup, agree some of the design principles (technical, product, interaction, content) and prepare the backlog for the first sprint. Note: There are other agile methodologies that do not rely on sprinting such as Kanban. However it’s common for teams to start with Scrum and sprint. A daily meeting with all of the team. It should take no more than 15 minutes and typically takes less time. It’s best if you do it standing up, in a semi-circle in front of the project wall. This helps keep it short and allows participants to point at story cards on the wall to keep things on topic. The normal format is each person answers the following 3 questions: (It sometimes helps to have an object that you (gently) throw randomly to the a person in the stand-up to signify they should speak next. It keeps people on their toes, the randomness stops it feeling too repetitive and allows the last person that speaks to choose the person they wish to hear from next. At GDS we use cuddly toys or a piece of fruit. It’s a bit of fun. You don’t have to this, it’s just something to experiment with.) It’s OK to ask a waffler to wind up. If people try to solve issues within the stand up then team members can call time on the conversation and decide to convene a huddle after the stand-up to discuss in a smaller group. I think it’s a sign of a good, constructive stand up if you spin up a couple of ad-hoc huddles afterwards and I believe it shows people are getting something out of the meeting. Sprint planning is done at the start of each sprint. It requires stories to have been written in advance with acceptance criteria. It’s the Product Owners job to read out the stories and explain the acceptance criteria in priority order. It is the team’s’ job to understand the story and acceptance criteria and agree the number of stories they will commit to within each sprint and agree the tasks needed to complete it. There is a good description of this meeting on the Agile Learning Labs website and how it has two parts: the “what we will do?” and the “how we will do it?” With bigger teams this can be a hard meeting to get right. Some people want to dig deep and question every story, others want to keep moving and don’t want to go into detail. Persevere! it’s an important part of the sprint and like Marmite, love or hate it, it’s good for you. Some teams choose to to write or refine their stories at a single, set time within their sprint cycle, others choose to do it over a couple of sessions. It’s up to you, but don’t miss it. It’s vital that you have good stories and there has been constructively discussed with relevant team members, subject matter experts and stakeholders in good time, ahead of sprint planning. Make sure stories are well formed (i.e. don’t skip the “So that…” part because it’s hard), have a good, sensible list of acceptance criteria that supplement your teams’ standing criteria for ‘definition of done’ and are estimated. If stories are too big then split them into smaller stories. They stand more chance of delivering shippable code. There needs to be a moment at the end of the sprint where the team get to demonstrate the work they have delivered. The whole team needs to feel on the hook for delivery. These meetings are important opportunities to iterate the team’s working process. Read detailed guidance about how to run retrospectives.","description":"Here are some of the common features of the agile development methods we’ve used at GDS.","link":"/service-manual/agile/features-of-agile.html"},{"title":"Running retrospectives","indexable_content":"One of the central principles of agile is quick feedback loops - we want to demonstrate something to the user as soon as we can so that we can see how well it suits their needs. Retrospectives are the way we apply this to our own teams and find out what’s working and what isn’t, so we can continuously improve.  A retrospective is a meeting at the end of a sprint where the team get a chance to talk about what went well and badly in that sprint, and take some actions to improve matters. It can also cover a larger scope, e.g. a full project retrospective. A retrospective takes the following form: A retrospective should be a chance for everyone in the team to contribute to improving process/productivity.  All retrospectives must be facilitated. The facilitator’s role is to give everyone a chance to air their concerns and positive feedback while ensuring the meeting remains a structured, productive meeting and doesn’t turn into just a moanfest. Ideally the facilitator will be someone outside the team so the team can all contribute, but this is not essential. The facilitator needs to: It is helpful to have some working agreements. These can be stated if necessary, for example in the first retrospectives a team has held. Working agreements might include:  The outcome of a retrospective should be some actions that can be taken to improve the process - ideally for the next iteration, but certainly as soon as possible. Actions should: Subsequent retrospectives should follow up on actions to make sure they have been done. If they are consistently not getting done, there may be too many.  This is a possible template for a sample retrospective for a team of 8-10, covering a two-week sprint. Each of the activities is timeboxed, and it is up to the facilitator to make sure that this is adhered to. For this scope and number of people, 90 minutes is a reasonable length of time. It is advisable to build in about 10% “shuffle time” to move between activities to ensure it doesn’t overrun. Explaining the scope, and if necessary, purpose. If the team don’t know each other and/or are shy, you could include brief introductions. Make sure they have been completed. If not, find out why not. Do they still need to be done? If so, why have they not been? Set a new deadline if necessary, but it’s not ideal to keep carrying actions over.  Give the team around 10 minutes to write on post-its all the things that went well in the previous two weeks. If anonymity is important to encourage free expression, collect them in and add them to the wall yourself. If not, have the team take turns adding their own to the wall, possibly saying a few words about each. Don’t allow this to develop into a discussion at this point - at the moment we are just gathering data. Group the post-its into common themes and discuss each of the main areas in turn. If there are too many, have the team prioritise, for example by voting with stickers. What should we keep doing? Why did those things go well and what can we learn? Are there any actions we can draw out? Give the team around 15 minutes to write post-its for anything that went badly. Again, group the post-its and prioritise if necessary, and discuss the main areas. Can we work out why these things went badly? Can we work out what we need to do to improve matters, or prevent specific things happening again? Can we draw out specific actions that someone here can take that will help?  Spend some time looking at the actions we have identified, assigning them to people present and setting realistic deadlines for completion. Total: 80 mins with 10% shuffle time. It is OK to finish early if people have said what they need to. It is not OK to overrun - if there is too much to say, have the team prioritise the top areas for discussion and/or book more time for the next retrospective. By having a retrospective regularly, we can make small improvements often, ideally before problems start to fester, and we can identify working practices which make us more efficient, productive or at least happier. Agile development practices are to help us work better, and the retrospective allows us to fine-tune the process and our environment to our needs. The Agile Retrospective Wiki has some very useful resources, including plans for different types of retrospectives. This is a very useful book Agile Retrospectives: Making Good Teams Great. Retrospective plan photo by Anna Shipman. Voting with stickers photo by Pete Herlihy. All other photographs by Paul Downey.","description":"One of the central principles of agile is quick feedback loops - we want to demonstrate something to the user as soon as we can so that we can see how well it suits their needs. Retrospectives are the way we apply this to our own teams and find out what’s working and what isn’t, so we can continuously improve.","link":"/service-manual/agile/running-retrospectives.html"},{"title":"What agile looks like","indexable_content":"Agile is a liberating way of working.  It does not preclude the use of existing skills and knowledge. But it does require teams, users and stakeholders to adopt new ways of working together.   This short guide lists a few of the behaviours common to agile projects that support successful delivery and learning.   Prioritise features for them over everyone else – including your big, scary stakeholders, and seek their feedback early and often.  Really listen to them.  Even when they tell you things you don’t want to hear or disagree with.  If possible, use data from real people using your product to influence the direction of the project. Your focus on the user should be relentless.  Iterate often. Build something focused on the next most valuable user need and show it to them; listen to their feedback and improve it. Keep doing this until you have something so useful that they would not be without it.  It perhaps sounds like over-simplifying the complexity of software development and project management, but at its heart this is what agile development is all about:  “What do you want next Friday?” The process of delivering incremental, production-ready software allows a team to deliver value to their users and stakeholders regularly.  It shortens the feedback loops that might otherwise have been longer using a waterfall methodology.  An iterative delivery cycle also forces the team to think about what the most important features are to deliver next and focuses the mind on useable software. At the end of each delivery cycle, or sprint, teams should run a retrospective to review ‘what worked, what could be improved’ in the next sprint.   The software and the team continue to learn through delivery and iterate and improve throughout the project.  Small teams of between five to ten people are often more productive and predictable than larger teams. Forget man-days and think about the team as the unit of delivery.   A good team includes members with all of the skills necessary to successfully deliver software. A fully-functioning team has three key roles embedded into them, usually full-time: Product Manager - responsible for delivering return on investment, usually by creating products that users love.  The team delivers the Product Manager’s vision. Delivery Manager (a.k.a. Scrum master or Project Manager) - is the agile expert that is responsible for removing blockers (things slowing a team down).  They also usually act as a facilitator at team get togethers. Team member - Self organising, multi-disciplinary team that delivers prioritised user stories. Responsible for estimation. You help each other and work together toward delivering your sprint goals.  It’s common to encourage team members to pair. It sounds counter-intuitive to have two people work on one thing, but this is not so.  Working together closely produces better software solutions, promotes better quality controls and spreads knowledge across the team. A good team can estimate their output, or velocity, very effectively and consistently.  This allows for much more accurate planning.  Releasing little pieces of code often improves quality and visibility and reduces cost to market, but using agile techniques does not guarantee success. You can still fail!  What agile methodologies do allow you to do is to spot problems earlier and resolve them.   Here’s a few examples of how: Don’t be afraid to fail or experiment.  Learn to fail, and create a culture that learns from failure.  It’s a myth that you don’t plan on agile projects.  The freedom of agile projects does not come free: you have to plan.  You just plan differently and continuously. Agile planning is based as much as possible on solid, historical data, not speculation. The plan must continuously demonstrate its accuracy: nobody on an agile project will take it for granted that the plan is workable. Typically teams plan together, usually on at least two levels: These plans are usually reviewed after every sprint and adjusted based on “the weather yesterday”, new facts and requirements that will inevitably be uncovered along the way.  Teams new to agile should be wary of these familiar situations and reactions to doing things differently.  They have a bad smell about them and undermine your project and its chances of success. This is by no means an exhaustive list, but these are most common things to watch out for.","description":"Agile is a liberating way of working.  It does not preclude the use of existing skills and knowledge. But it does require teams, users and stakeholders to adopt new ways of working together.  ","link":"/service-manual/agile/what-agile-looks-like.html"},{"title":"Writing user stories","indexable_content":"User stories are an essential part of the agile toolkit, a way of organising work into manageable chunks that each deliver concrete value, which may be then discussed and prioritised independently of each other. User stories only work in the context of an agile team with embedded user representatives.  A user story is represented by a story card with a title and a few lines of text. Story cards can be virtual as well as actual cards. On a large product/service you’re likely to keep your stories in a digital format, and then turn them into physical cards as part of sprint planning. Story cards follow a standard structure – title, actor, narrative and goal. They do not capture every detail. Rather, they are a promise of further discussion at the appropriate time. Building useful software systems is hard. How can we make sure we’re solving the right problem? Developers love solving problems and playing around with technology, so it can be tempting to dive in to solutions before really understanding the problem.  Agile methodologies emphasise an outside-in approach – what outcome is your service user trying to achieve? If we dive into the solution without a good understanding of our users, we risk solving the wrong problem, or coming up with solutions which don’t really work for our users in the real world. That’s why the most important part of a user story is the GOAL.  You can also use this goal to help you decide whether the story is “done” or delivered. i.e. Does the work that’s been done meet the goal?  As a service manager writing stories with your development team, always start by thinking about and discussing your users’ goals. Why do they want to use your service? What are they trying to achieve? What need has motivated them to seek out your service? In what context do they use it? At home? At work? On a mobile phone? Whilst caring for a child? How often? Suzanne and James Robertson have excellent advice on this in Mastering the Requirements Process (reference 1). Being specific about the ACTOR can help you to break down interactions into logical chunks. Sometimes the actor will be a user of your service. Sometimes the actor will be an administrator, technician or manager within your organisation. Hopefully you already have a good understanding of your users from preliminary project work or existing research. If not, make sure you take the time to develop that understanding. Finally the narrative should serve as a reminder of the main flow of interaction which needs to be addressed. However the story card does not need to spell out every detail. Agile teams prefer face-to-face conversation over detailed documentation. Face-to-face is faster and more accurate than written documentation and allows developers to build up a detailed mental model of the user goals, workflows, constraints and all of the myriad issues which must be taken into account when building a software system.  The story card is just a placeholder, a promise to have a conversation when the time is right. We can use the story card and some brief initial conversations to estimate a story and fit it into an agile backlog. When development work actually starts the users or user proxy can be consulted to fill out the details. The user story is the sum of all of these conversations, sketches and whiteboard diagrams, not just the card. These conversations need never be written down or archived, instead they are translated directly into automated tests and working code. Using user stories in this way allows us to avoid ‘analysis paralysis’, the painful condition of trying to speculatively detail some far-future goal. User stories are dependent on regular face-to-face communication between developers and user representatives. Your service manager and other user representatives need to be available to developers every day, and have sufficient time to think through and answer questions. Don’t underestimate how time-consuming this work can be! Stories can come from many places, but the most common sources include, See Chapter 4 of User Stories Applied.","description":"User stories are an essential part of the agile toolkit, a way of organising work into manageable chunks that each deliver concrete value, which may be then discussed and prioritised independently of each other. User stories only work in the context of an agile team with embedded user representatives.","link":"/service-manual/agile/writing-user-stories.html"},{"title":"Accessibility","indexable_content":"The services we provide are for the benefit of all citizens of the United Kingdom. No user should be excluded on the basis of disability. To do so would breach the Equality Act 2010. As a starting point, your service should aim to meet Level AA of the Web Content Accessibility Guidelines (WCAG) 2.0. Your service should be tested by disabled people, older people, and people who use assistive technologies. You should aim to do this at least twice as your service is developed. Find out more about how to conduct accessibility testing We work hard to make our sites and services as accessible and usable as we can for everyone who needs to use them. The GOV.UK website does not include a separate accessibility statement, however, as we are not comfortable with a statement that draws a distinction between accessibility and any other aspect of best practice development. This blogpost by GDS accessibility expert Léonie Watson explains in more detail why we took this decision. Your service should be usable by recent versions of Jaws, NVDA, VoiceOver, Window Eyes and Supernova screen readers, ZoomText, MAGic and basic operating system screen magnifiers, as well as speech recognition software including Dragon Naturally Speaking, and native operating system speech packages. If you want to read more about the accessibility testing we’ve carried out while building GOV.UK, this blog post by GDS Accessibility Lead Joshua Marshall will tell you more.","description":"The services we provide are for the benefit of all citizens of the United Kingdom. No user should be excluded on the basis of disability. To do so would breach the Equality Act 2010.","link":"/service-manual/design-and-content/accessibility.html"},{"title":"Browsers and devices","indexable_content":"Services should be universally accessible, regardless of how the user is choosing to access them. Due to the large range of browsers, devices and resolutions of access routes, it is to be expected that the user’s experience of a service will vary as the technical capabilities on browsers and devices vary. You must verify that your service works across a representative range of these devices and browsers, and makes accomodations for creating good experiences in all of them. These are the browsers we recommend testing on when developing your service.  This list strongly recommends testing on a range of browsers created within the last 3 years that cover the largest representation of the user base. This list is based upon usage statistics for GOV.UK.  It allows for a 95% coverage of all browsers used (the remaining browsers are individually insignificant). Browsers not listed may still work well, and it should be noted that this is not a list that intends to suggest that these are the only browsers the service will work on - this is simply a benchmark for testing against to ensure that the service will likely work for as many users as possible alongside appropriate cost-effectiveness and development overhead. Services should ensure there is a obvious way for users to report problems they may find, and additional testing and adjustments should be made upon receiving such a report. Note: An exception is made for IE6, as this is still in large-scale use in government departments. Two distinct levels of support are given and denoted next to each browser as C or F and are defined as: Where “latest version” is listed, it means the latest stable version plus one version back, as these browsers regularly self-update. Digital by default services must take into consideration the limitations of the browsers people use to access them. One important idea for achieving this is progressive enhancement. This recognises that different bits of technology have different capabilities. Whilst everybody gets access to core functionality, those using more sophisticated technology get an enhanced experience. Progressive enhancement is also important in delivering a consistent experience to people using mobile devices or those who may have limited bandwidth. Because mobile traffic now accounts for 13% of all internet use in the UK and around 20% of traffic to GOV.UK, this mode of access is not an optional extra to consider. Where we might previously have developed separate mobile and desktop versions of a service, or bought bespoke apps, design should now be done with one website in mind. This should be done using a responsive design approach. This means websites adapt to suit the dimension of the screen being used to view it. Don’t try to build services for every possible combination of operating system and browser. Avoid the temptation of designing for the obvious without first researching your users. Every service has an audience and you should investigate yours to see whether it has particular characteristics that you need to be aware of. Do you have existing data for the browsers and devices that your audience has been using already? If so, analyse it to see if you can identify any patterns in usage, or any combinations of: This data may sometimes support the case for deprioritising certain development work: although most of GOV.UK is designed to work across all screen sizes, the Trade Tariff team chose not to tailor their tool to the smaller screen as it is largely used by office workers between 9 and 5. Equally, if your audience is likely to include those from within the public sector there may be higher use of older, more limited browsers. Channel shift means you must also consider your potential future audience. It is anticipated that operating system, browser and device data from GOV.UK will be published as part of the GDS performance platform and this will provide a valuable insight into the audience for government services. Before launch we noted a marked difference between the existing non-government and government audiences so you should also investigate the data provided by NetMarketShare and GlobalStats who can provide UK and global trends. It is important to distinguish between those browsers and operating systems whose popularity is either increasing or holding steady and those for which the opposite is true. Although Internet Explorer versions 6 and 7 have only been used by a minority (almost 5% of the total visits to GOV.UK since launch) this still accounts for a significant number of individuals who government services must take into account. However, over time this will change. So it is important to set thresholds for abandoning support and for adopting new and emergent platforms. The iPad Mini, Kindle Fire, Windows 8 and Internet Explorer 10 highlight this dilemma - recently launched products might not appear in any data but it is likely that they will eventually enjoy widespread use. Decisions about compatibility can not be something you specify at the start of the project and then forget about. Transformed digital services need to reflect and adapt to the broader internet context of their users on an ongoing basis. James Weiner writes about the decisions made about browser support for the Beta of GOV.UK (January 2012) Ben Welby discusses the operating systems, browsers and devices supported for the launch of GOV.UK (October 2012). Tom Byers explores the practical ways in which GOV.UK has been designed for different devices (November 2012). Dafydd Vaughan with an update on browser usage on GOV.UK post-launch. The Guardian introduce their use of responsive design (October 2012). Helpful summary of progressive enhancement (October 2008).","description":"Services should be universally accessible, regardless of how the user is choosing to access them.","link":"/service-manual/design-and-content/browsers-and-devices.html"},{"title":"Data visualisation","indexable_content":"As we surface more data about government services, we need to make sure that the visualisations of it are easy to understand, visually compelling and drive action. To do that, we need to have a consistent visual grammar, for use both within GDS and across government. This guide sets out four principles of good data presentation, with easy to follow checklists to help you achieve this. For context, we’ve added examples of how the principles have been employed at GDS. The principles and examples found in this guide are likely to evolve over time, as we find new challenges and applications for them. There are many examples of best practice style guides already in place. For example, The Economist has a clearly defined house style that allows its readers to readily identify and understand their visualisations. They publish a new visualisation every day in their Graphic Detail. This guide attempts to build on the best practice from a range of organisations. The GOV.UK Performance Platform helps the Government make decisions based on data, often presented through innovative visualisations (built using D3.js). The example below compares weekly visitors to GOV.UK with the two main websites it replaced.  To effectively tell the story behind the numbers, you need to understand both your audience and the data. Only use visualisations if they make the story clearer. In many cases, a good table or words may communicate better. If there are very few data points (e.g. top rate income tax down 5%, all other rates unchanged), it’s clearer to write a sentence than draw a picture. If every item must be read precisely (to several decimal places) then a table is best. A good table will be clear and uncluttered. The data should be easy to read with the same decimal places or rounding and sorted into a logical order. Don’t use too many different types of font, and make sure your data is referenced. But visualisations often are the right answer and the data is the most important feature. It should tell its own story and it is best to not try to say too much in one go. Keep charts simple, cutting down on unnecessary items and jargon. Explanatory text will be needed in some cases, but it should not simply repeat the story being told in the visualisation. A well written chart title can reinforce the story of the data and reduce the amount of additional text needed. Choosing the right visualisation will help the data tell its own story and give powerful insight. There are many ways of displaying information visually. This guide focuses on charts. A guide to infographics is available here. Most computer programmes come with a range of visualisations. There are also visualisation tools available online and a blog of free ones here. Each chart has its own strength. Below are the core 5 with templates, a google spreadsheet of these is available here  Strengths - comparing items, or a small number of time periods.  Strengths - comparing items, especially if they have long names or many items.  Strengths - comparing over time or between variables for a single item (e.g. site traffic vs. site performance)  Strengths - simple share of total. Use with caution, column or bar charts are often better. Limit to two segments to avoid confusion.  Strengths - relationships between variables where there are many items (e.g. volume vs cost for numerous transactions) There is more help on choosing the right chart here. It is important to not confuse your audience. Choosing the correct visualisation is important and at GDS we reviewed what was being used in the performance dashboard. As the example below shows, Pie charts with many items are not clear. We used a stacked bar chart to better represent the data.   Keep in mind these useful tips when creating your charts. Keeping your visualisation simple will help the data tell its own story. Chartjunk is anything in your visualisation using ink that actively reduces clarity. Avoid: Know your audience so you give the right amount of supporting information. External or non-technical audiences will need more explanation but internal or expert audiences may find this tedious. Do not use the text to simply repeat what is being said by the data. Visualisations should avoid too much data. Only include what is relevant. If the trend is obvious, don’t include a trendline. Sometimes it may be more effective to focus on high value items only (if you are being selective, be open and clear about this). Poor colour choice can change how the data is perceived in a visualisation. For example, red is strongly associated with negative performance so is best avoided for positive/normal figures. Colour blindness makes it difficult for a user to differentiate between data sets. Labelling charts directly and different line styles can help. If your visualisations are likely to be printed it is important the colours work in greyscale as not all users will have high quality printers. The example below from GDS’ senior management dashboard shows how avoiding chart junk and limiting the number of datasets can enhance your visualisation.  The legend accounts for a quarter of display space. The Y axis quotes £ and not £m. The segments are profiles and proportionate for each time period, so the bright colouring adds no extra detail. The mix of bar and line is confusing with so much information in the chart.  The stacked column gave a level of detail which was not needed. This has been rationalised to best suit the audience. Axes have been standardised. The legend has been relocated giving the chart more space. Heavy grid lines and axes have been removed to give a clearer display. “We want transparency to become an absolutely core part of every bit of government business.” - Francis Maude Being open and transparent supports the Open Data white paper agenda. Similarly, our Open Public Services agenda is built on transparency. Sourcing data builds trust and credibility. Providing contact information shows ownership but also helps collaboration and information sharing. When presenting data be aware of axes and scales. Data can be misrepresented by only showing a selection if it is not clear why an extract has been chosen. Consider where the visualisation might be published. For example, if published alongside other visualisations the reader is likely to assume the scales are consistent. This might change how your data is perceived.   For more information on the GDS Performance Platform see this blogpost from Richard Sargeant, GDS’ Director of Performance & Delivery. This chart chooser from Andrew Abela builds on the work of Gene Zelazny’s classic book Saying it with Charts. This interactive tool from Juice Analytics helps guide your chart choice through filters. Brain Suda’s A Practical Guide to Designing with Data provides a comprehensive understanding of how to best engage the audience with your data. A video of Brian Suda presenting on a section of his book at the 2012 DIBI conference is available here. Dona M. Wong’s The Wall Street Journal, Guide to Information Graphics details the do’s and don’ts of presenting data. Edward Tufte’s The Visual Display of Quantitative Information is a seminal work on data visualisations and introduces the concept of chartjunk. A video of Edward Tufte discussing his theories on visual thinking and analytical design is available here. This article from the Peltier Tech blog covers the ten chart design principles. The Flowing Data blog is a source of data visualisation news.","description":"As we surface more data about government services, we need to make sure that the visualisations of it are easy to understand, visually compelling and drive action. To do that, we need to have a consistent visual grammar, for use both within GDS and across government.","link":"/service-manual/design-and-content/data-visualisation.html"},{"title":"How users read","indexable_content":"The style guide is set in best practice and relates to how users read. This is an explanation of some of our guidance and the reasons behind the rules. Users only really read 20-28% of a web page. With services, where users just want to complete the task as quickly as possible, you have added user impatience so you may find users skim words even more. The style guide and how we write give guidance on how to write. This page details why we do it. All of this guidance is based on the learning skills of an average person in the UK, who has English as a first language. You don’t read one word at a time. You bounce around. You anticipate words and fill them in. By the time you are 9 years old, your brain can drop up to 30% of the text and still understand. Your vocabulary will grow but this reading skill stays with you as an adult. You should also be confident in sounding out words and blending sounds. You may not know the word, but you have the skills to be able to learn it. We talk about the reading age being around 9 years old - this is why.  When you learn to read, you start with a mix of upper and lower case but you don’t start understanding uppercase until you are around 6 years old.  At first, you may sound out letters, merge sounds, merge letters, learn the word. Then you stop reading it. At that point, you recognise the shape of the word. This speeds up comprehension and speed of reading. So we don’t want people to read. We want people to recognise the ‘shape’ of the word and understand. It’s a lot faster.  Caps are reputed to be 13-18% harder for users to read. So we try to avoid them. Also, in modern usage it sounds like we are shouting. We are government. We should not be shouting. By the time you are 9, you are building up your ‘common words’. Your primary set is around 5,000 words in your vocabulary; your secondary set is around 10,000 words. These are words you use every day. They are a lot of your ‘plain English’ words. Which is why we should be obsessed with them. These are words so easy to comprehend, you learn to read them quickly and then you stop reading and start recognising.  We explain all unusual terms on GOV.UK. This is because you can understand 6-letter words as easily 2 letter words - if they are in context. Sometimes, you can read a short word faster than a single letter - if the context is correct. Not only are we giving users full information - which is obviously what we are meant to be doing - we are speeding up their reading time. By giving full context and using common words, we are allowing them to understand in the fastest possible way.  In tools and transactions you need to give people context. By giving them information they are expecting, you help them get through it faster.  This is a huge subject, with many different nuances. But one thing we can do is remember that people with some learning disabilities read letter for letter. They don’t bounce around like other users. They also can’t retain the comprehension of the sentence as a whole if it is too long. People with moderate learning disabilities can retain comprehension of 5 to 8 words. Now, we can’t go that far but if we concentrate on common words, we can get to the standard retention figure of around 25 words. Quite frankly, we all get a bit lost if you have a 72 word sentence. Again, why would we make it hard, when it can be so easy and we don’t lose the usability?  Our audience is potentially anyone living in the UK. We need to be able to communicate in a way that most users understand. Government can’t afford to be elitist and use language only those with a very good education can understand. We need to open up our information and services to everyone. That means using common words and working with natural reading behaviour. Nielsen: For more detail on why 20-28% of text is read.","description":"The style guide is set in best practice and relates to how users read. This is an explanation of some of our guidance and the reasons behind the rules.","link":"/service-manual/design-and-content/how-users-read.html"},{"title":"Buttons","indexable_content":"Buttons should be used to signify actions that the user can perform. Here’s how to create buttons in the GOV.UK style, using the GOV.UK button Sass mixin. The button mixin styles can be applied to links, inputs and button tags, like this:      Button tag     Link tag      The default button colour is $green, but different colours can be assigned. You need a good reason to do this though.       Primary action      Secondary action      Warning action    The button text colour automatically switches from light to dark, depending on the background colour. Use the ‘disabled’ attribute or class, depending on which kind of element you’re styling.      Button tag     Link tag      Buttons will inherit the font size of their parent elements. Use the standard paragraph text size wherever possible. More complex interfaces may occasionally require smaller buttons.      Primary action      Secondary action      Warning action         Primary action      Secondary action      Warning action         Next step         Next step         Delete account         Get started       on the HMRC website    Do use: Don’t use: Sometimes you want a single button to perform more than one action. For example, ‘Save and quit’. It’s worth trying to avoid this situation but if you can’t, use common sense. If one of the actions is obvious or not important to know, don’t mention it. For example, if a button saves the current state and moves the user to the next screen, don’t use ‘Save and next’, just use ‘Next’, because users will assume the former. When in doubt, test with real users.","description":"Buttons should be used to signify actions that the user can perform. Here’s how to create buttons in the GOV.UK style, using the GOV.UK button Sass mixin.","link":"/service-manual/design-and-content/resources/buttons.html"},{"title":"CAPTCHA","indexable_content":"CAPTCHA stands for “Completely Automated Public Turing test to tell Computers and Humans Apart”. These are usually images of jumbled up text that a user needs to decipher and enter before submitting a form. They are usually used to prevent bots (automated software) from completing a form or accessing a system. CAPTCHAs introduce significant problems to online services: Usability - they put the burden of detecting bots on the user rather than the system. As CAPTCHAs are designed to be hard to read and understand, this makes the service much more difficult to use. Accessibility - they are inaccessible by design. This effectively makes the service unusable by people with certain disabilities. Even CAPTCHAs that provide audio versions do not completely resolve this issue. Additionally, if a 3rd party CAPTCHA service is used, there are further problems to consider: Many of the risks that CAPTCHAs are aimed to mitigate can be addressed in other ways: It’s important to note that even with a CAPTCHA in place bots will still get through due to advances in computer imaging and the use of CAPTCHA farms. A combination of different approaches generally gives the best results. Further reading: In search of the perfect CAPTCHA CAPTCHA and the BBC Ticketmaster ditches CAPTCHA for something simpler","description":"CAPTCHA stands for “Completely Automated Public Turing test to tell Computers and Humans Apart”. These are usually images of jumbled up text that a user needs to decipher and enter before submitting a form. They are usually used to prevent bots (automated software) from completing a form or accessing a system.","link":"/service-manual/design-and-content/resources/captcha.html"},{"title":"Colour palettes","indexable_content":"This is the standard GOV.UK colour palette. We recommend you use the Sass variables where possible in case the colour values are updated. The variables are defined in ‘colours.scss’ in the GOV.UK Front-end Toolkit. $text-colour ($black) $secondary-text-colour ($grey-1) $link-colour (#2e3191) $link-visited-colour (#2e3191) $link-active-colour (#2e8aca) $link-hover-colour (#2e8aca) $border-colour ($grey-2) $panel-colour ($grey-3) $canvas-colour ($grey-4) $highlight-colour ($grey-4) $page-colour ($white) Sass: $purple Hex: #2e358b Sass: $purple-50 Hex: #9799c4 Sass: $purple-25 Hex: #d5d6e7 Sass: $mauve Hex: #6f72af Sass: $mauve-50 Hex: #b7b9d7 Sass: $mauve-25 Hex: #e2e2ef Sass: $fuschia Hex: #912b88 Sass: $fuschia-50 Hex: #c994c3 Sass: $fuschia-25 Hex: #e9d4e6 Sass: $pink Hex: #d53880 Sass: $pink-50 Hex: #eb9bbe Sass: $pink-25 Hex: #f6d7e5 Sass: $baby_pink Hex: #f499be Sass: $baby-pink-50 Hex: #faccdf Sass: $baby-pink-25 Hex: #fdebf2 Sass: $red Hex: #b10e1e Sass: $red-50 Hex: #d9888c Sass: $red-25 Hex: #efcfd1 Sass: $mellow-red Hex: #df3034 Sass: $mellow-red-50 Hex: #ef9998 Sass: $mellow-red-25 Hex: #f9d6d6 Sass: $orange Hex: #f47738 Sass: $orange-50 Hex: #fabb96 Sass: $orange-25 Hex: #fde4d4 Sass: $brown Hex: #b58840 Sass: $brown-50 Hex: #dac39c Sass: $brown-25 Hex: #f0e7d7 Sass: $yellow Hex: #ffbf47 Sass: $yellow-50 Hex: #ffdf94 Sass: $yellow-25 Hex: #fff2d3 Sass: $grass-green Hex: #85994b Sass: $grass-green-50 Hex: #c2cca3 Sass: $grass-green-25 Hex: #e7ebda Sass: $green Hex: #006435 Sass: $green-50 Hex: #7fb299 Sass: $green-25 Hex: #cce0d6 Sass: $turquoise Hex: #28a197 Sass: $turquoise-50 Hex: #95d0cb Sass: $turquoise-25 Hex: #d5ecea Sass: $light-blue Hex: #2b8cc4 Sass: $light-blue-50 Hex: #96c6e2 Sass: $light-blue-25 Hex: #d5e8f3 #0B0C0C, $black #6F777B, $grey-1 #BFC1C3, $grey-2 #DEE0E2, $grey-3 #F8F8F8, $grey-4 #FFFFFF, $white Sass: $hm-government Hex: #0076c0 Sass: $treasury Hex: #af292e Sass: $cabinet-office Hex: #0078ba Sass: $department-for-education Hex: #003a69 Sass: $department-for-transport Hex: #006c56 Sass: $home-office Hex: #9325b2 Sass: $department-of-health Hex: #00ad93 Sass: $ministry-of-justice Hex: #231f20 Sass: $ministry-of-defence Hex: #4d2942 Sass: $foreign-and-commonwealth-office Hex: #003e74 Sass: $department-for-communities-and-local-government Hex: #00857e Sass: $department-of-energy-climate-change Hex: #009ddb Sass: $department-for-culture-media-sport Hex: #d40072 Sass: $department-for-environment-food-and-rural-affairs Hex: #898700 Sass: $department-for-work-and-pensions Hex: #00beb7 Sass: $department-for-business-innovation-and-skills Hex: #003479 Sass: $department-for-international-development Hex: #002878 Sass: $government-equalities-office Hex: #9325b2 Sass: $attorney-generals-office Hex: #9f1888 Sass: $scotland-office Hex: #002663 Sass: $wales-office Hex: #a33038","description":"This is the standard GOV.UK colour palette. We recommend you use the Sass variables where possible in case the colour values are updated. The variables are defined in ‘colours.scss’ in the GOV.UK Front-end Toolkit.","link":"/service-manual/design-and-content/resources/colour-palettes.html"},{"title":"Forms","indexable_content":"Forms should be styled as per the examples on this page. The GOV.UK forms mixin provides a configurable framework for styling your forms in this way. Use it in your Sass like this: Check out the registration form example to see the different layout options in action. Wrap each control in an element with a class of ‘group’.          Label          Wrap your set of options in an ‘option group’ element.           Job offers          Networking          Business opportunities        Use nested inputs… or use a list… You might occasionally need to arrange form controls in a row. To do this, wrap the controls in an ‘inline group’ element.               Male              Female            Use these to break up forms into logical sections              First name                           Last name              There are times when you might want to treat a set of form controls like they were a single, compound control  (a date-of-birth selector for example). One way to do this is with a nested fieldset. On GOV.UK, when you nest a fieldset inside another, the legend is styled like a label. Note - if you're planning on doing this with left or right aligned form labels you'll need to wrap your legend text in a span. Blame inconsistent and generally poor support for legend positioning in browsers.            Full name                           Day               Day                Month               Month                Year               Year Hints can be placed above or below the relevant control.          Telephone         Include your country code                 Code         The three numbers on the back of the card                 Password         Make it at least six characters long          Use the ‘visuallyhidden’ class to hide labels. You need a really good reason to do this though.          Street                   Street line two                   Town/City                   Postcode          Nest rows of buttons in an ‘action group’ element.             See the seperate page on buttons for detailed guidance on how to style and word them. Summarise any validation errors at the top of your page like this: Each link should jump the user down to the corresponding form control. Add a ‘validation’ class to the control group and insert a ‘validation-message’ element.          Confirm your email address         Confirm email *          To see all the examples above in a single form, check out the registration form example. The framework provides support for top, left or right alignment because there are valid cases for the use of all three. The table below (from a great article on form design in Smashing Magazine) outlines the relative advantages of each approach: When a form is submitted, any validation messages are summarised at the top of the page. The messages link down to the part of the form they relate to. This helps users of assistive technology navigate around the form. The red bar connects the summary to the messages in the form and aids quick scanning of the form for errors.","description":"Forms should be styled as per the examples on this page. The GOV.UK forms mixin provides a configurable framework for styling your forms in this way. Use it in your Sass like this:","link":"/service-manual/design-and-content/resources/forms.html"},{"title":"Grids","indexable_content":"Use this mixin if you need to arrange content in a grid, or split part of a page into columns. You might want to do this for an image gallery, product catalogue or home page layout for example. The mixin accepts two arguments: $columns : The number of columns in the grid, or an array representing the relative width of each column. $min-height : An optional minimum height for grid elements. Useful if your grid elements contain varied amounts of content. The mixin is tag-agnostic, so the elements can be list items, divs, paragraphs etc. Avoid applying border effects to the elements as this will throw out the widths. Instead, style the contents of those elements. At mobile screen sizes the grid elements switch to being full-width. Item 1 Item 2 Item 3 Item 4 Item 5 Item 6 Item 7 Item 8 You can create grids of equally-sized elements by passing in a single value representing the number of elements in a row.  The following examples are for demonstration purposes only, and not ones we’d ever recommend. Item 1 Item 2 Item 1 Item 2 Item 3 Item 4 Item 1 Item 2 Item 3 Item 4 Item 5 Item 1 Item 2 Item 3 Item 4 Item 5 Item 6 Item 7 Item 8 You can create grids of unequally-sized elements by passing in an array representing the relative widths of the elements in a row.  Item 1 Item 2 Item 3 Item 4 Item 1 Item 2 Item 3 Item 4 Item 5 Item 6 Item 1 Item 2 Item 3 Item 4","description":"Use this mixin if you need to arrange content in a grid, or split part of a page into columns. You might want to do this for an image gallery, product catalogue or home page layout for example.","link":"/service-manual/design-and-content/resources/grids.html"},{"title":"Form example - Registration","indexable_content":"This example form incorporates most of the basic form elements and lets you play wth different label alignments. For a detailed breakdown of each element and how to code and style it, see our forms design pattern page. Click the label alignment options in the Sass snippet below to see how they affect the layout.      Label alignment:      top |      left |      right    Please try the following:              Title             Mr.Mrs.MissMs.Dr.Other              1. Enter your first name             First name *                                   2. Enter your last name             Last name *                               Day                 Day                  Month                 Month                  Year                 Year               Male              Female                         Enter email *                                        Confirm email *                           Telephone             Include your country code                         Street                           Street line two                           Town/City                           Postcode                           Write a few short words about yourself                            Make this biography public                         4. Select at least one area of interest              Job offers              Networking              Business opportunities                       ","description":"This example form incorporates most of the basic form elements and lets you play wth different label alignments.\nFor a detailed breakdown of each element and how to code and style it, see our forms design pattern page.","link":"/service-manual/design-and-content/resources/registration-form.html"},{"title":"Sass repositories","indexable_content":"Sass lets us share blocks of code and techniques. GOV.UK has a collection of Sass files which are bundled up into a gem that enable us to quickly build pages that conform to our styles. Within the govuk_frontend_toolkit gem we have a collection of Sass which is designed to enable you to easily add GOV.UK styles. They can be categorised into three different things: The first is the key bit which makes GOV.UK projects look the same. There are a collection of pre-defined font-sizes that we use on GOV.UK. There is a mixin for each one, for example heading-26. These also include a standard amount of whitespace around the text to help with vertical rhythm on the page, spacing things out nicely. The second is a way to develop responsive sites while not giving older browsers a bad experience. That is to still deliver a desktop stylesheet to older versions of IE which don’t understand media queries, and serve the correct media queries to modern devices. As a side effect of this approach we also have a very easy way of writing IE specific CSS in the middle of our stylesheets without using hacks. The third is a way to keep browser specific styles out of our projects. We encapsulate new or non-standardised CSS into mixins. In this way we can easily update all the instances of a new CSS property without having to do a search and replace across all of our projects. For a full list of the different Sass techniques we have available look at the readme on the govuk_frontend_toolkit gem. Some of the typography mixins we have available are: heading-36, heading-24, copy-19. They can be used as such: It is generally advised to write your markup with a mobile and up attitude. That is, add desktop styles to an otherwise narrow screen stylesheet. In this way you only add styles for desktop and don’t reset desktop styles for a mobile device. This can be done as such: There are two types of cross browser technique. There are some which are just for encapsulating vendor prefixes. Then there are some for using different methods to achieve a consistent effect. An example of these are: The border-radius line here is designed to use the different border-radius implementations to create a standard border-radius. The contain-floats however, uses a cross-browser techinque to ensure that the element wraps all of the floated elements within it. It is not a property that normally exists in CSS but is something we often need to do and don’t want to use different techniques everywhere. For more information check out the Readme on the govuk_frontend_toolkit gem","description":"Sass lets us share blocks of code and techniques. GOV.UK has a collection of Sass files which are bundled up into a gem that enable us to quickly build pages that conform to our styles.","link":"/service-manual/design-and-content/resources/sass-repositories.html"},{"title":"Shared asset libraries","indexable_content":"Shared asset libraries are helpful for using the same frontend and branding on multiple services. When building services around patterns and consistency, it’s important to share your frontend assets so that they can be easily reused as required. There’s are two additional benefits to this approach: The templates on GOV.UK are constantly changing as we react to user feedback and evolving best practice, so the best place to find them is on our open source frontend libraries - more explanation of these to follow: The static repository contains our wrapper templates, our basic CSS, and our basic Javascript. Anything added to this repository is built to be used across the entire gov.uk website and is used to provide a consistent look and feel. The frontend repository contains the wrapper templates and views for many of the various pages used across gov.uk. It gives a good indication as to how to structure HTML and assets together. Please see Sass repositories for more information on this repository. The template code contains direct links to CSS and JavaScript assets hosted on the GOV.UK domain. We recommend that you leave these links as is whilst you develop your service (as opposed to linking to your own copies of these files). That way, you’ll always be using the latest version of the assets and when they change you can identify and resolve any conflicts immediately. Once the service is ready for production we’ll need to decide whether you should continue to link to the assets in this way or whether you should now use your own copies of them. Please contact GDS to discuss this at the appropriate time. GOV.UK is continuously being improved, which means that template and asset code changes regularly. All services on GOV.UK are expected to keep their templates and assets up-to-date. How you do this will depend on how you implement the templates and where your service is hosted. Please contact the GDS team to discuss options.","description":"Shared asset libraries are helpful for using the same frontend and\nbranding on multiple services.","link":"/service-manual/design-and-content/resources/shared-asset-libraries.html"},{"title":"Typography","indexable_content":"Services should use clear, easy to read type, with consistent styles and a clear hierarchy of information. GOV.UK uses the typeface New Transport, cut especially for Government use. This typeface is embedded in the GOV.UK CSS and is served to browsers via a WOFF file (or an EOT file for Internet Explorer 8). The font is hinted to display well on all browsers. Older browsers do not receive the typeface. We currently use two weights of New Transport: Light and Bold. Italics should not be used. The number of different type sizes on a page should be kept to the minimum, and only one typeface/font should be used on each website. New Transport is not licenced for use outside of the GOV.UK domain. When your service goes live you’ll be given access to the typeface. If the service uses numbers in columns or tables, you should change these to use the tabular number version of New Transport. This replaces the standard numbers with new versions that have a fixed width. The main noticable difference is a base on the character 1. GDS has used this on the Performance Platform and Trade Tariff. Text must have enough contrast against the background colour to be readable. This should be tested to conform with our Accessibility requirements. Generally we use type in #0B0C0C against a white or light grey background. Links should be blue and underlined - see Colour palettes. Type should be large enough to be easily read. This is generally larger than many current websites: 36px for headlines, 19px for body text. This can be included using default styles in scss from the Frontend Toolkit. These include line height spacing that works across browsers. Why we’ve chosen Transport. Shared asset libraries Sass Repositories Colour palettes Accessibility requirements","description":"Services should use clear, easy to read type, with consistent styles and a clear hierarchy of information.","link":"/service-manual/design-and-content/resources/typography.html"},{"title":"Writing for transactions","indexable_content":"Microcopy is the term given to the short words or phrases used during transactions,  for things like buttons, form labels, help text, alerts and questions. Your first strategy when it comes to help text is to design a service that’s  so intuitive it doesn’t need any. For this reason it helps to stick to interface design conventions where possible. Avoid innovation for it’s own sake - the real innovation is an easy to use government service. Sometimes though, users need a little help. Here are some ways of providing it.          Telephone         Include your country code        Use this to provide examples for unfamiliar information requests or formats. This refers to help that appear on a page when the user interacts with a link, but remains hidden otherwise. It’s useful for delivering important help to some users, without distracting or confusing everyone else. For this reason it’s particularly useful for dealing with edge case user scenarios. This is the nuclear option and should only be considered as a last resort. Popups or lightboxes should ONLY be used for delivering help relating to the concepts or terminology involved in a service or transaction. If you’re using one to explain how to USE a service then you need to go back and make the interface more intuitive. The start point for any GOV.UK transaction should be a page on the GOV.UK domain. Users should not be able to jump to a later page in the service via some other means (e.g. Google). The design of the start page will be determined by the nature of the service and it’s audience. All start pages should meet the following goals: The page should include the name of the service, expressed as an action if possible (‘Renew your passport’, Claim for disability allowance’, ‘Book a driving test’ etc.). If you need to, include a very brief description of the service. People will tend to base their expectations of what a transaction involves on their experience of other digital transactions, for example, online shopping and banking. If your service has features that do not match these expectations then you should inform your users of this as early on as possible, ideally on the start screen. Users should be told about any financial costs or long waiting periods involved. If they’ll be asked to provide relatively obscure information let them know so they can get it ready. If there are specific eligibility requirements for the service let people know. If the eligibility requirements are complex you should consider using the Smart Answer format to help people understand whether or not they are eligible. It’s better to ask a few questions up front (and explain why you’re doing this) than to let people invest time and effort in a transaction only to discover part way through that they’re not eligible to use it. The end point of any transaction should be a page on the GOV.UK domain. These pages should let the user know: Information on designing forms that work on the formsthatwork.com site","description":"Microcopy is the term given to the short words or phrases used during transactions, \nfor things like buttons, form labels, help text, alerts and questions.","link":"/service-manual/design-and-content/resources/writing-for-transactions.html"},{"title":"Working with prototypes","indexable_content":"The best way to understand a product is to try to build it. Prototyping is an essential process to get a feel for the shape and edges of a product, to begin to estimate the work involved and to provide something you can quickly test with real users. This is a vital part of a process often known as “product discovery”: understanding your users and their needs, developing a sense of how you might serve those needs, and estimating the effort involved in building and running a service to do so. We built alpha.gov.uk as a prototype of what would later become the single domain www.gov.uk. It was built quickly without much concern to scalability, resilience, or any of the other considerations of a “real” product, because none of those matter unless the core concept is sound.. That allowed us to get feedback early and also understand some of the trickier concepts we would have to grapple with such as the fuzzy lines between different audiences, the operational processes that would be required, and so on. Prototyping can start on paper with sketches. Hand-drawn sketches of what a service might involve are a good way to begin thinking things through. We encourage everyone to get to running code as quickly as possible. It’s only when you start working in the same medium your users will be using (for online services that’s generally a web browser, but it may also be via an API) that you can really understand the experience you need to provide. The smart answer format for GOV.UK began as a series of paper sketches refined over a week by a small team. That process gave them a good sense of the boundaries of the problem they were trying to solve. As quickly as possible we prototyped the format using HTML and javascript so that it could be experienced in a web browser. That revealed more constraints, such as the fact that users might expect to be able to go back and amend an answer without realising that would change their whole journey through the format. This allowed us to quickly adjust the user interface to be clearer for its users before we started the work of building out the full system. Running code also forces you to think about your integrations with other services and how they might work–do you need to send email? integrate with an existing database? and so on–a prototype will rarely actually include these integrations but having a clear picture of them is vital if you’re going to understand the real effort involved in building and operating the service.","description":"The best way to understand a product is to try to build it. Prototyping is an essential process to get a feel for the shape and edges of a product, to begin to estimate the work involved and to provide something you can quickly test with real users.","link":"/service-manual/design-and-content/working-with-prototypes.html"},{"title":"Writing government services","indexable_content":"This section gives details on naming your service, tone, style, questions and why we write in this way. You may want to read: how users read this will tell you more about user reading behaviour - which will help when designing the content for your service. All services should follow the GOV.UK style guide. The way we talk to users is the same on all GOV.UK products. The only difference is the format. Services should be intuitive and government services should provide an exceptional user experience. You shouldn’t need many words at the top of the page and none of it should be instructions on how to use the service. If you need to manage a user’s expectations, tell them on the pre-transaction page, not on the service page. If you do have to add text, keep your sentences very specific, short, active (keeping the subject and verb close) and in plain English. Users won’t want to read much on a tool or transaction. They will want to click, answer questions and get to the end as quickly as possible. It’s unlikely they will read much at all, so only tell them what you need to. Don’t use terms like ‘my benefits’ etc. This will mean different things to different users. Some will think it is a list of all the benefits they have had, some will think it is an online account to manage benefits and others won’t know what to think. Don’t use puns or clever terms. Keep the service title clear, short and specific. Keep page titles specific to the questions on the page. If the page is just about the user’s address, call it ‘your address’. Don’t call it ‘about you’. However, if you are covering a number of different elements of ‘about you’ on the page - you can. Make sure you need any question you include in your service. Don’t include questions that you think you might need later or for another part of your organisation. Keep the transaction as specific as possible to the task the user is trying to complete. Jargon is used differently by different people, so you will be opening up the language to misinterpretation. We lose trust if we use spin or try to ‘convince’ a user of something. Give them the facts they need and don’t try to encourage. The facts should speak for themselves - if the information is clear enough, the user will make their own decision. You use fewer eye muscles to look down than you do across. That might be why users prefer to scan down a page and not across. By providing clear, specific subheads you are helping the user to scan the page and find the information they are looking for. In tools and transactions, it’s unlikely you will have a lot of information to break up but if you do, use subheads. The only way you will know if your service works, is to test it with users. Then modify and test it again. GOV.UK is to open up government information and services to anyone who is interested or needs to interact with government. As government we need to be inclusive - we can do this through our design and language. By making the design open and easy to read, the code clean and accessible and the words easy to understand, we are opening up government to all. Citizens of the UK can’t go to anyone else for many of our services. Or if they do, to get a better, clearer experience, they usually have to pay for it. If we make the information hard to read or the design so distracting or difficult that the user doesn’t understand what we are trying to say - they could end up doing it incorrectly or not doing it at all. This could lead to a fine. Or in very extreme cases, jail. Good design and words isn’t a desire or an ‘aim’ - it is our responsibility. The terms and conditions of your service must be understandable to the majority of users. GOV.UK’s terms and conditions were written to conform both the the legal terms defined by the Treasury Solicitors Department (TSol) and to the style guide of GOV.UK. These form a template for other services to use, and cover many of the conditions that operational services published as part of GOV.UK will require. If your service needs additional terms or conditions added to this list then these should be written clearly, in a way that most users will be able to understand. In practise, this means unpicking lots of the legal language and giving users concrete examples to help them understand what a specific term or condition means. Adding that term or condition should be done in consultation with GDS. Directgov proved to be a good model for this, as their terms and conditions were clearly written and easy to understand. GOV.UK continued this, with additional pieces of information added as transactions and services were incorporated into the site. How users read Style guide","description":"This section gives details on naming your service, tone, style, questions and why we write in this way.","link":"/service-manual/design-and-content/writing-government-services.html"},{"title":"Accessibility testing","indexable_content":"Accessibility testing is very similar to usability testing, in that it is about ensuring that a product or service is easy to use for its intended audience. That audience includes users who access the service via a range of assistive technologies, such as screen readers, voice recognition software, trackball devices and so on. It’s important to consider a range of disabilities when you are testing any product or service, including those with; Section III of the Disability Discrimination Act (DDA), also states that websites should be accessible to blind and disabled users. The Code of Practice for this section of the DDA was published on 27th May 2002. The elements most relevant to website designs are set out in this blogpost. Most accessibility testing is typically conducted after an accessibility audit has been conducted. Accessibility testing with participants with a range of needs is best conducted in the participants’ own homes. This is because they will often have things set up to suit their individual needs and the whole process is less stressful for them e.g. travel, environment. It is often difficult to conduct accessibility testing early on in the process of service design. The service must be fairly robust in order for it to be evaluated by people using assistive technologies e.g. a screen reader will read out the contents of a web page so the code needs to be well structured.  For accessibility testing to be worth doing, real content needs to be in place rather than ‘dummy text’ if it is to be assessed by those with any cognitive or learning difficulties. Interactive elements such as Calls To Action, hyperlinks, forms etc must be in place if motor skills are being assessed. Full lab-based accessibility testing is not necessary for every project. An accessibility audit may be a more efficient and cost-effective way to review a service, depending on its typical user needs. Accessibility audits are an alternative to standard accessibility testing. An accessibility audit involves an accessibility expert reviewing the site or service, highlighting all accessibility issues and making recommendations for fixing them.  They would typically use assistive software used by disabled web users (e.g. a screen reader) to effectively carry out the audit. See the W3C accessibility guidelines for further information. Accessibility audits are cheaper and quicker than accessibility testing but rely primarily on the expertise of the person conducting them. Disabled participants should be included as part of a wider user testing recruitment process. The numbers will be small, but should aim to capture a range of disabilities and assistive technologies. This is a harder to reach audience so the cost of doing so can be relatively expensive. Recruitment is best conducted through specialist organisations or agencies e.g. AbilityNet, RNIB, etc Additional costs can be incurred if these participants are travelling to your testing location and/or require specialist assistance with carers or travel. Recruitment via an agency can take up to 2 weeks, depending on the target audience. Conducting testing sessions can take between 2-3 days depending on the number of participants. This may vary depending on whether the sessions are lab-based or structured sessions in a ‘home environment’. Analysis and reporting should take up to a week. These estimates are dependent on the project’s scope, and the availability of ‘robust’ testing assets.","description":"Accessibility testing is very similar to usability testing, in that it is about ensuring that a product or service is easy to use for its intended audience. That audience includes users who access the service via a range of assistive technologies, such as screen readers, voice recognition software, trackball devices and so on.","link":"/service-manual/making-software/accessibility-testing.html"},{"title":"Analytics tools","indexable_content":"There are various web analytics tools available to help you measure how people are using your service. You will need to assess how well a particular tool meets your needs before deciding which one to use.  This guidance describes some of the criteria you should consider and reviews some of the main analytics tools against them. When deciding which analytics tool is most appropriate for your service, you should consider the following: For each of these criteria, you should identify which are fulfilled as part of the standard quoted package and what is charged for any additional features. The privacy and security of data is of the utmost importance. Make sure your analytics solution and processes take the following into account: The following table assesses four major analytics tools against the criteria set out above. This information was compiled from Econsultancy’s Web Analytics Buyer’s Guide 2012 and input from GDS and other government departments. There are numerous other vendors in the market place, including open source solutions such as Piwik, which may also be worth considering. Install and configure analytics tools that meet your needs. Where possible, use platforms that enable the data to be piped automatically into other systems.  Using APIs (Application Programming Interfaces) will stop you having to input data manually and allows for aggregation across multiple platforms. You will need to answer the following:  We are using Google Analytics to measure how users interact with GOV.UK pages. We want to know how far down the page they are reading so that we can tweak the content, if necessary. This involved triggering events that can be picked up by Google Analytics at various points down the page: 25%, 50%, 75% and 100%. The funnel visualisation shows the proportion of users who move on to the next page and the number who exit the site (i.e. the drop out rate).  We are developing a new service to allow people to join the Electoral Register online. The service is in alpha at the time of writing but we have already installed Google Analytics and configured it to measure how users flow through the transaction. The funnel visualisations show at each stage the proportion of users proceeding through and the number who exit the process. This enables us to quickly spot where users are experiencing problems and where we might need to test alternative page designs. The Google Analytics Help Centre is a useful resource if you use that particular platform, see for example this guide to setting up goals and funnels. Blog article by Morgan Brown with a good discussion on user flows and conversion funnels.","description":"There are various web analytics tools available to help you measure how people are using your service. You will need to assess how well a particular tool meets your needs before deciding which one to use. ","link":"/service-manual/making-software/analytics-tools.html"},{"title":"APIs","indexable_content":"Martha Lane Fox’s report report called for government to act as a “wholesaler as well as the retail shop front for services and content by mandating the development and opening up of Application Programme Interfaces APIs to third parties.”  This section is a set of guiding principles for exposing a digital service as an API. When building an API there is always a danger of building the wrong thing in the wrong way for the wrong people. This is especially a risk in the absence of a developer community driving the needs behind the API. The simplest way to ensure your API is useful and consumable is to build a website using your own API. Building a Web site leads to considering how to best model content and data in terms of bookmarkable resources, and ensures data is presented in human as well as machine readable representations.  Becoming a consumer of your own APIs not only validates your API, but exposes services on The Web. Consider an API to be a part of a Web site. Provide links to machine-friendly formats from human readable pages, and enable agents to easily construct URLs which link to human-friendly representations of pages. Use standard formats for content, and follow established Web patterns for authentication. Building a service to enjoy mass adoption and support from a wide disparate community of developers and programming environments whilst being able to reach a world-wide audience is difficult. Whilst proprietary and open technologies abound for machine-to-machine communication, none combine the interoperability, reach and ability to scale to compete with The Web. Standards are powerful agreements, and nowhere are agreements more quickly established and adopted than on The Web. Using HTTP and URIs, the core technologies of The Web, together with emergent standards such as JSON and OAuth changes a Website from a retail shop window into a wholesaler, meeting our design principle to Build digital services, not websites. Expose data as a set of resources, offering a clean URL for each thing, and each collection of things. Only use query strings for URLs with unordered parameters, such as options to search pages. Consider creating URIs for different granularity of resources. For example, /members.json could return a list of names, whilst /members.json?detail=full could return detailed information about each member in a list. These principles enable network effects which arise through linking and allow information published beyond the Web, sent in alerts email, SMS, XMPP and other messages to link back to the canonical content, on The Web. Ensure all HTTP GET requests are safe, and actions which change state are conducted using a POST, PUT or DELETE method. Use PUT and DELETE which are commonly blocked by firewalls, intranet proxies, hotel Wifi and mobile operators with caution; always offer a POST alternative. Avoid HTTP methods which are not well defined, such as PATCH. Offer content for each thing as a human-readable HTML, with links to content in alternative machine-readable representations: Where possible, also offer other formats most suited to a specific domain, such as: Include hyperlinks to alternative representations as link headers as well as in content. Consider also encoding meta-data inside HTML content using semantic markup: Microformats. RDFa or schema.org. The representations supported by an API for input will vary depending upon the complexity of the action, but where possible should include application/x-www-form-urlencoded to allow the construction of simple POST forms. Use names for fields, formats and path segments in a URI path consistently across your API. Establish conventions others may easily follow, and anticipate. Where possible, reuse names widely used elsewhere on The Web, as with the Microformats naming policy. Building a Website which exposes the data through links, and services through HTML forms encourages exploration and leads to discovery through hypertext. Provide documentation for your API using examples. Collect how people are using your API, especially link to any open source projects for projects, wrappers and programming language libraries. Provide simple ways to experiment, as with The Guardian API explorer. Be clear in Web pages and other documentation as to the security, availability, rate-limiting, expected responsiveness of the platform and the provenance of data, so consumers may plan their commitment to using your API. Lower the barriers to others using your data; don’t demand registration or API keys for public data. Open data increases the number of people able to use your data and service, and leads to feedback loops where consumers become motivated to resolve issues at source, feeding back issues and correction to your service and the data within. Where content is sensitive, or requires authentication, use encryption (HTTPS) and a standard authentication such as Basic Authentication or OAuth, depending upon the sensitivity of your content. Where a revolutionary change is unavoidable, communicate a breaking change by changing the URL. When changing URIs, continue to honour old consumers, possibly use a redirection. Cool URIs don’t change. Don’t do everything yourself (you can’t). Sometimes the functionality your service needs will be provided by other parts of your organisation, other government departments or by reliable third parties via APIs. Most modern digital services are built on top of a wide range of APIs. This allows each part of the service to focus on its core responsibility rather than constantly reinventing the wheel. When consuming APIs you should be careful to keep the integration with your code clean and distinct. This is important to ensure that you can swap between providers or update to new versions of an API without making substantial changes to your core code.  At GDS we encourage the use of adapter code that is entirely focussed on interfacing with the system and mapping code that will provide the linkage between your code’s domain model and the concepts and services provided by the API. You should consider carefully how you intend to test your integration with the service. In day to day development you will want to be able to test your code without making (computationally or potentially financially) costly calls out to third party services so you should come up with a way of providing mock versions of those APIs. For full system tests, however, you will want to test the full flow including the third-party service so an automated mechanism should be built for that. Many of the GOV.UK publishing applications send emails to provide alerts for content designers. When running tests we don’t want to send lots of fake emails so we swap the normal email adapter for one that logs the emails it would have sent. This lets us test our code is doing the right thing without depending on external services. Our “data insights” code involves significant interactions with Google Analytics. It wouldn’t be practical to test this by sending events to google, waiting for them to be processed, and then reviewing the results. Our developers therefore built a mock service that can be run alongside tests and provides a dummy version of google’s api that lets us check the right data is being sent. Our publishing systems make use of a single sign-on service. In most of our tests the interaction with that service are mocked so the applications’ tests can be run in isolation, but we also have a suite of “smoke tests” that run in our preview environment and use dummy accounts to ensure that the full authentication and authorisation flow is working. The Licence Application Tool integrates with a number of third-party payment services. It makes use of test accounts with those services to verify it is able to communicate with them and is sending the right data to complete payments. By depending on a third party API you could very easily be tying your service’s availability to that of the third party. In some cases that may be acceptable, but often you will want to ensure that you have a fallback plan in place. The details of that fallback will vary according to your service. It may be that you will need to offer the user the opportunity to use an alternative service, or queue the action to take place later. That could be an automated queue with software that monitors it and retries transactions, or it could be a manual queue where someone follows up to collect further details. You should be clear with your users about what is happening. If a third party payment provider isn’t available you might queue the transaction to try again later. That will mean you can’t offer users the same guarantee that their payment will be processed correctly and you should tell them so. The API Craft Group is a reasonably active public forum for discussing publishing APIs. The Open Web Application Security Project (OWASP) maintains a large repository of security information applicable to building APIs, a including a REST Security Cheat Sheet.","description":"Martha Lane Fox’s report report called for government to act as a “wholesaler as well as the retail shop front for services and content by mandating the development and opening up of Application Programme Interfaces APIs to third parties.” ","link":"/service-manual/making-software/apis.html"},{"title":"Choosing technology","indexable_content":"At different points in your project you’ll have to choose one technology over others. That might be a programming language, database, operating system or some small tool that helps the development team work more efficiently. The most important consideration is to work on the assumption that most technology choices can change, especially during the early stages of development.  On day one of your project you simply won’t know enough about the domain or the user need to select the right technology. It’s OK to make an educated guess at this stage, as long as everyone understands that is what is happening. Then find time to challenge the selection as you learn more about the problem at hand. Maybe you selected a programming language that you knew would be easier and quicker to prototype in for the early stages of the project, and then moved to another one that was easier to work in large teams with for the final product. Or maybe you started out with an open source product to allow you to get started quickly, before going on to buy a commercial product which provided some required feature (or vice-versa). It’s very easy to immediately jump to a specific product when making technology choices. This tends to be based either on past experience or on fashion.  Try and take a step back and think about the capabilities of the technology you’re after. Are you looking for a relational database? Or a document database? A key/value store? Or maybe a graph database? Argue first about the capabilities, rather than any specific implementation. With the growth of open source many technology products (databases, operating systems, programming languages, development tools, etc.) are freely available. But a large market still exists for commercial software products.  When choosing technology make sure you do consider the total cost, as well as any upfront fees. Try and take into consideration costs for things like staff, support and productivity. Try and involve the whole team in technology choices. That doesn’t mean no-one owns the decision making but that you want the development team to buy in to the technology choices made. Technology preferences vary, and technology choice can divide opinion.  All things being equal, picking technologies that developers and operations staff like will likely result in improved productivity. Technology lock-in happens when previous decisions regarding technology limit future decisions, possibly so that only one real choice exists. For example, if you select a database that only runs on one operating system you have no choice about the operating system you will use. If the costs of that operating system jump you have no simple way of reducing that cost quickly or cheaply. Over time, and after many decisions, you can find yourself in a situation where all your technology decisions are tightly coupled and you are locked-in to one vendor or one way of doing things. This can have unforeseen financial costs (for example an overnight cost increase) but can also limit how quickly you can iterate on your product in the future, for instance if the ideal technology choice isn’t compatible with your current vendor or technology. Aim to have a clear understanding of the cost or implications of moving away from a technology when you commit to it. Choosing technology is important, but it’s probably not quite as important as you think. What is important are the users of that technology and being able to deliver quality at a sustainable pace and suitable cost. When making technology choices, and importantly as you develop your product and constantly reassess your selections, try and make decisions that: Open Source Considerations","description":"At different points in your project you’ll have to choose one technology over others. That might be a programming language, database, operating system or some small tool that helps the development team work more efficiently.","link":"/service-manual/making-software/choosing-technology.html"},{"title":"Testing code","indexable_content":"We use automated testing to ensure that our code does what is intended, to protect against mis-use of that code, and to provide assurance that iterating that code for better design or new features doesn’t break existing behaviour.  We also add manual testing as an extra check where appropriate. Any code written for your service should have a suite of tests operating at two levels: Requires broad tests that run through high-level functionality end-to-end, making sure that the pieces of the system come together to provide the right service.  A developer should be able to describe the steps in any acceptance test to the product/service manager in a way that makes sense to them and matches how they expect the service to be used (or abused!) Focussed on the specific details of the code ensuring that each discrete unit of code does what is expected of it. They allow the developers to verify that complex calculations are performed correctly, to ensure that code handles bad input properly, and that optimisations to the code don’t break its behaviour. We aim to write a first set of tests at the start of working on a feature. An acceptance test that describes the end-to-end behaviour ensures that everyone involved understands the objective of a piece of work, and can demonstrate progress through the story at hand. Unit tests can then be written to understand the implementation of the code. Tests are often described as “happy path” or “sad path”. Happy path tests verify that the system can be used as intended, while sad-path tests verify that it handles errors (whether bad input from a user, a vital API being unavailable, or some other issue) gracefully. We start with happy path tests and a few simple sad path tests and then add more sad path tests as our understanding of the code and its dependencies develops. Tests should also be written whenever a bug is discovered. A test to reproduce the bug should be written before it is fixed, allowing you to verify that the bug has been fixed and ensure that it isn’t reintroduced later. Developers are expected to run tests regularly, especially before sharing new code, they are verified as part of the code review process, and they are also run regularly in a shared continuous integration system to ensure the whole team has a chance to see how they’re performing.","description":"We use automated testing to ensure that our code does what is intended, to protect against mis-use of that code, and to provide assurance that iterating that code for better design or new features doesn’t break existing behaviour. ","link":"/service-manual/making-software/code-testing.html"},{"title":"Configuration management","indexable_content":"Your system is likely to be much larger than a single application, relying on other supporting infrastructure components. Even a simple application probably requires some configuration, to provide database credentials or a web service endpoint for instance.  In order to build robust, scalable and portable systems this configuration data should be well managed. Configuration management tools help with documenting and maintaining the configuration and dependencies of a software system. Although this could be done using hand-made software, it’s common to use existing tools. Three examples of existing open source configuration management tools are CFEngine, Chef and Puppet. One approach to managing configuration is to describe the configuration and the software dependencies in code. This brings with it all the advantages of programming in general, including: Once described in code the infrastructure configuration is executed against the servers, networks and software in question. Moving software systems between providers can be difficult and time-consuming. Even with compatible providers and simpler procurement rules it’s possible to build lockin with simple technical inertia.  Configuration management encourages a deep understanding of the configuration of the system and this can be used to move software easily between providers. A common problem in software systems is seen when code written by a development team works on their machine or a test environment but not on the production environment. A common cause of this is differences in configuration - different versions of software, different types of database or application server. This can be avoided by using the same tools for both development and production environments. Existing approaches to managing configuration are often manual, process heavy, slow and error prone. Ultimately people are bad at carrying out detailed monotonous tasks. And installing and configuring software across tens or hundreds of servers (if done by hand) is definitely monotonous. Even if this could be done to provide everything correctly configured on day 0, over time configuration drifts if not kept in check. One traditional approach to this problem is to make configuration changes hard, thereby limiting the number of them. When trying to build agile and flexible software systems rapid change is needed and manual processes break down. ","description":"Your system is likely to be much larger than a single application, relying on other supporting infrastructure components. Even a simple application probably requires some configuration, to provide database credentials or a web service endpoint for instance. ","link":"/service-manual/making-software/configuration-management.html"},{"title":"Cookies","indexable_content":"This short guide tells you what to keep in mind when including cookies into your services, and how and why we notify users about cookies on GOV.UK. You should minimise the use of cookies throughout services. Store as little information as you require for as short a time as necessary to deliver a good service to users. If your service requires new cookies to be set then you need to ensure that they can be explained simply and clearly, in a way that the majority of users can understand. You will need to notify users whenever a specific action sets a cookie. This will look like this: “sets a cookie” That link will take users to the cookies page, and the appropriate description of the cookie being set. Whenever a user visits GOV.UK for the first time we notify them on our use of cookies using this message: “GOV.UK uses cookies to make the site simpler. Find out more about cookies.” On the cookies page at GOV.UK you can see a breakdown of the kinds of cookie used throughout the site, followed by an explanation of each cookie’s purpose. For example: Some pages on this site provide you with information based on your location. So when you enter your postcode, we save your approximate location to a cookie. This means you don’t have to keep typing your postcode in, and means we’ll always point you towards services that are closest to you e.g. the nearest UK online centre. The GOV.UK cookies policy does not cover 3rd party transactions. Unfortunately, most existing services were built before the updated EU Privacy Directive came into force, so weren’t built with it in mind (hence not having cookie policies). New and redesigned services will be expected to notify users that cookies are being set beforehand, using a ‘sets a cookie’ line of text near the action that triggers the cookie (this text should be linked to the appropriate part of the cookie information page). You can see an example of how that works on GOV.UK here. Where a user can’t be notified before the cookie is set (i.e. if it’s set when the user first visits the service using implied consent), the service should use a banner similar to that on GOV.UK to inform people that this is the case and link them to the further information. This policy is in line with EU Law and was devised following extensive research during the beta of GOV.UK. It follows the The Privacy and Electronic Communications (EC Directive) (Amendment) Regulations 2011 (PECR). You can read more about how we decided this in this blog post by GDS Developer Dafydd Vaughan. The ICO’s latest guidance This blog post by GDS Developer Dafydd Vaughan explains how cookies were used on the beta version of GOV.UK.","description":"This short guide tells you what to keep in mind when including cookies into your services, and how and why we notify users about cookies on GOV.UK.","link":"/service-manual/making-software/cookies.html"},{"title":"Development Environments","indexable_content":"The following describes the general capabilities required of any potential development environment for use by the exemplar projects to: Although this document does not describe the capabilities and characteristics of a production environment, there is a general presumption that any production environment should enable the exemplar project development teams to: The essential capabilities of the development environment without which the development team will not be able to operate. Optional capabilities which would make a marked difference to the production of the services.","description":"The following describes the general capabilities required of any potential development environment for use by the exemplar projects to:","link":"/service-manual/making-software/development-environment.html"},{"title":"Information security","indexable_content":"It goes without saying that security of Government services is incredibly important. The Government Information security community and processes exist to help service managers both meet their obligations to those processes, and more importantly to help build world class services. The assurance and accreditation processes exist to provide a structure for those activities with a shared language to allow risks, mitigations (and opportunities) to be clearly understood by everyone throughout the organisation delivering a service. The important thing to note about building trustworthy and secure systems is that it’s a team game. assurance and accreditation should not be a completely separate strand of work, or seen as a hurdle to be got over (or around). Only by engaging with risk and making decisions based on a range of expert opinion will you end up with the best product. NOTE: The following uses quite a lot of acronyms. Unfortunately these are in common usage and it’s very hard to engage with the existing documentation and processes without speaking the lingo. All the acronyms should be explained before being used. It’s important to understand the different roles involved within the process detailed below. One of the first things you should do on your project is to establish who plays each of these roles. Note that all of them require formal training and specialist skills. Senior Information Risk Owner (SIRO) Generally a senior member of the lead organisation providing the service. Ultimately responsible for the risk profile of the service: have all the risks been identified, are there appropriate mitigations in place so that the risks can be accepted, and so on. Accreditor More hands on than the SIRO, the accreditor or accreditors will work with the project team to help with understanding the process, identify risks and suggest mitigations. CLAS Consultant Part of the formal Accreditation process. Responsible for much of the formal documentation. CESG CESG are the government agency responsible for Information Security. Can provide technical assistance or consultation on project issues. Assurance is the wider set of activities involved in assessing and managing the risks associated with the system under development. Accreditation is a subset of the assurance work, involving a formal and externally verified process similar to ISO27001. All projects, however small, should involve some level of assurance. This may be as simple as documenting the limited risks and proposing to the SIRO that the project does not require a formal accreditation. For anything involving sensitive data or of interest to lots of people an accreditation stage will be required. Business Impact Levels, often shorted to Impact Levels (IL) are a set of numbers used to guide discussions of risk in Government projects. Specifically they are numbers between 0 and 6 for each of the three following criteria: More details about identifying these numbers can be found in this extract from HMG IA Standard No. 1. The role-holders listed above will work with the wider team to bring the appropriate concerns to bear in the process of designing the service. The team as a whole will need to make a range of decisions about topics such as what information needs to be captured, how it is processed, whether it is stored, and so on which will have a direct impact on the assurance/accreditation process. A close working relationship will be essential to ensure that business impact levels and other details are kept up to date as designs evolve and that risk management plays an appropriate role as a constraint in the design process. The Good Practice Guides (GPG) are documents published by CESG on specific topics of interest to various types of projects. These can act as a good starting point when looking to identify risks and put in place mitigations.  Unfortunately many of these documents are Restricted. It is advisable to establish a working relationship with CESG early on in the project to make sure you can access these documents. Examples include: The Risk Management Document Set or RMADS are the result of the formal accreditation work. This is likely a large set of documents, including the Baseline Control Set (BCS), system overview and supporting evidence, presented to the SIRO for sign-off as part of go-live conversations. The IT Health Check (ITHC) forms part of the formal Accreditation. In essence it is a penetration test carried out by a CESG approved supplier (specifically a CHECK certified individual). Read the guide about penetration and vulnerability testing for more details. The assurance and accreditation work described above is not just about getting a project to launch. It also covers the running of the resulting service. New threats may emerge or systems and processes change over time.  Documentation should be kept up-to-date and additional penetration tests organised on a regular or as-needed basis. It is important to start understanding risks and engaging with the assurance and accreditation process as early in a project as possible. The following is a good starting place for milestones to add to a project plan: It’s important to understand the assurance and accreditation processes and tools are all about managing the risk associated with the running service. Security is part of this, but just one part.  Nearly everything brings with it risks: technology choice, staffing, processes, access to restricted documents, data aggregation, etc. It is important to understand those risks and put in place sensible and suitable mitigations. It is unrealistic in most cases to aim for a system with no risks, and ignoring them is a recipe for future exploits.  The aim is a system where the risks are known and the team, working with risk professionals, have made careful decisions about how to deal with them.","description":"It goes without saying that security of Government services is incredibly important. The Government Information security community and processes exist to help service managers both meet their obligations to those processes, and more importantly to help build world class services.","link":"/service-manual/making-software/information-security.html"},{"title":"User accounts and log-ins","indexable_content":"Our advice is that teams do not build login systems. Building a login system is a significant undertaking. While there are numerous open source libraries that make it trivial to add login functionality to your service, the moment you add that feature you’re significantly increasing your user support overhead (people forget how to sign in, lose their passwords, etc), you’re accruing personal data that you will need to constantly review and protect, and you’re adding a relatively complex interaction for users to complete. Many features that are often implemented using login systems can be completed in other (and potentially more useful) ways.  Saving search results, for example, doesn’t require a login but just a way of helping users remember a specific URL. Instead of having them log in you could provide a tool to help send the URL to an email address or instructions on creating a bookmark in their browser. Or perhaps you could just take their email address and let them know if the search results change?  The precise details will vary according to what users need from your service, but if there’s an alternative to a login system that should be preferred. If after careful review and design work there is no option but to build a login system you will need to consider a few questions: If building a service for a small number of clearly identified agents then it is probably safe to proceed. You should ensure that any authentication and authorisation code written for your system is carefully separated from the application in such a way that you can: If you need to build a system for a broad range of citizens and businesses, or you need to do sophisticated matching with other systems in order to build trust in the identity of your users then you should explore the advice published by the ID Assurance team. The ELMS license application system on business link required a login to complete an application. In building a new version of the system for GOV.UK we removed that requirement and usage rates have increased considerably. There is still a login system for approved users in local authorities who need to process those applications.","description":"Our advice is that teams do not build login systems.","link":"/service-manual/making-software/logins.html"},{"title":"Open source","indexable_content":"This section presents architectural practice and considerations for using, publishing and contributing to free and open source software (FOSS). Use open source software in preference to proprietary or closed source alternatives, in particular for operating systems, networking software, Web servers, databases and programming languages. Problems which are rare, or specific to a domain may be best answered by using software as a service, or by installing proprietary software. In such cases, take care to mitigate the risk of lock-in to a single supplier by ensuring open standards are available for interfaces. Where possible use DNS addresses you own for services, and demand open formats for the import and export of your data. For unique needs and common problems which have yet to be solved well elsewhere, develop software by coding in the open and publish under an open source licence (Legal processes/Open standards and licensing). Whenever possible construct software in the form of small components and utilities, re-usable both inside and outside of your organisation. Keep infrastructure code and secrets, including passwords and deployment configuration and scripts, separate and privately from publicly visible source code. A successful open source project will garner contributions from a large number of sources, both inside and outside of a single organisation. Allow developers time to review contributions, and answer issues and discussion raised by others using the software. Larger open source projects often evolve an extension model to enable others to continue to use the service in a variety of often unexpected and possibly undesirable ways whilst keeping the core project coherent under the editorship of a small, trusted group of committers. Provide developers with ready access to open source development tools with which they will be familiar so they may be productive immediately. Ensure developers have the ability to install and experiment with open source software, have environments to easily publish prototype services on The Web, have convenient access to a wide variety of network connected devices for testing Web sites, and have unrestricted access to collaboration tools such as GitHub, Stack Overflow and IRC. Take every opportunity to contribute back to open source projects you use. Contributions may be in the form of source code, patches, bug reports, feature requests, sponsorship of developers and support staff, engaging in community discussion groups, and giving public attribution to projects. Cite the open source code you use, as in the GOV.UK colophon. Free and open source software has a number of architectural benefits over closed source and proprietary alternatives and is the basis of our tenth design principle — Make things open: it makes things better. Coding in the open lowers the barriers to collaboration with others inside and outside of your organisation, increasing the speed of new developers being productive on a project. Modern developers are usually more familiar with open source tools and ways of working than with proprietary products. Freedom at the point of use means open source software may be downloaded and assessed by developers without payment, prior agreement, a need to sign non-disclosure documentation, needing to waiver rights, or enter into aporia agreements on behalf of themselves or their organisation. Using closed source software increases the risk of lock-in to a single supplier. This is a risk, even when building with open standards due to the possibility of inadvertently using propriety features. Access to the source code enables support to be sourced from a number of independent suppliers, enabling better prioritisation of bug-fixes, allowing systems to be maintained outside of vendor product life-cycles, and mitigating the risk of planned obsolescence or abandonment. Keys, passwords and other secrets need to be stored safely and securely away from source code following Kerckhoffs’ principle. Whilst it is possible to publish a project initially built in private, coding in the open means confronting this separation of concerns early during development.  This separation of project code from deployed instances of a project is good development practice, and using open source enables developers to easily fork and experiment with multiple development, and operations to quickly spin-up multiple test and integration environments without encountering limits of licensing. Although Linus’s Law “given enough eyeballs, all bugs are shallow” is contentious, transparent access to the code increases the difficulty of hiding a back door or Trojan horse inside a FOSS product. It is argued a system is most vulnerable after a potential attack is discovered, but before a fix has been deployed. A number of metrics and models attest to the quicker response to security issues in open source products when compared to closed source equivalents. For projects with a high impact levels, particularly with a small number of participating developers, it is advisable to have a private space to discuss security issues and develop a patch rather than risk flagging a vulnerability before a fix has been deployed. Software is expensive to maintain, and can be seen as a liability, yet the value of software increases the more people use it. Publishing as open source increases the likelihood of code-reuse. For example GovSpeak and unicorn herder are small components which developed as a part of GOV.UK, are used by several different organisations, and have received a number of public contributions. Open source encourages re-use and benefits from network effects, and as a result has developed tools, services and processes for coordinating massively distributed development teams spread across multiple organisations. Finally, by lowering the barriers for reuse, building for extensibility, encouraging forking, mutation and experimentation, open source fosters a culture of open innovation, leading to better products and services. The legal obligations for using open source software are outlined the section open standards and licensing.","description":"This section presents architectural practice and considerations for using, publishing and contributing to free and open source software (FOSS).","link":"/service-manual/making-software/open-source.html"},{"title":"Open standards and licensing","indexable_content":"Best practice and considerations for using open standards and using, publishing and contributing to free and open source software (FOSS). Please note, this is for guidance purposes only, and should not be taken as legal advice. Preference should be given to using open standards with the broadest remit over local standards as prescribed in the consultation on open standards As with any software, using open source comes with an obligation to follow the terms of the software licence. FOSS differs from proprietary software in that the source code may be modified, in which case the licence may demand any derived code be republished with attribution, and under the same terms of the original licence, sometimes called copyleft. These obligations may introduce difficulties when mixing open source software with closed source software, particularly when combining GPL software with software developed behind closed doors or using GNU AFFERO licensed software in proprietary services. These risks may be mitigated by developing software in the open. Software should be published as open source, under a permissive free software licence such as BSD or MIT. The following text is the MIT licence used by GDS when publishing software in the open: It is sufficient to include this as a single LICENSE [sic] file, though you may elect to include the licence in  individual source code files, particularly those which may be taken in isolation, such as example code. There are many other advantages to developing software in the open described in more detail in the open source considerations guidance section, and prescribed by the tenth GDS design principle: Make things open: it makes things better.","description":"Best practice and considerations for using open standards and using, publishing and contributing to free and open source software (FOSS).","link":"/service-manual/making-software/open-standards-and-licencing.html"},{"title":"Releasing software","indexable_content":"Constantly improving online services means releasing changes to the underlying software. How often you want to do this will affect how you design and build the applications and presents a number of challenges that this guide hopes to address. It is important to think about how you release changes to a running application as early in the products development as possible. This is because it affects how software is developed and tested and how a product may be supported. Being able to release software on demand is important. 6 monthly or longer release cycles are dangerous. Not only do new features rarely see the light of day but fixing known problems have to fit within a rigid release schedule.  Note that it’s important to make the distinction between releasing regularly and the ability to release all the time. The application should always be in a state where it could be released, that means quick changes can be made when needed. As an example changes to the software running GOV.UK are made on average 5 times per day. In order to do that you have to consider: Although tools, potentially including commercial tools, are required to aid rapid releases the discussions should not start with what tools should be used or procured but with the needs of the service and the product team. In some organisations, people fear releasing new applications or new versions of software. Lots of websites, especially large applications within large traditional organisations, don’t change very often. Many will have fixed release schedules which might mean one release every six months or so. This means bundling up lots of changes into a single release, which is bad in at least two ways: It could be weeks or months before an improvement that only took a few days to finish is actually released for people to use, and the complexity means there are lots of different ways the release can go wrong. The combination of complexity, risk and the infrequent nature of releases makes for a stressful event for all involved. No wonder most people don’t like release day! Releasing software comes with risks, so trying to minimise those risks is prudent. We do that in a number of ways: As well as reducing risk, being able to release early and often also helps products improve quickly, by reducing a potential barrier to quick experiments and rapid iteration. Finally consider the following two measures of a system; mean time between failures and mean time to recovery. A very traditional approach involves focusing completely on reducing the time between any failures happening, by hopefully improving the quality of the overall system. But problems will always happen at some point, so focusing some effort on reducing the time taken to fix problems that do occur can often be much more cost effective as well as improve the overall system uptime.","description":"Constantly improving online services means releasing changes to the underlying software. How often you want to do this will affect how you design and build the applications and presents a number of challenges that this guide hopes to address.","link":"/service-manual/making-software/release-strategies.html"},{"title":"Sandbox and staging servers","indexable_content":"Everyone working on design, development or maintenance of a service should have a clear, easily accessible place to review the latest version of the software. Those working hands-on building the software for the service should be able to run their own reasonable replica of the entire service.  There should be a clear staging environment where changes are reviewed and tested in the context of the entire end-to-end service before they are deployed. Everyone working on a service should be able to see progress and understand their work in its full context. That means a shared environment where any team member can see the current state of the service and where stories can be signed off, and individual sandboxes where more experimental or early stage work can take place and be reviewed. In addition to those environments it can be helpful to have a separate staging environment where final quality assurance and testing can take place before changes are deployed to the live/production environment. This should be identical to the production environment so it can be used effectively for performance testing. For those working on GOV.UK we use the Vagrant tool to provide all developers with a development environment configured similarly to the production environment.  We then have a preview environment that is updated by our Continuous Integration system whenever tests have passed on a change. There is then a staging environment for review of specific changes before they go to the production environment. It is updated and reviewed as part of the release process.","description":"Everyone working on design, development or maintenance of a service should have a clear, easily accessible place to review the latest version of the software. Those working hands-on building the software for the service should be able to run their own reasonable replica of the entire service. ","link":"/service-manual/making-software/sandbox-and-staging-servers.html"},{"title":"Standalone apps","indexable_content":"At the Oct 2012 Digital Leaders’ meeting, the position was clarified: native apps could not be developed without Cabinet Office approval. The Nov 2012 Digital Strategy says: “Stand-alone mobile apps will only be considered once the core web service works well on mobile devices, and if specifically agreed with the Cabinet Office” Government’s position is that native & hybrid apps currently rarely justified. Ensure your service meets the Digital by Default Service Standard and it will work well on mobile devices. Make your data & API available for re-use and you will stimulate market if there is real demand for native apps. Confusion is understandable. So-called “Apps” come in several very, very different flavours. Not as expensive to maintain as a native app, can access device functions and be persistent but still requires a new parallel version of your web service, and multiple versions to be developed for each device. If there is a market for native or hybrid apps, why should the government monopolise it? There is a vibrant market of 3rd party native app developers using government data & APIs. Government’s position is that native & hybrid apps currently rarely justified. Ensure your service meets the Digital by Default Service Standard and it will work well on mobile devices. Make your data & API available for re-use and you will stimulate market if there is real demand for native apps. We are backing open standards (HTML5) rather than risking proliferation of parallel versions of services as devices proliferate. And while people spend as much time using apps as using mobile web the vast majority of app use is for gaming & social networking. For “utility” needs, such as those met by government services, the mobile web is preferred to native apps But what about exceptions ie those times when only an app will do?  We recognise that there’ll be a few exceptions, to help you determine whether your case is likely to be considered an exception, consider the following:  NOTE: If these conditions are not in place, it is unlikely you’ll obtain approval for your app proposal. If you believe there are compelling reasons why these conditions have not been met, please be prepared to provide them.  What’s the user need? Please provide evidence for this. Are you sure there are no 3rd party native or hybrid apps that meet this user need? If ‘yes’ and condition 2 has been met, please provide your thoughts on why no 3rd party has developed a native/hybrid app. Is this user need of sufficient importance to (your users) justify the lifetime cost of your proposed app? If yes, how have you determined this? You might want to review articles within the service manual such as, Know Your Users and Writing User Stories.  Is there evidence of demand for this app amongst your target users?  If ‘yes’, please provide supporting  evidence.  Given the sheer number of apps out there is there evidence to suggest that this app will actually be downloaded? If ‘yes’, please provide supporting evidence e.g. examples of similar apps that have proven popular with your target audience and evidence of their popularity. Is there evidence of platform penetration amongst your users to justify building an app for the platform you’re proposing to do this for? If ‘yes’, please provide supporting evidence e.g. analytics data that shows proportion of visitors to your content/service that currently access it using relevant devices","description":"At the Oct 2012 Digital Leaders’ meeting, the position was clarified: native apps could not be developed without Cabinet Office approval.","link":"/service-manual/making-software/standalone-apps.html"},{"title":"Testing in an agile environment","indexable_content":"Fundamentally, the basics of any testing approach still apply in the Agile world. However, the focus of testing can be quite different.   It is important to recognize why we are testing in the first place, and that is to build the best quality system we can, that does what the customer requires, at a cost that everyone agrees we can afford  (cost being money, business change, risk etc.).  Too often, the focus of testing is to validate what has been produced and that alone, when in actuality it should be more about the following 7 concepts: You cannot test quality at the end, which is the mistake many teams make. Expend the vast majority of your effort building quality in at the start instead.  When you write user stories, ensure that you define acceptance criteria against which you can test. Once past that point, testing should primarily be a verification of what we already know and understand to be true, there should be no surprises in the latter stages. Service quality isn’t just a testing issue. The quality of a system is defined by the people who create it. If there is a problem in the quality of the system being produced, then it should be evident to everyone involved, and every person on the project should be taking action to increase quality and fix issues. Agile is reliant on fast feedback loops, so that we can actually be agile and change when we need to change. Testing should be about giving that fast feedback, at the time when it is useful. Agile test techniques (e.g. Behaviour driven development, Acceptance test driven development) have their place and we may well use them, but they are not the primary focus of the test approach. By this we mean that testing is built with reuse in mind. It takes a lot of effort to do testing correctly, we don’t want it to be a throwaway exercise that has to start from scratch each time there is a new release, or a new project. Automated tests need to be written with the same care and rigour as production code. While testing is necessary and valuable to the programme as a whole, any time it takes for that code to go live once written is essentially wasted time. Testing should be a tool that is used to get the fastest possible confirmation that it is as expected, or that it isn’t and needs to be reworked. Testing doesn’t always need to be exhaustive at every level, it needs to be applicable to the situation at hand. It should be the job of the team to agree on the necessary testing at each level, based on the appetite for risk of the product owner and the likelihood of risk in the application. Everybody in the programme needs to understand and agree the approach to testing, and everybody needs to understand where they fit in and what they are required to do. Testing, done well, will inform the the best way forward and get the best “bang for buck” in terms of effort expended in various functional or non-functional areas.  It will help make the tough decisions, and drive the development effort based on the risk of each choice of story.  It will help the prioritisation based on the understanding of the complexity of the system. The most noticeable difference with testing in an Agile world is that the majority of your  test effort will be focussed on automated tests.  These tests run in Continuous Integration (C.I.) which means that they form part of your code base and every time you make a change to your code, your tests are automatically run. This gives you immediate feedback on the quality of your code and helps prevent bugs being found at a later stage when they are expensive and complicated to resolve. Read the guidance about testing code.   Exploratory testing is a term commonly used to describe unscripted manual testing whereby the tester uses his or her knowledge, experience and intuition to navigate through the software and identify bugs. A scripted test can only ever test a predetermined outcome. Exploratory testing aims to find and test the less obvious outcomes. A good way to think about it is that automated tests prevent bugs whereas exploratory tests find them.   Exploratory testing is normally time-boxed and has a specific purpose - e.g. ‘I will spend x hours exploring y and z aspects of the system (though my explorations may take me elsewhere and it may take more or less time, I’ll use my judgement as I go)’.  The term does not imply that the tester has not prepared for the testing. They will have given the testing some detailed planning in advance, thinking for instance about the specific aspects they want to explore, and any data or other system set-up that they will need.   Automation may still play a part - not to run the tests themselves but, for instance, to set up the data or to get a set of transactions into predetermined states. In a team where you have a one or more dedicated ‘quality analysts’ or ‘testers’ this type of testing will normally be part of their role. In a developer-only team time will need to be put aside for them the developers themselves to do this type of testing. As a developer has been deeply involved in writing the code, it is sometime difficult for them to step far enough enough from the system to see paths through the system that they hadn’t previously envisaged. To help with this it’s ideal if they can be assigned to exploratory testing for a full day to allow the appropriate amount of context switching. It is also preferable if they are exploring parts of the system that they have been less involved in developing. When a manual test uncovers a defect, it is important to always add an automated test to catch it going forward and hence prevent any reoccurrence. Read Cem Kaner on exploratory testing Read the guidance about load & performance testing Read the guidance about penetration testing Read the guidance about accessibility testing Crowd sourced testing is a good way of speeding up your manual testing and/or achieving better coverage.   There are external companies who provide this as a service, but at GDS we do it internally. We simply put out a call for as many volunteers as possible across the organisation to put aside a few hours on a particular day for testing. We then give them some guidance on what we need testing and a central place for them to log bugs and hey presto! Sometimes we incentivise people by creating a ‘leader board’ showing who has tested the most things.   Examples of where we have used this effectively include for pre-release testing of additional devices/browsers and for the detailed checking of hundreds of pieces of content on GOV.UK back to old content on DirectGov and BusinessLink. Don’t forget, don’t just test the product itself - test your ideas.   For information on how to do this read the guidance about user research.","description":"Fundamentally, the basics of any testing approach still apply in the Agile world. However, the focus of testing can be quite different.  ","link":"/service-manual/making-software/testing-in-agile.html"},{"title":"Version control","indexable_content":"All software development projects must use a version control system. Version control allows you to track changes to code over time, meaning that you can quickly step back to an earlier version where necessary and you can annotate your changes with explanatory details to help future developers understand the process. Version control will also provide tools to audit who has made changes to the code and what has changed. Those updating the code should make small, discrete ‘commits’ of changes that are grouped according to their intention. They should be committed with a clear message explaining what the intention of the change was and (where appropriate) providing links to any supporting information such as development stories, bug reports, or third-party documentation. At GDS we prefer to use distributed version control systems, specifically Git, which is the highest profile option. This means that everyone involved in the process has a full copy of the code and of its history. This makes it easier for developers to create ‘branches’ in their code to explore new features or approaches without treading on the toes of those working on different aspects of the service. It also provides extra resilience; if the network is unavailable the developers can continue to work and make small incremental commits, merging their changes back with everyone else’s at a later date. It’s a good idea to also use version control for other aspects of your work, not just code. We use the same version control tools to manage this document as we do our code, and the Government Digital Strategy was also produced that way.","description":"All software development projects must use a version control system. Version control allows you to track changes to code over time, meaning that you can quickly step back to an earlier version where necessary and you can annotate your changes with explanatory details to help future developers understand the process. Version control will also provide tools to audit who has made changes to the code and what has changed.","link":"/service-manual/making-software/version-control.html"},{"title":"Completion rate","indexable_content":"The completion rate measures the proportion of people who start a transaction and are able to complete it. A transaction is a self-contained process that the service manager has defined in relation to the service. Typically this will be completing an entire transaction from end to end but where a transaction can be completed only partly online, it may be completing a discrete process within the transaction: booking an appointment or completing a part of an application, for example. When users are unable to complete a digital transaction it can lead to avoidable contact through other channels. This in turn leads to low levels of digital take-up and customer satisfaction, and a higher cost per transaction. Measuring end-to-end completion rates helps identify whether users have problems completing a transaction; subsequent analysis of drop-out rates at each step of a transaction can then pinpoint the specific processes that users fail to complete. The end-to-end completion rate can be calculated as the number of completed transactions divided by the number of started transactions; it is expressed as a percentage.  Users should be told clearly what the outcome of the transaction will be, who can use it, how long it is likely to take, and what they will need to complete it (eg a reference number or credit card) before the transaction has started.  They will also need to be provided with the eligibility criteria and the costs to complete the transaction. This will help to reduce dropouts later in the transaction. Typically this information will be given on a single page. In some cases, there may be a set of pages that checks a user’s eligibility based on information they provide. It should not be possible to bypass start pages via links or search engine results. Users who try to access another transaction page directly should be referred back to the start page, unless they are resuming a previously saved transaction. Transactions will begin and end on GOV.UK to allow GDS to monitor completion rates.  A transaction is considered to have started only when the user proceeds from the start page. A user then has to reach the end page before the transaction is counted as complete.  Data on the number of started and completed transactions will be shared with service owners.  Service managers will be responsible for measuring and monitoring drop-out rates within transactions which are not hosted on GOV.UK. You should analyse this data and use it to improve your service. You should build your service with unique URLs for each step / page. This will make your service much easier to measure. Some services allow users to save a transaction mid flow and to resume it another time. Ideally it should be possible to match saved transactions with resumed transactions so that, for the purposes of completion rate, they are treated as one continuous process. Service managers could consider applying a nominal time limit to saved transactions after which, if they haven’t been resumed, they are classed as failed. Alternatively, saved transactions could be set to expire after a given length of time.                           Some services have complex flows and there may be several points at which a transaction can be successfully completed. These will need to be defined by service managers and should point to appropriate end pages on GOV.UK so they count towards the completed transaction total. Some transactional services have parts which are digital and others which are non-digital. For example, when granting a lasting power of attorney, users can start and finish the transaction online, but are required to print, sign and post a form in the middle of the process.  In these situations the second online part of the transaction should be treated as a resumption of the first part. Where it is not possible to match the two, the discrete digital parts of the service should be treated as separate tasks, with the measure completion rate measured for each.  Offline parts of the process can still be measured but this is likely to be done through qualitative feedback (e.g from surveys, diary studies, focus groups). In order to successfully measure your service completion rate we recommend follow the guidance below: Discovery Alpha Beta Live Completion rate should be measured continuously. Once the service is live this will be done on GOV.UK. Before launch completion rates can be measured with usability testing. This should be iteratively tested with at least five people.  This should identify over 85% of usability problems. Users will be given a pre-determined set of tasks that reflect what needs to be done to use the service. These tasks will include all aspects of using the service that apply, such as registering, applying, submitting, verifying, amending and unsubscribing. If users are having difficulty completing tasks, further development should be followed by further rounds of testing. Digital analytics will be the primary method for measuring task completion rates post launch. Please note that this relies on extra configuration in the analytics tool. It will not be available by default. The focus of the service team’s activities will be to continually improve this by monitoring where users are dropping out of the transaction process and testing out new designs for those pages. End to end completion rates will be piped automatically from GOV.UK’s digital analytics into the Performance Platform and will be publicly available from the point of launch. Further usability should also be carried out once a service have gone live to measure use of the service and identify any issues and improvements that can be made. ","description":"The completion rate measures the proportion of people who start a transaction and are able to complete it.","link":"/service-manual/measurement/completionrate.html"},{"title":"Measuring cost per transaction","indexable_content":"Cost per transaction is a measure of the total cost to the government of providing each completed transaction. Cost per transaction is an important measure of a service’s efficiency. As services become more efficient, the cost per transaction will fall. The difference in the transaction costs through each available channel can also be a useful measure. Understanding cost per transaction for each channel will help you to accurately forecast savings and build a strong business case for channel shift. The average cost per transaction is calculated as the total cost of providing the service divided by the total number of completed transactions. The total cost includes all fixed and variable costs of the transaction through a given channel, including overheads. It does not include start up costs. Where resources (eg call centres) are shared with other services, costs should be apportioned. For example, if half of all calls received relate to a specific service, then 50% of the call centre costs should be apportioned to that service. The cost per digital transaction is the total cost of providing the digital service divided by the total number of transactions completed digitally, including assisted digital transactions. Where processes and costs are common to more than one channel (e.g. processing wet signatures for passports, or printing driving licences), they should be apportioned. For example, if half of all transactions are completed digitally, then 50% of the common costs should be apportioned to the digital channel. The full cost of the transaction should include: The following should be excluded: Having agreed a goal for your digital take-up, the following formula will provide a starting point for discussions with GDS on your overall cost per transaction. CPT=A+((B-A)/(1-X))*(Y-X) e.g. if the current average CPT is £2 and digital CPT is 50p, and if current digital take-up is 40% and the target is 80%, the target for average CPT would be £1. GDS can calculate this for you based on current figures. Cost per transaction is measured on a quarterly basis. It should cover the last twelve months to eliminate seasonal fluctuations. Cost per transaction should be measured for the existing service - if there is one - to create a baseline against which future cost per transaction can be compared. When the digital service is exposed to real users (whether in alpha or beta) it should be included in the reported cost per transaction. This worked example sets out the DVLA methodology used to calculate unit costs, and shows how this has generated a historic and future profile for vehicle licensing transactions and unit costs. The timeline runs from the inception of the electronic licensing channel as this provides a useful example of a multi-channel service. The first consideration should be the purpose and use of the cost. An internal recharging model will be more concerned with the direct costs of the service, whereas an external cost recovery model will need to consider overhead apportionment and other forms of contribution. Once the purpose of the model has been established, appropriate expense heads need to be identified. Consideration needs to be given to the cost type, i.e. whether they are fixed, semi-fixed or variable in nature. Fixed cost elements (as the latter worked example will show) will have the result of reducing unit costs as volumes increase. Purely variable costs are reliant on volume so there is a linear relationship between a pure variable unit cost and the volume of the transaction. In many cases, the unit cost is made up of a combination of fixed and variable elements. In the example below the fixed cost elements include expense heads relating to the development of the digital channel; support and maintenance; and salaries (although they are more appropriately classified as semi-fixed). The variable costs that feed into the model include consumables costs, postage and those relating to the intermediary service – i.e. the contracted price per transaction. In establishing the unit costs of the licensing service via the various channels all relevant costs (as outlined above) are collated to give overall totals and these overall totals are then divided by the actual, or where applicable, the forecast transaction volumes. Detailed assumptions are maintained that underpin each of the expense heads, their relevance to the transaction and any adjustments that have been made. Version control is key in order to provide an audit trail. DVLA Licensing transactions since the inception of electronic channels (EVL). There are currently three channels through which the user interacts with the DVLA to license a vehicle – the intermediary Post Office; the Local Services Network; and via the Electronic Vehicle Licensing channel. Prior to 2004 all licensing activity was processed through the Post Office (DVLA intermediary) with a low and static number of complex transactions via DVLA Local Services Network. The overall volume of licensing transactions is itself fairly stable with perhaps a 2% increase in total volume every 5 years. Following the introduction of the electronic channel there has been a steady increase to date in usage which is forecast to continue beyond 2013/14. The corresponding effect of this is a decrease in post.","description":"Cost per transaction is a measure of the total cost to the government of providing each completed transaction.","link":"/service-manual/measurement/costpertransaction.html"},{"title":"Digital take-up","indexable_content":"Digital take up is the proportion of people completing your transaction online, including Assisted Digital users. A digital transaction is one where the primary interaction between the user and the service has been through a digital user interface. Where it is possible to complete only part of a transaction online,  such transactions should still be classified as digital. Digital take-up is a long term strategic measure of how well digital by default service is working. You will monitor this on a monthly basis to track that it is on course with the desired performance trajectory. Digital take up is calculated taking the number of completed digital transactions in a calendar month divided by the total number of transactions in the same month, expressed as a percentage. The total number of transactions will be made up of all non digital transactions including call centre transactions and paper based transactions. This data will need to be recorded and collected to ensure your digital take up figure is accurate. The Digital Landscape research found that 82% of adults in the UK are online. It therefore seems sensible to aim for something in that region. However, about a third of those online have never accessed government information or services online so this will involve a significant channel shift for most services. The extent to which people are online varies by age, gender and socio-economic status. The digital landscape research has a useful breakdown that should act as a guide. Digital take-up is a long term strategic measure and you should be looking to achieve the target level within five years of launch. Some users will never use the digital service but will still have to have access to the same level of service as those using the digital service. This is called assisted digital and departments will need to determine the appropriate mix of channels to support these users. For more information on assisted digital please refer to this section of the the manual.","description":"Digital take up is the proportion of people completing your transaction online, including Assisted Digital users.","link":"/service-manual/measurement/digital-takeup.html"},{"title":"Performance framework","indexable_content":"This guide aims to empower service managers to design and improve transactional public services. It is based around seven principles, with easy-to-follow checklists to help you get started. In order to improve the performance of a service, you have to analyse and identify the information different audiences will need to assess that performance. There is typically a hierarchy of interested users, all of whom will have different metrics for success. Technical staff might have operational and optimisation concerns, while senior management and ministers will have more strategic and comparative concerns.These needs will be reflected in the organisation’s business objectives. Develop simple, actionable and collectable metrics based on your understanding of user needs. Identify where that information will come from and how frequently it will be needed. Key Performance Indicators (KPIs) will be few in number - typically around five - and are top-line indicators of how well a service is performing. For almost all transactional services, these are likely to include the volume of transactions, the cost per transaction and success (or conversion) rate - the proportion of users attempting to use a transactional service that successfully complete the task. There will be many other, more granular metrics that will be useful for different audiences. It’s good practice to record every event generated by the system even if it’s not currently of interest. Don’t obsess over what to measure: measure everything (where it is cost-effective to do so). This maximises the flexibility to come back later and revise the chosen metrics and tailor data visualisations to different audiences. This information could come from a variety of sources, for example Oracle and Excel for CRM and finance databases, Google Analytics for web testing and optimisation tools, or Nagios and Pingdom for system monitoring. Install and configure reporting platforms that meet your needs. Where possible, use platforms that enable the data to be piped automatically into other systems. Using APIs (Application Programming Interfaces) will stop you having to input data manually and allows for aggregation across multiple platforms. There are a number of open source products available as well as paid alternatives. If you are using a third party supplier, ensure that you have access to the raw data behind the measures specified in the contract and make sure data is not thrown away after a short period of time. Establish a ‘baseline’ based on current performance trends by channel, against which changes to the service will be judged. This will help you pinpoint the effect of your initiatives, and identify what worked. It is good practice to look at performance trends over time, rather than take a snapshot at a particular point in time. Peaks and dips in performance are then measured relative to this base (or trend) line which helps to identify the effect of communications or design initiatives. It also reveals seasonal variations in performance. Benchmarking against other services can also provide a useful context for judging performance. By comparing to other similar services (e.g. requesting a license or reporting information) you know whether the service is significantly better or worse than expected. A complete list of the government’s transactional services is available on GOV.UK. You need to know who your users are to be able to determine whether they are likely to need assisted digital to help them use the digital service. The proportion of users needing assisted digital will vary based on the users of the particular service. For example, a higher proportion of people applying for incapacity benefits are more likely to need assistance with digital services than those needing to pay their road tax. Collect and aggregate performance information from multiple sources and across multiple channels. Make sure you understand what this will mean in terms of system requirements. Combining data often reveals useful insights, for example into service efficiency (eg cost per transaction and total cost to serve) or proportional usage by channel (eg percentage digital uptake vs post, phone etc). Be aware though that combining data from different data sources can lead to huge storage requirements: large, data-driven organisations now talk about storage in terms of petabytes, the equivalent of one million gigabytes. However, there are cost-effective solutions to this problem already in place. For example, the Hadoop software framework was developed to enable collection, aggregation and querying of such huge data stores, and is based on work done by Google and Yahoo to develop search engines. Communicate performance information to your users through the appropriate dashboards, reports and alerts. For the 4 KPIs set out in the digital strategy this will be done through the performance platform. Highlight specific segments that you know users are interested in, and make sure that your visualisations are simple, actionable and contain minimal amounts of chart junk. Typical segments include: * Channel used to access service: through which channel(s) did the user find out about and attempt to use the service? * New vs. repeat visitors: are first time users behaving differently to those who have used the service before? * Geographical region: how popular is the digital service by region and how does that compare with online penetration in general? * Product type: does the user experience vary depending on the type of product or service? * Value: is performance dependent on the monetary value of the service being sought? Dashboards are objective-focused and will help inform decisions, often with the help of real-time data. Reports provide regular, scheduled snapshots of data and tend to require extra context and time to digest. Alerts are used to inform the users about a change or an event, often using attention-grabbing delivery mechanisms. By making your visualisations clearly visible you maximise the likelihood that the information will be acted upon - and services thereby improved. Best practices include: * keeping charts plain: don’t use shading, 3D or other distracting effects * removing clutter: don’t use trend lines, grid lines, unnecessary labelling * not using pie charts: they require more space and are harder to read than bar charts * using text where a chart adds nothing. Test a range of performance improvement initiatives and monitor to see which work well. These can be piloted on a subset of your users to minimise risk. Implement the best performing solutions widely and then repeat this process relentlessly: what you measure will change over the course of a product or project’s lifetime. Any service that meets user needs will include an element of user feedback. This should be monitored and acted upon so as to continually improve the service for users. A range of options are available for improving the overall performance of a service. The following examples are based on the 4 Ps of marketing: * Price: can the price be changed, for example to attract people to the digital channel? * Product: can the user experience be improved (eg from user feedback, user testing, A/B testing, multivariate testing)? * Placement: can the digital service URL be placed on printed materials and voice recordings? * Promotion: can greater use of email and social media be used to promote repeated use of the digital service? Taking an iterative approach to service development increases the pace of improvement and minimises the risk of failure. Don’t wait until the end to do this, it should happen continuously throughout the process. The NAO report [Digital Britain One: Shared infrastructure and services for government online] (http://www.nao.org.uk/publications/1012/digital_britain_one.aspx) identified that there was a lack of information on the costs and associated benefits of digital services. Occam’s Razor is an excellent blog by Avinash Kaushik with loads of useful advice on metrics, KPIs and analytics. See for example this article on how to set good performance indicators. The Google Analytics Help Centre is a useful resource if you use that particular platform, see for example this guide to setting up goals and funnels. Blog article by Morgan Brown with a good discussion on user flows and conversion funnels. This article in The Guardian shows online customer satisfaction scores for retailers. These scores are based on the Customer Satisfaction Index. ###Aggregate data Great article by Mike Loukides on the role of the data scientist. These webinars by Cloudera provide a useful introduction to Hadoop. Total Cost to Serve is a method (PDF, 79k) for calculating the cost of a transaction for both the service provider and the user. HMRC have developed a method for calculating the cost of users time when interacting with government. This is important because some channels may be quicker to use than others. Designing with Data is an excellent book by Brian Suda which helps you to design beautiful and powerful data visualisations. Juice Analytics is a great website with loads of useful resources on how to design and develop useful data visualisations and dashboards. Edward Tufte’s The Visual Display of Quantitative Information is a seminal work on data visualisation and introduces the concept of chartjunk. The Flowing Data blog by Nathan Yau is a useful source of data visualisation news. The D3 Gallery is a stunning collection of data visualisations. Nicely presented overview of some of the tools available for data visualisation. This article in Wired shows how A/B testing was used to good effect in Obama’s election campaign. This article in eConsultancy shows how multivariate testing was used to improve conversion rates at Lovefilm.","description":"This guide aims to empower service managers to design and improve transactional public services. It is based around seven principles, with easy-to-follow checklists to help you get started.","link":"/service-manual/measurement/performanceframework.html"},{"title":"User satisfaction","indexable_content":"Defined as the percentage of people who answered either “very satisfied” nor “satisfied” on a five-point scale in response to the question: Overall, how satisfied were you with this [eg car tax] service today? A good service enables users to complete tasks successfully. A great service is also enjoyable to use. Satisfaction provides a qualitative measure of how satisfying the experience is. Many government transactions are mandatory and therefore not inherently enjoyable - sometimes referred to as grudge transactions - but you should endeavour to make them as pleasant as possible for users, who may be nervous or stressed when interacting with the government. Asking users how satisfied they are with a service can provide a measure of all the elements contributing to the overall user experience such as ease of use, navigation and design. GOV.UK will provide a user satisfaction survey at the end of your transactional service and make this data available. You should measure all user journeys through your transaction to understand drop off points. If a user drops out your should try and collect some qualitative data to explain why they did not complete the transaction. For example: ‘Please tell us why are you unable to complete this transaction.’ Although this survey will not be contribute to the overall measure of user satisfaction it will help you understand service drop of points and find ways to improve transactions. In order to successfully measure the user satisfaction of your service we recommend you follow the guidance below: Discovery Alpha Beta Live An exit survey will be run continuously on your service, and report satisfaction on a monthly basis. You can use this data to improve your service. You should also carry out a more comprehensive user satisfaction survey every six months. You could consider analysing the key factors driving satisfaction with the service. For example, by asking additional questions (e.g. on ease of use, accuracy, look and feel) you can determine which of those factors is most positively contributing to user satisfaction and hence prioritise where to focus ongoing design efforts. Survey design","description":"Defined as the percentage of people who answered either “very satisfied” nor “satisfied” on a five-point scale in response to the question:","link":"/service-manual/measurement/usersatisfaction.html"},{"title":"Helpdesk","indexable_content":"In order to provide high-quality service, you will very likely want a dedicated group of specialists - or current staff dedicating some of their time - to handle user enquiries and to help direct them to the information they want. We’ll use the term “Helpdesk” to refer to them. Your planning will be greatly aided by drawing on the traditional contact centre management and planning skill sets; if you do not have access to staff with these skills, you may have to work closely with other groups who do in a consultancy capacity. This may include the team at GDS, or outside specialists. You’ll need to have some idea of the volume and type of contacts that you will receive for the service you’re supporting. In many cases, you can use historical data for similar services as a baseline.  This element of your planning will also need to take into account the contact channels you intend to support (e.g., email, phone, chat, Twitter, Facebook, surface mail, etc.). Ideally, each of these channels will have a separate contact forecast to aid your planning.  This portion of your planning will also be informed by or will itself drive decisions about the technology you’ll use to route, handle, and store historical data from these contacts. You’ll also want to have at least some idea of the average handling time (AHT) and variance for each type of contact.  If you’re building off of deep historical data, you may be able to model the AHT separately for each kind of contact. Minimally, you should use the best historical data available to calculate an average number of contacts per day that can be handled by a single agent doing a representative mix of contacts. You’ll also need to have an idea of the service level that should be maintained by your Helpdesk.   Generally, service level (“SL” or “SLA”) is expressed as x percent of contacts resolved in y time units. As an example, you might plan to answer 80% of your incoming email contacts in 24 hours.   You’ll need to know the desired service level to plan how many agents you’ll need for your Helpdesk.  Conversely, if your staffing is constrained by other factors, you may use this value as a constant and solve for the best service level that can be achieved at a certain volume level. Once you’ve planned for volume, handling time, and service level, you can finalise your demand forecast for contacts and plan to add staff as necessary to handle these contacts.  The same historical volume you used to create your contact forecast will likely give you an idea of how the contacts will be received throughout the planning period (likely a week). This, in turn, will give you good direction on how to schedule your Helpdesk staff to achieve the service level you’ve planned for. Once your Helpdesk is operational and actually supporting users, you can collect data on all the areas for which you planned. This data can then be used to improve and refine the forecasts you made previously and to create longer-term projections about the performance of your Helpdesk.   You will want to develop at least minimal reporting to allow you to evaluate how the group itself and the individuals in it are performing. Depending on the size of your Helpdesk and the details of your contact mix, this might be very detailed and sophisticated or very simple, but you’ll want to plan for how to measure these elements from the very beginning.  The points above are good starting points to consider as you plan to support new digital products and initiatives. There is a quite extensive body of online information that expands on the points made here, including some free planning tools. However, as with all areas of specialised knowledge, you should ideally access the services of an experienced analyst rather than depending entirely on online sources for guidance.","description":"In order to provide high-quality service, you will very likely want a dedicated group of specialists - or current staff dedicating some of their time - to handle user enquiries and to help direct them to the information they want. We’ll use the term “Helpdesk” to refer to them.","link":"/service-manual/operations/helpdesk.html"},{"title":"Hosting","indexable_content":"The software running your service will need servers to run on. This guide will help you decide how you host your applications and the things to think about if selecting a vendor. The recommended approach is to involve a small cross-functional group of people. They will quickly access different options, shortlist suppliers, interview  and finally make a decision. This group should include people with a knowledge of the available Procurement options and acceptable costs but must include people with a hands-on technical understanding of the service and underlying software. It is important to keep good notes from any interviews or deliberation sessions and to access different suppliers equally. A scoring matrix can help here. It is worth stating that it’s very common to use multiple suppliers. This may be due to them offering different but compatible services or potentially for additional redundancy. This can be technically challenging but for larger projects can provide extra resilliance. There are a number of different approaches taken by suppliers of hosting services, which can make comparing offerings difficult. The following is intended only as a brief introduction. This advice is complicated because many service providers redefine the meanings for marketing reasons. In particular Infrastructure as a service and Platform as a service are marketable at the moment and often used incorrectly. Always look into the details of the services being offered. For particularly large projects with very specific requirements you may decide that purchasing hardware and even running a dedicated data centre is suitable. The costs and timescales involved here are very high and this is unlikely to be the best option in most cases. Many providers offer co-location services which is where you purchase your own hardware to put into a managed data centre. This provides a great deal of flexibility but can introduce lead times and other physical constrains. It also requires a wide range of technical specialist skills.  Lots of service providers have a shared or managed hosting option. This tends to mean renting specific virtual or physical machines for fixed periods of time. Different suppliers other different management services, some just manage the underlying machine while others will support the operating system and even specific applications running on the machines. In the last several years Infrastructure as a service has become a common approach to managing hosting and infrastructure requirements. This tends to involve a capability to rapidly add or remove capacity, often in minutes, and to be billed only for what is used. This provides a great deal of flexibility and the ability to hold costs down but also requires a degree of technical skill to manage well.  Similar to Infrastructure as a service above, Platform as a service offerings tend to allow for quickly adding or removing capacity and fine grained pay on demand pricing. The difference is is that you are abstracted away from the underlying infrastructure completely. The unit here is the running application, not a virtual or physical machine. Using a Platform as a service places a number of constrains on the software architecture but can move the support burden for parts of the stack onto the supplier. Making a decision about your hosting supplier involves weighing up a wide range of different components, including: This is, unfortunately, a technical field with many options. Seemingly similar services can have wildly different architectures or different cost models can result in large differences in total cost of ownership. It is recommended to involve technical colleagues or trusted third parties in any discussions and decision.","description":"The software running your service will need servers to run on. This guide will help you decide how you host your applications and the things to think about if selecting a vendor.","link":"/service-manual/operations/hosting.html"},{"title":"Load and performance testing","indexable_content":"History is littered with countless Government projects which collapsed under load or which worked slowly enough to frustrate users. As a Government service it is important that your systems and applications are performant and can deal with expected (and unexpected) levels of traffic.  This means doing some capacity planning work up front but it also means doing specific load and performance testing.  Capacity planning is the process of determining what amount of infrastructure and software is required to run a live system. Importantly you should also look at how this changes over time. Will traffic or database load increase every month as the site grows, or are some days or months (self assessment deadline or bank holidays) particularly spikey.  The output should help with both estimating ongoing costs as well as setting up realistic load and performance tests. There are several companies that offer products and services around load testing. In many cases working with these can be useful, especially for final testing or testing of mature components.  However it’s important to also have some capability within your development team to do more ad-hoc load and performance testing. This is important to allow for a rapid iterative development style, otherwise scalability or performance issues may be introduced but caught late, when they are much harder to fix. Although related, and tested in similar ways, load testing and performance testing are done for different reasons. It’s worth understanding the subtle difference and making sure you consider both when testing and analysing results. We test sites and applications under realistic load to ensure that, when launched, they work for the people using them. This should involve testing in load in excess of expected traffic in order to simulate certain types of Denial of Service (DoS) attack, including a Distributed Denial of Service (DDoS) attack. Even if a site or application is able to scale out successfully it doesn’t mean it is fast. Site performance is a factor of many things, from the software running the site to the networks, proxies and caches involved in serving traffic over the internet.  Fast sites are generally much more effective to the extent that Google now includes performance in it’s algorithms for determining which sites to feature in search results.","description":"History is littered with countless Government projects which collapsed under load or which worked slowly enough to frustrate users. As a Government service it is important that your systems and applications are performant and can deal with expected (and unexpected) levels of traffic. ","link":"/service-manual/operations/load-and-performance-testing.html"},{"title":"Managing user support","indexable_content":"Once you have created a helpdesk, you will want to figure out how to make the best use of user feedback to improve your service and the user experience. Your ability to act quickly and constructively on user feedback will ultimately depend on the degree to which you can stratify contacts for analysis. Depending on the complexity of your contact types and the sophistication of the systems you use to handle them, this might be almost completely manual (with metadata created for the contact by the staff handling it) or largely automated (with the software you use to aid contact handling adding most of the metadata automatically). Minimally, you’ll want to stratify contacts by channel (phone, email, chat, social media, surface mail, etc.) and by the target group that can act on the feedback. For example, some feedback may be directly dealt with by your organisation, while other types may need to be sent to other departments or groups for their use. In addition to the very basic channel and target stratifications mentioned above, you will likely want to consider the following categories as potentially applicable to your contacts/feedback: Enquiry type Is the contact a:  Requester details You will want to be very thoughtful about data collection of user details, given privacy concerns, but you will almost certainly want to gather minimal detail on the requesters of your contacts. Reply type Is a reply necessary/expected or not? Enquiry status Whether a contact is open, pending some other action, solved, etc. Enquiry category or categories Some internal sectional or functional categories along which you’ll want to be able to stratify–think of separate URLs or sections on a large website as a concrete example. Service category or categories Aspects of the handling processes you use that you should capture for further analysis–support level, priority, internal resolving group, resolving agent(s), day/date/time received, and day/date/time resolved are all examples you’ll likely want to have available for analysis. Root cause category or categories The ultimate reason for the contact; for example, page failing to load, database down, forgotten password, user error, software bug, etc. As with most of these categories, you will tailor these categories based on the specifics of your product and support model. Once you have gathered data for contact stratification, you’ll need to decide how to share it. If all the data can be handled and used directly within your organisation then this step may be easier, however there will likely be different teams that will act on different types of problems, some of them outside of your department. Depending on the nature of your contact handling systems, you may be able to use automation to do most of this routing, and you should plan for that if possible. One very ambitious improvement model that you can use treats each contact from a user as ultimately due to a defect in the service itself, either in your support model for it or in the communications processes you use to handle contacts.  While this perhaps overstates the reality for some kinds of contacts–is a user’s email of thanks and appreciation genuinely a defect?–it is an excellent way to focus critical thinking on how you provide service.  Once the contact-level improvement data has been sent to the right group, you can begin doing analysis for process improvement. You may want to marry your contact level data with cost data to help prioritise which areas to target first, but you may also simply use the number (or percentage) of affected users as a prioritisation mechanism.  In any case, you will want to have access to staff with experience in process analysis and improvement, ideally with relevant contact centre or technology experience. Directly involving the individuals who handle contacts and those who built the service into improvement projects will yield better and faster results.","description":"Once you have created a helpdesk, you will want to figure out how to make the best use of user feedback to improve your service and the user experience.","link":"/service-manual/operations/managing-user-support.html"},{"title":"Monitoring","indexable_content":"Any online application should have some tools dedicated to alerting the people running the service to problems. This might involve low level issues involving the infrastructure underpinning the service to a sudden high rate of user errors. Knowing the current state of your service and infrastructure can help identify problems before they happen, as well as alert you to issues that need immediate attention. The main goals are: Monitoring is not something that should be left to the end, to be tacked on as part of running the final production service. By talking about monitoring, and agreeing an approach, you are more likely to build useful checks as you go along. Writing tests at the same time as writing code is common, monitoring checks can be viewed as tests for the running system. Often monitoring is seen through a very technical lens, so teams may only look at web application performance, available disk space or memory usage. Although these are important it’s also essential to track these alongside more business related metrics.  For example being able to compare page loading tests with failed transactions and application errors can alert you to problems and help identify the cause at the same time. It also grounds conversations about low level problems (disk space, slow performance) in terms of the service performance.  When errors occur they should be recorded and tracked over time. Errors always contain interesting information, potentially about a user problem, attacks in progress, failing systems or just a capacity problem.  It’s important to be able to see errors both at the level of the entire system and related to a particular application or machine. The monitoring system, or rather any dashboards, interactive tools or reports, should be as widely available as possible. They should ideally be useful outside just the group responsible for the day to day operations and systems administration.","description":"Any online application should have some tools dedicated to alerting the people running the service to problems. This might involve low level issues involving the infrastructure underpinning the service to a sudden high rate of user errors.","link":"/service-manual/operations/monitoring.html"},{"title":"Operating a service.gov.uk subdomain","indexable_content":"While the start and end of a user’s journey will be on GOV.UK (ie on gov.uk/transaction-name), the service itself will hosted elsewhere, and will need a different domain name as a result. Note: Where the words MUST, SHOULD, MAY and MUST NOT appear in this document in capital letters they MUST be interpreted as defined in RFC 2119. Every digital service offered by the UK government MUST have a single, well-known place on the Internet where users can go to when they want to use the service.   That well-known place is the relevant start page on www.gov.uk - for instance, the DVLA’s tax disc service is at https://www.gov.uk/tax-disc. Service owners MUST advertise the relevant start page as the starting point for the relevant service. This is what gets printed on literature, email signatures, TV adverts, etc.  The start page URL for a given service will be allocated by GDS based on discussions with the service owner and analysis of user behaviour, search referrals and other relevant data. The transactional part of a service - the dynamically generated pages where users interact with the service - will typically not be hosted on the “www.gov.uk” domain. That means that each service needs its own domain name for the transactional part of the service.  Note: This does not apply to the set of interactive tools on GOV.UK known as “smart answers” which are developed and maintained by GDS in partnership with other government departments. For all new digital government services going live from 1 April 2013 the GDS will create a domain name of the form “servicename.service.gov.uk” (where “servicename” is a plain English description of the service agreed between the relevant dept/agency and GDS). This will introduce consistency across central government domains for digital services and remove the dependency on departmental sub-domains (which are of course vulnerable to machinery of government changes) and the now-retired DirectGov and BusinessLink online brands. The process of obtaining a service.gov.uk sub-domain begins either when the service manager asks a GDS product manager for a start page on GOV.UK (for services already under development at 13 March 2013) or when the service manager asks for a sub-domain to be created via the GOV.UK service desks’s government contact form (for services where development starts after 13 March 2013).  Sub-domains of service.gov.uk SHOULD describe the service (eg lastingpowerofattorney.service.gov.uk) and SHOULD NOT contain the name of the service owning department or agency (eg ministryofmagicwandregistration.service.gov.uk) The service-owning dept/agency will be given delegated authority to manage the domain and its sub-domains, although in some cases this work will be carried out by third party suppliers. This section give some guidance about which sub-domains a service owner should create once they have been given control of servicename.service.gov.uk. Maximum number of visible sub domains The user-facing live service SHOULD usually be operated using at most three user-visible sub domains of servicename.service.gov.uk: You SHOULD NOT create separate domains for APIs unless there’s a really good reason to have a completely separate domain. (Really good reasons are few and far between.) Service managers should notify the GDS technical architects (via your Transformation team contact) if you intend to create user-visible sub domains other than the three listed above. We’re developing some patterns for more unusual system designs as well as for mainstream transactional services, and we’re always up for a discussion about exceptions and edge cases. Usernames and passwords If the service is a private alpha or private beta release then it should be protected by a username and password known only to the development team and the users who are testing the service. If a service, or part of a service, is a public alpha or beta releases then it should be clearly marked as such with a text label on every page (i.e. don’t use an image containing the word alpha or beta) and in every API response. multiple environments It is good practice to have multiple “environments” for the development, testing and live (aka production) versions of any service. (see also http://guidance.digital.cabinet-office.gov.uk/making-software/sandbox-and-staging-servers.html)  Typically, the sub domains used to access a development or testing instance of the service are structured in the same way as the sub domains used in the live version of the service. Therefore, you MAY create other sub domains of servicename.service.gov.uk for use in testing and development, such as www-preview.servicename.service.gov.uk and www-dev.servicename.service,gov.uk. If there is a compelling reason to use a non gov.uk domain for testing and/or development sub-domains that’s also acceptable. Regardless of the domain name used, web-based services on testing and development domains (including APIs) should be protected by a username and password along the same lines as private alpha and beta releases. Cookies used on “www.servicename.service.gov.uk” and “admin.servicename.service.gov.uk” MUST be scoped to the originating domain only. Cookies MUST NOT be scoped to the domain “servicename.service.gov.uk”. Cookies SHOULD NOT be needed on “assets.servicename.service.gov.uk” (they introduce a browser overhead that slows down the response time for users without providing any benefit for the service owner - more on this at http://developer.yahoo.com/performance/rules.html#cookie_free ). Cookies MUST be sent with the Secure attribute. See also “Transport Level Security” below. Many transactions will collect personal information from users. It’s very important that this information can’t be intercepted by malicious third parties as it travels over the Internet.  Therefore, all transactions accessed through service.gov.uk domains (including APIs) MUST only be accessible through secure connections. For web-based services this means HTTPS only (often referred to by the acronyms TLS or SSL, which both refer to the protocol underpinning these secure connections). Services MUST NOT accept HTTP connections under any circumstances. Once a service owner has verified that their HTTPS setup is working fine they SHOULD enable HSTS on the production domains (www, admin and assets). We will provide templates for HSTS headers in due course, but when you first enable HSTS we recommend setting a fairly short TTL (ie a few hours) just in case you make a configuration error. This can be increased to 1 year once the service owner is confident that HSTS is configured correctly. GOV.UK is the place for users to find all government servicess, so it’s important to ensure that users always start on the relevant GOV.UK page, rather than a different or duplicate start page on www.servicename.service.gov.uk.  That means services need to ask search engines not to index pages on their domains, so that the relevant GOV.UK page and the service domain don’t compete with each other in search engine results. This can be achieved by redirecting users to the www.gov.uk start page if they go directly to the service’s domain name, and by asking search engines not to index pages on the service’s domain name. Therefore, every service hosted on a service.gov.uk domain MUST: If you have contracted with a CDN-based DDOS-protection supplier(s) (such as Level 3, Fastly or Akamai) then you SHOULD register the following additional sub domains for use by your supplier(s): www-production.servicename.service.gov.uk, admin-production.servicename.service.gov.uk and assets-production.servicename.service.gov.uk. Your supplier(s) will use these sub-domains to identify the IP address(es) of your service’s www, admin and assets sub-domains.  Detailed configuration advice for origin servers is outside of the scope of this document, but in general it’s worth configuring them to only listen for traffic from a) the DDOS protection provider’s servers and b) from the location(s) where the service itself is being developed and/or managed. At present we advise against allowing DDOS protection suppliers to terminate SSL connections for transactional services carrying personal information, but this behaviour is not prohibited at present. Although SSL termination on the third party network would allow the supplier(s) to carry out additional analysis and potentially extra mitigations against certain types of attack, it would also give the supplier access to all the personal information being submitted to your service. There are obvious downsides to allowing this level of access, especially if the supplier’s network and processes have not been accredited to the same level as the rest of the service. It’s a risk based decision, but if in doubt we suggest a presumption against SSL termination on third party networks. Many suppliers offer IP forwarding DDOS protection, which does not have the same security issues as SSL termination, and is recommended in preference to SSL termination.  If your service requires transaction monitoring (which is not at all the same thing as DDOS protection) you should contact your CESG account manager for advice.  This is interim guidance and will be updated - check back in early May 2013 for an update. Emails to users of your service SHOULD be sent from a human-monitored email address that originates from the domain servicename.service.gov.uk (and not the dept/agency or any other domain name).  You SHOULD enable SPF on the sending domain. You MAY also want to use DKIM on the sending domain; it can provide additional guarantees about message delivery and help recipients to more easily distinguish genuine mail from forgery.","description":"While the start and end of a user’s journey will be on GOV.UK (ie on gov.uk/transaction-name), the service itself will hosted elsewhere, and will need a different domain name as a result.","link":"/service-manual/operations/operating-servicegovuk-subdomains.html"},{"title":"Vulnerability and penetration testing","indexable_content":"Ensuring web based systems and applications are secure requires more than just good design and development. In order to identify vulnerabilities it is often a good idea to involve an idependent body to help find potential security problems before releasing to the public. Sometimes referred to as pen testing, vulnerability and penetration testing is the act of analysing and testing a service for security problems. This is often a specialist activity done via a third party. It’s often a good idea to view security testing as an ongoing activity for any project, not as a final check. Security is important to both a product and technical audience. It’s essential that vulnerability testing reports and the risks they identify are understood by non-technical audiences as well a developer audience.  Security is rarely binary, in that it’s not generally a matter of being secure or not, rather reducing the risks of a wide range of potential issues. CESG are the National Technical Authority for Information Assurance. Based at Cheltenham they provide both standards and advice for information security.  For any sizable project it is wise to engage with them as early as possible. They can provide guidance and expertise on potential problems and help you make sure the right things are tested. Testing for security issues should be done throughout the development of a service, as well as regularly when it’s up and running. Having third parties do this testing is a good way of introducing genuine experts and getting a different view on something. However it’s also important to make sure the team building the software know that security is the responsibility of the team, and not something that is outsourced. Remember that when testing for vulnerability to look at the whole system, not just the software involved.  An obvious example would be physical security (where is the equipment housed and how secure is it?) but a more interesting example is often the interplay between an online system and a call centre.  It may be that by using information available from a call centre you can exploit the software system in some way. For instance, getting a call-centre team to change an email address on record for someone else, and then using a forgotten password facility which relies on the email address being trusted. Although some level of exploratory manual testing is always a good idea when looking for vulnerabilities this is time consuming and expensive. Having some level of automation can often be a good idea.  This may take the form of tests written or tools used specifically to test the security of a feature. For instance Static Analysis or input fuzz testing. The web is a hostile environment, and the nature of Government services means they can be targets for a wide range of different threats; from financially motivated criminals and online activists up to nation states. Even where personal or sensitive information is not at risk the reputation of government can be damaged by even the smallest issues. Web application exploits tend to follow a relatively small number of common patterns, which means automated and manual testing, as well as awareness of these common problems can have a drastic effect on the security of the system. The Government Accreditation processes mandate some form of vulnerability testing, often working with CHECK approved suppliers. This should be viewed as the minimum effort required. ","description":"Ensuring web based systems and applications are secure requires more than just good design and development. In order to identify vulnerabilities it is often a good idea to involve an idependent body to help find potential security problems before releasing to the public.","link":"/service-manual/operations/penetration-testing.html"},{"title":"Alpha","indexable_content":"When designing a digital system it is impossible to predict everything up-front. Each project features many challenges, and in your alpha you will start building solutions for these. You may need to bring more developers and designers into the team, but the whole phase should not last longer than about six to eight weeks. By the end of the phase you should have a clear idea of what’s required to build a beta. The objective is to build a working prototype. This will be used by stakeholders or a closed group of end users to: Following demonstration of your alpha, you may choose to discard the code and start fresh in the beta. If, however, your code is robust enough you may continue to iterate against your prototype. The alpha doesn’t need to be a complete, end-to-end service. You’re looking to demonstrate just enough so users gain some understanding of the service. Think of it as a proof-of-concept. Is the solution appropriate? Is your approach viable? Do you have enough understanding of your users needs to meet them? If not, find out more and make a new prototype. The alpha phase is another relatively short phase. At GDS, we try to limit these to about two months, running in 6-9 week long sprints. This phase involves a relatively small core team, who will be capable of rapidly iterating solutions. It will probably expand and contract in size as different specialisms are required. Previously: Discovery Next: Beta","description":"When designing a digital system it is impossible to predict everything up-front. Each project features many challenges, and in your alpha you will start building solutions for these.","link":"/service-manual/phases/alpha.html"},{"title":"Beta","indexable_content":"You’ve tested your solutions to user needs and built up a clear picture of what it will take to build and operate your service. Now you will build an end-to-end protoype, test it in public and prepare to run it. The objective of this phase is to build a fully working prototype which you test publicly with users. You will continuously improve on the protoype until it is ready to go live, replacing or integrating with any existing services. This is achieved by delivering the user stories in the backlog created in the alpha phase. This is the time to resolve any outstanding technical or process-related challenges, get the service accredited and plan to go live. You will also be resolving technical and process challenges, meeting for the first time many of the technical criteria outlined in the service standard. You should be rapidly releasing updates and improvements into the development environment, and measuring the impact of your changes to the KPIs established in your discovery and alpha phases. The exact duration of your beta will depend on the scope of your project, but an appropriately sized team shouldn’t take more than a few months to deliver a public beta. Following the release of your beta you will spend some time iterating on the service until it is ready to go live. You will now know what size team you need to deliver the service, scoping it in response to the findings of your alpha prototype(s). It will be run by a single, suitably skilled service manager, and will include designers, developers, web operations specialists and performance analysts as appropriate. You will have: Previously: alpha Next: live","description":"You’ve tested your solutions to user needs and built up a clear picture of what it will take to build and operate your service. Now you will build an end-to-end protoype, test it in public and prepare to run it.","link":"/service-manual/phases/beta.html"},{"title":"Discovery","indexable_content":"The discovery phase is your chance to gain an understanding of what the users of the service need, what the business requirements are and what technological or policy related constraints there might be. This is achieved through research, workshops and interviews.  Typical outputs from this phase are a list of user needs, high level wireframes or prototypes and possibly user personas. It’s a very short phase, probably taking no more than a week. The discovery phase will give you a high-level understanding of users needs, what the existing service landscape looks like and a sense of what your initial protoypes will explore. The high level business context will become clear, and you will begin setting targets for your KPIs. You will also get a better understanding of the legacy interfaces and infrastructure you must deal with, and what existing process are in place for replacing or decomissioning these. This information is found through: A small team will be required, consisting of your stakeholders and any core team members that have been identified, including the service manager. The phase should not take longer than a week. At the end of the phase a decision should be made whether to proceed to the alpha phase. You will leave the discovery phase with: Next: alpha","description":"The discovery phase is your chance to gain an understanding of what the users of the service need, what the business requirements are and what technological or policy related constraints there might be. This is achieved through research, workshops and interviews. ","link":"/service-manual/phases/discovery.html"},{"title":"Live","indexable_content":"You’ve been building a service to meet users needs, and after your public beta you have a tested solution that is ready to release. In order to provide a fully resilient service to all end users the service should now meet all security and performance standards. You have configured your analytics to accurately monitor the KPIs identified in the building of your service, and you have planned the transition or integration of any existing services. You have liased with the team governing the Digital by Default Service Standard to ensure that you have met the requirements of new and redesigned services. And, most importanly, you have met the user needs identified in the discovery, alpha and beta phases. This is not the end of the process. The service should now be improved continuously, based on user feedback, analytics and further research. Operational support - both technical and customer-focused - is in place, and you have implemented pro-active monitoring methods. These will help you to: You will repeat the whole process (discovery, alpha, beta and live) for smaller pieces of work as the service continues running. Find something that needs improvement, research solutions, iterate, release. That should be a constant rhythm for the operating team, and done rapidly. You will have identified the roles required to run your service, including the service manager and user support teams, while building the service. As different areas of your service are iterated and improved the team size will expand and contract, accomodating specialists as appropriate. Previously: beta","description":"You’ve been building a service to meet users needs, and after your public beta you have a tested solution that is ready to release.","link":"/service-manual/phases/live.html"},{"title":"search.md","indexable_content":"","description":null,"link":"/service-manual/search.html"},{"title":"Accessibility skills","indexable_content":"Accessibility is everyone’s responsibility. Provide training and resources to help your team build accessibility into everything they do. That includes planning, designing, building and managing. Include at least one person with strong accessibility knowledge in your team. Give them overall responsibility for educating and supporting your team’s accessibility goals. The key skills required by an accessibility-focussed developer are: Accessible, usable products are the heart of everything we do at the GDS. Your service needs to be just as thoughtful to the needs of all possible users so that no one is excluded on the basis of disability. Read the guidance on accessible design","description":"Accessibility is everyone’s responsibility. Provide training and resources to help your team build accessibility into everything they do. That includes planning, designing, building and managing.","link":"/service-manual/the-team/accessibility.html"},{"title":"Delivery manager job description","indexable_content":"NOTE: the following template was used to advertise a delivery manager vacancy at GDS POST TITLE:             Delivery Manager\t\t\t\t\t\t\t\t PAYBAND:                A\t\t\t\t\t\t\t\t UNIT/DIVISION:          Government Digital Service\t\t\t\t\t\t\t\t LOCATION:               London\t\t\t\t\t\t\t\t TYPE OF POSITION:       Permanent\t\t\t\t\t\t\t\t The Government Digital Service (GDS) is at the centre of government, coordinating the digital strategy across all departments into a single, comprehensive and accessible Government website and supporting the government’s aims for a digital by default strategy. The GDS’s aim is to make government easier for the citizen to engage with, to build trust and to put users at the heart of this transformation. It does this by working alongside the Prime Minister’s Office, Cabinet Office and all government departments and their executive agencies – and a whole community of support outside government. Headed by Mike Bracken, Executive Director, The Government Digital Service comprises approximately 180 highly mobile and skilled IT professionals based in Aviation House. They form a number of specialised digital teams, including Digital Engagement, Business Transformation, the Innovation and Delivery Team, Directgov and the Single Domain Team. GDS is an innovative and exciting place to work and is fully engaged with delivering a better service to the citizen, business and to other organisations and with a digital vision for the 21st Century. To be effective within Government Digital Service you will have worked within structured programme and project environments, delivered a range of services through digital / web channels by sourcing and using open source and cloud technologies and have applied agile development methodologies. You will also have supported business change, rationalisation and transformation programmes. The Transformation team within Government Digital Service is responsible for two key areas: 1) The operation of the department’s portfolio and programme management processes. 2) The definition and delivery of the ‘Digital by Default’ mission across government. You will be responsible for driving the successful delivery of projects within the Transformation portfolio, within relevant time and costs constraints and to the appropriate level of quality. You will work closely with the Proposition Directors, Product Managers, in house delivery team and external delivery teams to build user centred digital products. The ability to work under pressure and be able to respond quickly to changing circumstances and to tight timetables is essential. You will need to be confident in dealing with, and influencing, senior officials, as well as producing clear advice on complex issues. FOR INFORMATION REGARDING RECRUITMENT PROCESS, ELIGIBILITY, TERMS & CONDITIONS: cabinet-office.external@dwp.gsi.gov.uk FURTHER INFORMATION REGARDING THESE POSTS: CLOSING DATE: EXPECTED SIFT DATE: EXPECTED INTERVIEW DATES: APPLICATIONS ARE WELCOME FROM PART TIMERS AND JOB SHARERS SALARY SCALE: The main responsibilities of the post are: Essential Essential Essential Essential Desirable Essential Desirable Essential Desirable Essential Desirable Essential","description":"NOTE: the following template was used to advertise a delivery manager vacancy at GDS","link":"/service-manual/the-team/delivery-manager-jd.html"},{"title":"Delivery manager","indexable_content":"The delivery manager sets the team up for successful delivery. Remove obstacles, or blockers to progress, constantly helping the team become more self organising. They enable the work a team does rather than impose how it’s done. Skilled delivery managers remove obstacles, or blockers to progress, constantly helping the team become more self organising. They enable the work a team does rather than impose how it’s done. It’s not about micro managing! Equally important in an agile team – and particularly important to the delivery manager – is ongoing effort to improve products, services or processes. Their role in this is to facilitate project meetings- including daily stand-ups, sprint planning meetings, and retrospectives. They also track progress and produce artefacts for showing this, like burn down/up charts. They must be able to enable the team to produce estimates of how much effort is required to produce features that the Product Manager wants.  Delivery managers need to have: A delivery manager will also need the following skills: Delivery Manager Read articles in the manual of particular interest to delivery managers. Good health check from the Scrum Alliance for delivery managers. A day in the life of a delivery manager - blog","description":"The delivery manager sets the team up for successful delivery. Remove obstacles, or blockers to progress, constantly helping the team become more self organising. They enable the work a team does rather than impose how it’s done.","link":"/service-manual/the-team/delivery-manager.html"},{"title":"Designer job decription","indexable_content":"NOTE: the following template was used to advertise a designer vacancy at GDS POST TITLE:             Designer    PAYBAND:               \tA\t\t\t\t      \t\t\t UNIT/DIVISION:       \tGovernment Digital Service\t\t     LOCATION:              \tLondon\t\t\t     \t\t\t\t\t\t TYPE OF POSTING: \t      To build world class digital services we need to build a world class design team. We’re looking for a mid weight designer to be part of this team. This is the ideal opportunity to gain experience working on large digital products used by millions of people every day. Let’s be clear about the impact of your work, making better digital services has a positive impact on millions of people. The ideal candidate will have a degree in design or similar, have a love of the web and its possibilities, a passion for design and its history and an understanding of how form and function work together. You will proactively seek new learning opportunities and welcome constructive criticism of your work. You’ll: * be good at designing information regardless of what media it’s for * have mastered basic typography and have an interest in learning more * be able to create design concepts that answer a brief and go beyond expectations * have contacts in the wider industry * be comfortable speaking about your work to key stakeholders * be able to work calmly under tight delivery deadlines * be able to plan your day and resources * present at internal show and tell * have experience of web design / interface design. Take 5 minutes to read our Design Principles to see how we think","description":"NOTE: the following template was used to advertise a designer vacancy at GDS","link":"/service-manual/the-team/designer-jd.html"},{"title":"Design skills","indexable_content":"Design of the service is everyone’s responsibility. Those with the title of designer may have a particular focus on one or more specific design disciplines - interaction, graphic – but a good digital service needs needs talented, flexible designers to help build user-centred products. Designers and front-end developers should work together in one team, designing in-browser. This is a better way of working, avoiding silos and ensuring that decisions are made with complete awareness of the implications. As a result, the people you hire should already have worked like this, or at least understand it. When building a team ask to see examples of work and ask the designers to talk you through their contribution. Avoid CVs that emphasise the terms ‘UX’ and ‘creative’. Especially avoid ‘creative directors’. We mean avoid ‘UX’ and ‘creative’ as nouns because we need them as adjectives. Let’s take ‘creative’ first. A CV that emphasised that term would probably be an advertising creative which is not what we’re looking for. Of course we want designers (and developers and policy people) who are creative, but we think there are other ways of demonstrating that than putting the word in your CV a lot. UX is harder, but similar. We strongly believe that UX is the responsibility of the entire team. From how fast the servers are, to how the copy is written, to what the structure of the url is. It’s worth looking at Frances Berriman’s talk on this. Too often, when UX is seen as a distinct discipline it leads to it getting brought in for a week or two at the end of a project and that is not what we want. We can’t build user-centred products with just UX experts. We need a whole organisation focused on user needs which is why our first design principle is start with needs. See an example of a designer job description provided by GDS. Specific guidance for designers working on digital by default services.","description":"Design of the service is everyone’s responsibility. Those with the title of designer may have a particular focus on one or more specific design disciplines - interaction, graphic – but a good digital service needs needs talented, flexible designers to help build user-centred products.","link":"/service-manual/the-team/designer.html"},{"title":"Developer job description","indexable_content":"NOTE: the following template was used to advertise a developer vacancy at GDS POST TITLE:              Developer           PAYBAND:                 A    UNIT/DIVISION:           Government Digital Service                    LOCATION:                London     TYPE OF POSTING:         24 Month Fixed Term Appointment with possibility of extension or permanency. The post is fixed term to cover a finite piece of work.     The Government Digital Service (GDS) is at the centre of Government, coordinating the digital strategy across all departments into a single, comprehensive and accessible Government website and supporting the Government’s aims for a Digital by Default strategy. The GDS’s aim is to make Government easier for the citizen to engage with, to build trust and to put users at the heart of this transformation. It does this by working alongside the Prime Minister’s Office, Cabinet Office and all Government Departments and their Executive Agencies – and a whole community of support outside Government. Headed by Mike Bracken, Executive Director, and comprising of a number of specialised digital teams, including Digital Engagement, the Innovation and Delivery Team, Directgov and the Single Domain Team, the GDS is an innovative and exciting place to work. The whole GDS team is fully engaged with delivering a better service to the citizen, business and to other organisations and with a digital vision for the 21st Century. You will be responsible for designing, creating and improving new and existing products, platforms and transactions across government. Working alongside a multi-disciplinary team of developers, designers, editors, analysts, the main responsibilities of the post are: * designing and building web and mobile products to serve a variety of citizens’ needs * designing and implementing APIs for internal and external use * building up a useful, robust automated test suite to support a Continuous Deployment environment * being involved in the wider web development community, identifying good practices we can adopt and sharing our experiences * solid experience of web application development, ideally in Ruby and Javascript * training and mentoring new developers, and introduce non-programmers to programmers and programming CONTACT DETAILS FOR INFORMATION REGARDING RECRUITMENT PROCESS, ELIGIBILITY, TERMS & CONDITIONS: cabinet-office.external@dwp.gsi.gov.uk FURTHER INFORMATION REGARDING THESE POSTS: CLOSING DATE: EXPECTED SIFT DATE: EXPECTED INTERVIEW DATES: APPLICATIONS ARE WELCOME FROM PART TIMERS AND JOB SHARERS SALARY SCALE: package of up to £73,000 per annum available for exceptional candidates depending on specialist skills and expertise. The total package will comprise base salary, additional pensionable allowances, pension benefits, generous annual leave allowance and flexible working arrangements. 11th January 2012 w/c 16th January 2012 w/c 23rd January 2012 Essential * provides a vision and defines clear team roles, responsibilities and objectives * empowers, motivates and inspires teams to deliver timely results, systematically reviews progress and team performance * develops the capability of teams to deliver the Department’s objectives Essential * thinks strategically when integrating complex or conflicting analysis from a range of sources to provide balanced advice * makes sound, evidence-based decisions, assesses risks and defends decisions and actions * champions and encourages others to think strategically when developing new approaches or addressing novel problems Essential * shapes, links and manages customer and stakeholder expectations to determine delivery capabilities, accounting for changing requirements * builds trust and openness with customers and stakeholders, keeps them updated on progress and acts upon feedback * empowers others to improve the scope, delivery capability, measurement and provision of customer and stakeholder services Essential * ensures timely performance assessment to provide or gain constructive feedback on the delivery of objectives and equal opportunities for people to develop their capabilities Desirable * reinforces a culture that values all people and rewards productive behaviour, promptly addresses any poor performance * aligns people’s development needs with strategic corporate requirements to enable talent to flourish Essential * delivers timely project objectives within budget and to quality standards, ensures that business benefits are realised Desirable * manages people, resources and relationships efficiently to ensure effective delivery of programmes and projects * evaluates outcomes of programmes and projects to build capability and share lessons learned Essential  * plans effectively and utilises budgets and resources, including the authority to re-deploy funds across functions, to deliver measurable value for money Desirable * ensures an appropriate understanding of roles and responsibilities in complying with guidance on finance policies and procedures * anticipates business needs and manages resource requirements Essential * promotes effective information sharing Desirable * uses experts to help ensure the effective communication and marketing of business objectives * works in partnership with experts to define, plan, implement, review and measure the effectiveness of communication and marketing channels Essential * solid experience of web application development, ideally in Ruby and Javascript * enthusiasm for sharing knowledge and working in a multi-disciplinary team. * a habit of writing robust automated tests for your work * ability to quickly learn new languages and frameworks and happiness to pick the right tool for the job * enthusiasm for sharing knowledge and working in a multi-disciplinary team Desirable * experience of building and scaling high-traffic websites * knowledge of content management techniques, workflows, etc. * ability to train and mentor new developers, and introduce non-programmers to programming concepts How to Apply Please send completed application form to: cabinet-office.external@dwp.gsi.gov.uk quoting ref 1225793 No later than close of business 11th January 2012 Late or faxed applications will not be accepted Candidates will be asked to complete a test. More information will be provided to successful candidates who are notified of an interview date.","description":"NOTE: the following template was used to advertise a developer vacancy at GDS","link":"/service-manual/the-team/developer-jd.html"},{"title":"Developer skills","indexable_content":"Developers build software with a relentless focus on how it will be used. They continually improve the service by identifying new tools and techniques and removing technical bottlenecks Good digital services will require code to be written, adapted, maintained and supported. A team of skilled developers will be able to make sure that happens in an efficient and transparent way, and to help continually improve the service by identifying new tools and techniques and removing technical bottlenecks. No digital service can be effectively built, delivered, owned and operated without the technical skills to understand and improve the software enabling it. In order to provide the best service to your users it is vital that you are in a position to rapidly tailor that software to their needs and to the efficient running of the underlying systems. Developers will be able to work directly on those services, but are also an important part of service innovation as they bring ideas, generate prototypes and contribute to a rounded team.  Once a service is live the need for developers continues. There will always be technical optimisations that can be made - faster and more responsive systems, acting to mitigate the risks of a constantly changing security environment - but as you respond to new or more clearly understood user needs the software will need to adapt. As the policy and other context around a service changes the software may need to integrate with new systems or provide new features and a development team can help ensure that work is pro-active. A great developer: You would expect any developer to: Depending on what you’re building and the size of the project you may need to introduce additional roles into your development team such as “tech lead”, “junior/senior developer”, “technical architect”. GDS will provide more details on how we approach this in the coming weeks. Read guidance in the manual of particular interest to developers. See an example of a developer job description provided by GDS.","description":"Developers build software with a relentless focus on how it will be used. They continually improve the service by identifying new tools and techniques and removing technical bottlenecks","link":"/service-manual/the-team/developer.html"},{"title":"Service manager job description","indexable_content":"NOTE: the following template was used to advertise a service manager vacancy at GDS POST TITLE:   \t\t\tService Manager \t PAYBAND: \t\t\t\tSCS1\t\t\t\t\t\t\t\t\t UNIT/DIVISION: \t\t LOCATION: \t\t \t\t\t TYPE OF POSTING:     \t24 Month Fixed Term Appointment with possibility of extension or permanency This role will be in the [Business Unit] Team who are responsible for on and offline continuous service delivery, developing and delivering all the changes and improvements necessary to provide effective and holistic services for users.   Your role is to be the owner of a high quality user experience between people and your department, by being the driving force behind service provision. You will lead services that are not just best in class, but give the same level of digital experience users expect from daily interaction with the most respected web services. You will be representing your service to Departmental Board-level officials and senior officials in [Dept], using information from diverse user, commercial and service sources to create and project a compelling product vision. You will already be an experienced leader, with an in-depth understanding of service delivery and equipped to represent your service and its users’ needs at all levels within and outside an organisation. You will have the digital literacy to engage with technical staff and suppliers to define the best system and platform configurations to achieve business/user objectives.  CONTACT DETAILS FOR INFORMATION REGARDING RECRUITMENT PROCESS, ELIGIBILITY, TERMS & CONDITIONS:  FURTHER INFORMATION REGARDING THESE POSTS:  CLOSING DATE: \t\t\t\t\t\t\t\t\t EXPECTED SIFT DATE: \t\tW/c \t\t\t\t EXPECTED INTERVIEW DATES: \tW/c  APPLICATIONS ARE WELCOME FROM PART TIMERS AND JOB SHARERS  SALARY SCALE: You will be responsible for acting as the user advocate at all times to ensure that the service remains relevant, convenient and straightforward for them to use.  Your role will involve managing the product lifecycle - initial design and/or redesign,  delivery, on-going success and continuous improvement - of one or more transactional services and/or platforms, plus sharing your expertise across government  You will: Essential * develop an in-depth insight into customers, citizens, services, communities and markets affected by their area and the wider public sector context * create joined up strategies and plans that have positive impact and add value for stakeholders, citizens and communities * shape strategies and plans which help put into practice and support the Department’s vision and long-term direction, including those shared with other departments Desirable * anticipate and predict the long term impact of national and international developments, including economic, political, environmental, social and technological, on own area * identify and shape how own area fits within and supports the work of the Department Essential * seek and encourage ideas, improvements and measured risk taking within own area to deliver better approaches and services * encourage a culture of imaginative thinking, seek to expand mindsets and genuinely listen to ideas from employees and stakeholders * identify step changes that quickly transform flexibility, responsiveness and quality of service * lead the transformation of services to users, moving to a digital approach whenever possible Desirable * challenge the status quo in own and related areas to achieve value-adding improvements and change  * create effective plans, systems and governance to manage change and respond promptly to critical events Essential * take quick, confident decisions at a strategic level to move things forward * outline direction of travel, recommendations and decisions for their area, taking account of financial and implementation issues * ensure involvement and consultation where necessary and take decisive action when required * interpret a wide range of political and national pressures and influences to develop strategies Desirable * weigh up competing views to generate ways forward which will meet organisational goals * articulate options and large-scale reputational risks and impacts, including economic, environmental, political and social, and recommend plans to manage and mitigate Essential * communicate with conviction and clarity in the face of tough negotiations or challenges * influence external partners, stakeholders and customers successfully – secure mutually beneficial outcomes * inspire staff and delivery partners to engage fully with long term vision and purpose of the Department, supporting them to make sense of change Desirable * actively promote the Department’s reputation externally and internally – publicise successes widely * actively promote diversity and equality of opportunity inside and outside the Civil Service * lead from the front, communicating and motivating people towards stretching goals Essential * proactively create, maintain and promote a strong network of connections with colleagues across the Department, wider Civil Service and externally * actively promote knowledge and resource sharing with peers and across functions * build high performing teams within own area, aligned around common goals * encourage teams to engage with a variety of delivery partners and stakeholders and listen to their feedback * set out clear expectations that bullying, harassment and discrimination are unacceptable Desirable * encourage and establish principles of working effectively across boundaries to support the business Essential * encourage work-place based learning, ensure colleagues take responsibility for their own learning and share it to build organisational capability * devote dedicated time to supporting and empowering people through coaching and mentoring and sharing expertise/knowledge * identify capability requirements to deliver Departmental 3-5 year strategy and grow sustainable capability across all groups including those identified as having high potential Desirable * champion development, talent and career management for all staff and make learning a reality by encouraging and providing a range of development experiences * create an inclusive environment, one from which all staff, including under-represented groups, can benefit * role model continuous learning and self development, evaluating own effectiveness and growth and planning next learning steps accordingly Essential * promote a strong focus on the needs of customers, suppliers and other delivery partners to develop new commercial models for the delivery of policy and business goals * manage strategic commercial relationships and delivery arrangements actively and effectively to provide ongoing value for money to the tax payer * ensure teams appreciate how market demands, investment decisions and other commercial considerations such as funding and pricing models influence suppliers and the delivery of services Desirable * identify and implement different ways of working deployed in other sectors e.g. using resources, assets and commercial arrangements * develop and apply market and economic understanding and insights, working with commercial experts, to support sound commercial decision-making and recommendations * take a wide view, successfully achieving common goals with organisations that have different priorities Essential * understand the financial position of own area, the organisation and the wider economy and recognise impacts of this when delivering services  * make and encourage strategic choices on spend, challenge high risk costly projects and forgo non-priority expenditure  * promote and visibly demonstrate a culture of value for money in own area/function in order to focus managers on getting a good return for taxpayers‟ money * interpret a wide range of financial and management information and use financial data effectively in decisions * develop robust business cases, with fully costed options identifying clear policy advantages and/or returns on investment to assist decision making Desirable * understand and manage the risks and cost-drivers for own areas of responsibility in the context of strategic priority Essential * clarify and articulate the diverse requirements of customers and delivery partners to support effective delivery * interpret customer insight and user data to determine and drive customer service outcomes and quality throughout own area * translate complex aims into clear and manageable plans and determine resource requirements to support implementation  * maintain and improve service by managing risks to ensure own area and partners deliver against defined outcomes * work collaboratively with customers or service delivery partners to manage, monitor and deliver against service level agreements Desirable * facilitate flexible use of resources across grades through innovative structuring of teams and resources within own area Essential * translate strategic priorities into clear outcome-focused objectives for managers and provide the energy and drive in achievement of these objectives * take ownership of delivery against outcomes and give credit for others‟ delivery * maintain a strong focus on priorities, holding others to account for priorities and swiftly respond to changing requirements * act as a role model for delivery by injecting enthusiasm and energy to achieve results * promote resilience and responsiveness in the organisation by being open and honest about challenges, and the actions required to address unexpected developments Desirable * drive a performance culture within own area and support and encourage a focus on performance and priorities Essential * proven leadership in successful service vision and delivery harnessing the power of digital.  * proven grasp and expertise in analysing and using customer insight and user and performance data to design and continually improve digital services to fully meet user needs in a convenient and straightforward way. * successful design, delivery and ongoing management of high quality services which  maximised opportunities offered by digital technology. * practical success in interpreting user data and feedback to design and implement channel shift strategies to move users from traditional to digital delivery channels * track record of successful negotiation with colleagues and stakeholders to identify potential for working across traditional service divides to enhance and improve user experience. * proven ability to challenge and remove any unnecessary barriers to excellent digital service delivery. * experience of using lean and agile approaches to service development * demonstrable communication skills and impact, with the ability to communicate and gain commitment to the benefits of digital effectively to a large range of stakeholders inside and outside government How to Apply Please send completed CV and Covering Letter to: [department email address] quoting ref [XXXXX No later than close of business [Date] Late or faxed applications will not be accepted","description":"NOTE: the following template was used to advertise a service manager vacancy at GDS","link":"/service-manual/the-team/service-manager-jd.html"},{"title":"Service managers","indexable_content":"Service managers are individuals who work full-time to develop and deliver all the changes necessary to provide effective user focused digital services. Outside government, organisations in the public and private sector are learning that empowered, experienced and highly skilled managers (often called Product Managers in the commercial world) are necessary to deliver high-quality digital services. We are adopting that model, requiring each transactional digital service handling over 100,000 transactions each year to be developed, operated and continually improved by an experienced, skilled and empowered Service Manager. These are not technical IT posts, nor are they confined to running a website. Instead, they are individuals who work full-time to develop and deliver all the changes necessary to provide effective digital services. With a handful of exceptions, this is a new role within government. These Service Managers will: This will depend on the scale of the service you are working on.  In some cases the Service Manager will also be able to fulfil the role of Product Manager - working closely with the delivery team (the “makers”), prioritising stories for each sprint, attending daily stand ups, being on hand to comment on solutions as they emerge, and accepting stories once they are delivered. However, in many cases it is likely that the Service Manager won’t have the capacity to be this hands-on, so they are likely to need a dedicated Product Manager. The government digital strategy requires services handling over 100,000 transactions each year to be re-designed, operated and improved by a skilled, experienced and empowered Service Manager. Cabinet Office will help departments to recruit suitably skilled individuals. Newly appointed Service Managers will be supported by GDS through a specialist training programme. This will include the hands-on process of designing and prototyping a digital service. An example job description for this role provided by GDS. Read guidance within the manual of particular interest to service managers. Building a team","description":"Service managers are individuals who work full-time to develop and deliver all the changes necessary to provide effective user focused digital services.","link":"/service-manual/the-team/service-manager.html"},{"title":"Delivery manager job decription","indexable_content":"NOTE: the following template was used to advertise a web operations vacancy at GDS POST TITLE:             Web Operations\t PAYBAND:               \tA\t\t\t\t\t\t\t UNIT/DIVISION:       \tGovernment Digital Service\t\t LOCATION:              \tLondon\t\t\t\t\t\t\t\t\t TYPE OF POSTING: \t    2 year FTA with possibility of extension or permanency. The post will be reevaluated at the end of the period to match against the on-going requirements of GDS. The Government Digital Service is part of the Cabinet Office. Our job is to help transform digital services throughout government. We do that by building world-class services, by developing digital capability throughout the civil service, and by setting a standard for service design across government. All of our work puts the needs of users first. Only by developing around the needs of citizens can we create digital services so good that people prefer to use them. The Government Digital Service is seeking motivated individuals with sound judgment to work within the Infrastructure team supporting our Web Platform while developing new solutions to meet the needs of stakeholders across government.  The applicant must have demonstrable experience configuring web and application servers and possess a fundamental understanding of Linux. An ideal candidate will have previous experience supporting a large production platform. Participation in an out of hour’s on-call rota is a requirement of this role. CONTACT DETAILS FOR INFORMATION REGARDING RECRUITMENT PROCESS, ELIGIBILITY, TERMS & CONDITIONS: \t\t\t\t\t\t\t\t cabinet-office.external@dwp.gsi.gov.uk FURTHER INFORMATION REGARDING THESE POSTS:  Please contact Carl Massa - carl.massa@digital.cabinet-office.gov.uk CLOSING DATE: \t\t\t\t EXPECTED SIFT DATE:\t\t EXPECTED INTERVIEW DATES:\t APPLICATIONS ARE WELCOME FROM PART TIMERS AND JOB SHARERS   NOTE: SUCCESSFUL APPLICANTS WILL TRANSFER ONTO CABINET OFFICE TERMS AND CONDITIONS INCLUDING SALARY. FOR POSTS ADVERTISED ON LOAN, WHERE EXISTING SALARIES EXCEED CABINET OFFICE PAY SCALES IT MAY BE POSSIBLE TO REMAIN ON THE PARENT DEPARTMENT’S PAY AND APPRAISAL SYSTEM. ALLOWANCES WILL NOT BE CARRIED OVER UNLESS APPLICABLE TO THE JOB. EXCESS COSTS WILL BE PAYABLE ONLY IF THERE IS A PROVEN BUSINESS CASE. Essential * anticipate technological developments to keep activity relevant and targeted Essential * understand and identify the role of technology in public service delivery and policy implementation * spot warning signs of things going wrong and provide a decisive response to delivery challenges Essential * make difficult decisions by pragmatically weighing the complexities involved against the need to act * draw together and present reasonable conclusions from a wide range of incomplete and complex evidence and data - able to act or decide even when details are not clear * identify the main issues in complex problems, clarify understanding of stakeholder expectations, to seek best option Essential * seek constructive outcomes in discussions, challenge assumptions but remain willing to compromise when it is beneficial to progress. Essential * question and challenge the value being delivered through commercial arrangements with delivery partners Essential  * ensure the service offer thoroughly considers customers’ needs and a broad range of available methods to meet this, including new technology where relevant. Essential * maintain effective performance in difficult and challenging circumstances, encouraging others to do the same. * review, challenge and adjust performance levels to ensure quality outcomes are delivered on time Essential * understanding of common web application architectures * experience configuring and managing Linux servers for serving a dynamic website * experience debugging a complex multi-server service * scripting or basic programming skills * familiarity with network protocols - TCP/IP, HTTP, SSL, etc. Desirable * installation and management of open source monitoring tools * configuration management tools like Puppet, Chef * deploying and configuring machines in a Cloud environment * understanding of application deployment strategies and continuous integration * working within a product-centric environment","description":"NOTE: the following template was used to advertise a web operations vacancy at GDS","link":"/service-manual/the-team/web-operations-jd.html"},{"title":"Web operations skills","indexable_content":"Web operations (sometimes called systems administrators, operations engineers or site reliability engineers) run the production systems and help the development team build software that is easy to operate, scale and secure.  This involves expertise in infrastructure, configuration management, monitoring, deployment and operating systems. Web operations people help run the eventual production systems, but also to help the development team build software that is easy to operate. Thinking about how the eventual system will be run at the very start of the project is important if you want to smoothly move from prototypes to production systems. At a high level they will: With specific skills: And ideally have an interest in or some experience with: A sample job description for web operations provided by GDS.","description":"Web operations (sometimes called systems administrators, operations engineers or site reliability engineers) run the production systems and help the development team build software that is easy to operate, scale and secure. ","link":"/service-manual/the-team/web-operations.html"},{"title":"Your working environment","indexable_content":"Working spaces for digital projects will vary, but there are some things you can do to ensure that the space you have available can be used in the way your team needs. Those working in creative and technical fields often need plenty of space for focussed, detailed work. It’s not uncommon to see people spending a significant chunk of their day with headphones on to help them focus, or locked in conversation with just one person with whom they’re pairing.  Equally, you might expect to see lots of short meetings throughout a day, often around walls covered in notes or reference material. These regular exchanges are important to ensure the quality of the work, but can seem strange in office cultures that are much more used to formal meetings or conference calls.  The room a team works in is a tool. It is just as important as the choice of project management tools or choice of programming language. Teams should dedicate time at the beginning of a project to making sure they have everything they need and addressing any problems. This might include setting up a project wall, configuring collaboration software like email groups or project trackers, building and installing dashboards, putting up whiteboards, or simply moving desks about so the team can sit together.  Removing those dividers between desks makes a big difference and allows conversation between the team to flow more freely. If large monitors are getting in the way remove them.  You might also want to think about getting desk tidies like these. The difference a tidy environment makes to ability of a team to think and work is striking.  This process is known as ‘hacking the environment’. This might seem like an obvious one, but teams working together to deliver a product should sit close together. Short, informal conversations are an important way to test assumptions, and this gets much harder when a team is distributed across an office, or worse, in different buildings. When working on the design, development and operation of a service it’s essential that your team be able to be in constant contact with one another to make rapid decisions, provide support and information and to ensure everyone’s aware of the project as a whole.  Some of that will be achieved through regular short meetings such as a daily standup or weekly “show and tell” session, but there’s also a need for an asynchronous mechanism that’s more immediate and conversational than email but that allows people to dip in and out.  Internet Relay Chat (IRC) – and similar tools such as group messengers, Campfire, and so on – operate as software running on the user’s computer as a dedicated application or in a web browser, supporting a constant stream of live conversation. Typically, a user would leave the service running in the background while working on other things, switching focus when a break is needed or when you need to ask a specific question. Questions might be along the lines of: Anyone on the team would be able to respond with information, suggestions of other people to talk to, and so on. Because it is network based it will also work regardless of geography so distributed teams can continue to communicate as if they were in one room. Paul Graham on Makers’ Schedules vs. Managers’ Schedules","description":"Working spaces for digital projects will vary, but there are some things you can do to ensure that the space you have available can be used in the way your team needs.","link":"/service-manual/the-team/working-environment.html"},{"title":"Working with specialists and suppliers","indexable_content":"Specialists can help fill the gaps in capability you identify throughout your service’s lifespan. However, the benefit of their skills and perspective can come with the challenge of procuring and inducting them. As the service manager or product owner’s vision is shaped by the team, you may encounter areas where you lack the necessary expertise to deliver to a high standard. First, talk to your team and see if they can identify the skills lacking. If you find these can’t be found within your wider department then it might be time to think about procuring specialists. You can talk to GDS and other service owners at any time for their advice on what to do next. Trust domain experts to define what you need to be looking for. Developers and designers will be best placed to tell you what front end skills you need, for example. GDS can help provide guidance and job specifications if you need to further define the role. If you know particular individuals are the best fit for a particular skill then there’s nothing wrong with hiring them. GDS are happy to provide advice on a prospective candidate too. Equally there’s nothing wrong with using a recruitment specialist to get the candidate you need. Remember that the service will be judged on what it delivers, so you must have confidence that the specialists you recruit are the right people to deliver that work. GDS can also provide support to departments to assist them during their procurements for Digital / Agile services, whether this is using existing purchasing frameworks / tools (e.g. G-Cloud, Spot-Buy, etc) in advance of the Digital Procurement Framework being available, or following its go-live. This support could include early-stage supplier engagement in the form of hack days, proof of concepts, prototypes, etc, either in advance of, or as a first-stage evaluation for, a formal procurement. The Digital Procurement Framework is being designed by the Government Digital Service and Government Procurement Service and following an Official Journal of the European Union (OJEU) review is expected to be available to departments from the middle of 2013. This will enable departments access to a wide range of suppliers of all sizes. These will supply people and teams rather than hardware, software and the like, to build digital services. What follows is a brief description of a framework which will come on stream in Summer 2013. GDS will ensure there’s a sufficient supply of companies able to deliver digital capability where government does business. The list of suppliers available on the framework will be refreshed periodically (e.g. every 4 to 5 months) as technology and the market moves. We want to provide an online portal that enables departments to buy digital/agile services from the suppliers on the Digital Procurement Framework, and are currently looking at utilising the platform that GDS is building for the G-Cloud team, ideally leading to a single point of entry for buyers and suppliers alike. This portal would support the ‘source to settle’ process, including: In addition to those features described in the following table, The Digital Procurement Framework will also have a more suitable supplier payment structure and Terms & Conditions.  While the GDS Digital Framework focuses on Agile development, some services to be available on the new framework are currently available in G-Cloud. However G-CLOUD’s primary focus is commodity services, such as infrastructure and hosting.   Achieving the right mix of contract versus permanent staff on your team will depend on your product. Contractors should receive an induction and be thoughtfully integrated into the team - where do they sit, who next to, how do they fit? Use some common sense; think about how long will this person be with you and tailor the induction accordingly. For example, in the GOV.UK platform team, GDS hired a specialist search engine consultant to work with the team for two days. He worked with two permanent members of the team so they could learn from him. Skills transfer is vitally important - we can all learn from specialists. But more importantly, don’t allow a contractor or consultant to become a single point of failure in your team. Knowledge must be shared for the maximum benefit. For the good of your team and to get the best results from bringing in a specialist contractor, you should not treat them any differently from a permanent member of the team, they are people too. An underlying principle behind agile development is that people are more important than process. Throughout the digital world, it is proven that great teams make great products whether they’re in government or business. At GDS, we do not call people “resources”, whether permanent or contract. You look after people, resources are things you use (and discard!). Some guiding principles for service managers or delivery managers: And don’t forget, contractors can’t sign things! A blogpost by Meri Williams of GDS on people management in an agile setting","description":"Specialists can help fill the gaps in capability you identify throughout your service’s lifespan. However, the benefit of their skills and perspective can come with the challenge of procuring and inducting them.","link":"/service-manual/the-team/working-with-specialists.html"},{"title":"Card sorting","indexable_content":"Card sorting is a research method used to understand the way that the intended users of a web site naturally organise or think about different types of information or content. It’s also a method service teams can use to sort and arrange user needs. The method is very simple to use: it involves simply writing the needs or topics on pieces of card (maximum of around 50) and asking representative users to organise these into groups, which are then given a meaningful label. 15 to 30 users are recommended: a minimum of 15 users will give reasonable confidence in the results, but any more than 30 gives diminishing returns. It is also possible to conduct a closed card sort, whereby the users are given pre-determined categories and are asked to allocate each card to one of these categories. Open card sorts are most suitable for identifying how users think about information, in order to build an information architecture. Closed card sorts are most suitable for validating an information architecture. The analysis of card sorting can be difficult if no clear categories emerge. It can also be difficult to conduct a card sort on a very large website with a broad scope of content (e.g. GOV.UK) since the maximum number of cards may not be representative of all of the content. Card sorts can also be conducted using an online tool such as OptimalSort. This is a cheap and effective way of reaching a large sample of users without needing to bring them into a dedicated lab facility for the study.","description":"Card sorting is a research method used to understand the way that the intended users of a web site naturally organise or think about different types of information or content. It’s also a method service teams can use to sort and arrange user needs.","link":"/service-manual/users/card-sorting.html"},{"title":"An introduction to user research","indexable_content":"This guidance provides a broad overview of the methods and techniques available to conduct user research. More detailed guidance on each of these techniques can be found in the links below.  User research can be categorised into two broad themes – product research, and strategic research.  Product research can incorporate both qualitative, and quantitative techniques. Qualitative techniques are intensive, and often small scale. These include focus groups and 1:1 interviews, and are typically used to explore and analyse unstructured data.  Quantitative techniques involve higher-volume research, and include online surveys, face-to-face interviews, and involve a structured approach to data collection and analysis.  Product based research can be conducted in-house or via a specialist agency. In-house approaches can be quicker as they involve less lead time (e.g. procurement), but require skilled in-house researchers, and can involve the procurement of some specialist software. Typically agencies are only used when specialist skills/experience is required that is not available in-house, and/or the scale of the project would mean it is difficult to provide dedicated internal resource.  Strategic research also uses quantitative and qualitative methods, and is used to help understand the appetite for a product or service – typically looking at the size of the market, trends, type of users etc. Secondary data, also known as desk research, can be used, and involves the analysis of existing research or information sources. Ideally, secondary research should be conducted before embarking on any research project to understand what is already known, and what research is required to fill the knowledge gaps. This can range from published research, news article, or internal research that has been conducted previously. The table below illustrates the typical product lifecycle, showing the stages at which user research should be conducted. This chart can be used as a reference to ensure that user needs are being factored into design at every stage.  Typically this will involve some initial fact finding in the early stages to understand the user needs, including who they are, how they currently do things, how they’d like to do them, and what information is currently available. ","description":"This guidance provides a broad overview of the methods and techniques available to conduct user research. More detailed guidance on each of these techniques can be found in the links below. ","link":"/service-manual/users/introduction-to-user-research.html"},{"title":"User needs","indexable_content":"Deep understanding of your users needs is crucial for building a successful digital service.  Any thinking about a service, whether online or offline, must start with the question: What is the user need? Defining a user need must be strict and honest. For GDS, it’s the need the user has of government, not the need of government to impart information to the user. That’s an important distinction, because it means that you’ll be able to more measure the success of your services and iteratively improve them to meet the needs of the people who will make use of them. You need to set the boundaries of what your service can and should offer, to prevent a bottomless list of user needs.  This is especially true of a government website such as GOV.UK. As well as identifying user needs, we identified which of those needs the government must answer. This led to the ‘It’s in if…’ criteria: Similar criteria may help you determine which needs you build a service to meet. Expressing a user need mustn’t imply the solution. The user need should be expressed as a user story, so the service team can discuss and explore possible solutions. Read more detailed guidance about writing user stories. What was the evidence? Users’ information needs and analytics Using search data to meet user needs Exploring user needs What we know about the users of Inside Government Meeting the needs of businesses Introducing the Needotron: working out the shape of the product","description":"Deep understanding of your users needs is crucial for building a successful digital service. ","link":"/service-manual/users/user-needs.html"},{"title":"Community user groups","indexable_content":"An online research community is an engaging website that allows pre selected respondents to interact and complete a series of tasks.  The tasks differ in format and can include; online forums, blogs, polls, surveys, instant chat and a lot more. Tasks are usually creative, and ask different questions relating to the research objective/s. The types of community differ and run across a spectrum of openness, permanency and size. Research communities are private, often have between 300-500 members and focus on building relationships between participants. This type of community offers a great source of deep insight over long periods of time. Panels on the other hand can be public or private, are large in scale (10,000 to 100,000 participants) and focus on one way, quantitative research. Research communities can be used for a wide range of projects such as audience understanding, diary studies, concept development or simply idea generation. They allow a business to get close to users and offer meaningful dialogue between an organisation and it’s customers. The key benefits are: Communities require constant management. Participants need to be kept engaged, which means daily interaction, new questions and inventive approaches to maintain their interest. The community needs constant moderation, usually by a research manager or third party research agency. The quality of community members is paramount. Their feedback needs to be quality checked and panel attrition needs to be monitored on an on-going basis to ensure fresh ideas.  Participants can be obtained internally through websites, emails, and other user channels, or via third party sample companies.","description":"An online research community is an engaging website that allows pre selected respondents to interact and complete a series of tasks. ","link":"/service-manual/users/user-research/communityusergroups.html"},{"title":"Discussion Guides","indexable_content":"Discussion guides are used in order to ensure that focus groups and 1:1 interviews cover the required topics, and information is obtained from the sessions that will address the needs of the research.  In preparation for a focus group / 1:1 interviews it is helpful to generate a list of questions that address the information that you’re interested in obtaining from the sessions. These questions should be open ended, and structured in a manner that will form the basis for a draft discussion guide, and help elicit information from respondents in a sensible flow. Writing a discussion guide should be an iterative process and once the initial draft has been written it is helpful to get input from other people on the project team.  As suggested by the name, discussion guides should be used as a guide to the discussion, and in comparison with a structured questionnaire, questions and areas for coverage should not read out verbatim. This enables the discussion to be led by respondents’ own experience. Typically each section of the guide would include time guidance to ensure all areas can be covered and it should also indicate when stimulus is being used, and specify participants’ tasks. A discussion guide written for testing that was conducted with BIS experts for the development of GOV.UK (PDF, 157kb).","description":"Discussion guides are used in order to ensure that focus groups and 1:1 interviews cover the required topics, and information is obtained from the sessions that will address the needs of the research. ","link":"/service-manual/users/user-research/discussionguides.html"},{"title":"Ethnographic research","indexable_content":"This guidance looks at Ethnographic research and how it can be used in order to provide user research to feed into product and service design.  Ethnographic research usually involves observing target users in their natural, real-world setting,  rather than in the artificial environment of a lab or focus group.  The aim is to gather insight into how people live; what they do; how they use things; or what they need in their everyday or professional lives.  Observations can be made at home, at work, or in leisure environments - people can be studied with their family, on their own, with work colleagues, or as part of a group of friends.  Often one participant may be recruited, but several more may be studied as part of that person’s family or friends.   Ethnographic research relies on techniques such as observation, video diaries, photographs, contextual interviews, and analysis of artifacts such as for example devices, tools or paper forms that might be used as part of a person’s job.   Data collection can range from a 4-5 hour contextual interview, through to following a participant for several days, or even a longitudinal study over several weeks or months to investigate, for example, how a particular product or service might be used over time.  It doesn’t necessarily involve “full immersion” in a person’s life: it can involve a depth interview in a person’s home or it might involve a person simply maintaining their own video diary over a period of time. Ethnographic research can provide extremely rich insight into “real life” behaviour, and can be used to identify new or currently unmet user needs. This approach is most valuable at the beginning of a project when there is a need to understand real end user needs, or to understand the constraints of using a new product or service by a particular audience. Ethnographic research can provide a significant amount of qualitative data, and analysis can be time consuming.   The term ‘ethnographic’ can be misused, it’s currently a bit of a “buzz word” with some agencies who may not fully understand the approach.  It is recommended that a specialist agency is used, who can demonstrate successful case studies (collecting and analysing the data).  In principle, anyone could participate in this type of research.  As with any user research, the recruitment of suitable participants is key.  The full implications of the research should be fully explained to potential participants, as some may not feel comfortable with this level of intrusion in their lives. Depending on the study needs and the approach, but 6-8 weeks from briefing to results can provide rich insight.  It may take time to build trust with participants, and the analysis period needs to be sufficient to be thorough. Ethnographic research can be expensive and time consuming, but this depends on the needs of a particular project. The benefits derived can be extremely valuable.","description":"This guidance looks at Ethnographic research and how it can be used in order to provide user research to feed into product and service design. ","link":"/service-manual/users/user-research/ethnographicresearch.html"},{"title":"Expert reviews","indexable_content":"Expert reviews  - also known as heuristic evaluations - are low cost usability methods that don’t involve participation of real end users. An ‘expert’ usability evaluator can assess a product (or web site) against a known set of ‘heuristics’, or usability guidelines (best practice). An alternative approach is to conduct a ‘cognitive walkthrough’ against specific use cases or scenarios.   Ideally two usability experts will conduct the review independently, and then meet to discuss and agree the findings before making recommendations to the service manager. A list of widely accepted (although not necessarily validated) heuristics are provided by Jakob Nielsen or an alternative set can be found in the International Standard ISO 9241. A ‘cognitive walkthrough’ is a usability inspection that aims to identify usability issues by focusing on how easy it is for users to accomplish specific tasks with the system (or website).   This method starts with identifying the user goals, then conducting a task analysis to specify the sequence of steps or actions required to achieve each task. The usability expert, along with designers and developers, then walks through these identified steps to assess the extent to which a user can achieve their goal. Both of these approaches can be used to evaluate an existing product or website, or as a quick, low cost method of evaluating a product in development. These approaches can be considered to be “better than nothing”, but will never provide the same quality insight as testing with real end users. However, a competent usability specialist will often identify issues that are subsequently seen in traditional user testing or in actual use. Some agencies conduct expert reviews as the first step of all usability projects. The fundamental weakness of any expert review is that it doesn’t involve use by real end users. Some people may therefore consider this to be purely opinion, but input from an experienced usability expert is better than no user testing at all. Dependent on whether there is any in-house expertise available to conduct the review. An agency can be commissioned, with costs upwards of £2000 depending on the service scope. Depends on the scope of the service - for a simple website application this could be turned around in 1-2 days.","description":"Expert reviews  - also known as heuristic evaluations - are low cost usability methods that don’t involve participation of real end users. An ‘expert’ usability evaluator can assess a product (or web site) against a known set of ‘heuristics’, or usability guidelines (best practice). An alternative approach is to conduct a ‘cognitive walkthrough’ against specific use cases or scenarios.  ","link":"/service-manual/users/user-research/expert-review.html"},{"title":"Focus groups, mini groups, and 1:1 interviews","indexable_content":"Focus groups, mini groups, and 1:1 interviews involve unstructured interviews or group discussion.  A focus group is normally comprised of 6-12 people, although sometimes mini groups are favoured (4-5 people) as they can lead to a greater depth of discussion. 1:1 interviews are conducted by a moderator with a single respondent, sometimes these are conducted over the phone (telephone depths). Interviews and discussion groups are both facilitated by a trained moderator using a specially designed topic guide in order to ensure the discussion is focused and keeps on topic. Sessions normally last 1-2 hours and will often involve participants being shown stimulus – e.g. communications materials to inform discussion, and sessions may include interactive techniques e.g. role play scenarios. Focus groups and 1:1 interviews are useful techniques for exploring and mapping reasons for attitudes and behaviour, understanding how target audiences approach issues or may be encouraged to change. They also enable participants reactions to be monitored, and the moderator to probe on interesting issues when necessary. As stated in the previous section, the presence of a moderator also enables discussions to be focused and kept on track. Dedicated viewing facilities are often used that enable sessions to be recorded, and interested parties can observe the sessions (often via a two way mirror) in a separate room to the participants. This often results in the research having more credibility as interested parties will have been able to view the sessions, and witness the findings first hand - this also means key issues can be acted upon quicker as observers can feed straight back to their teams. Some sessions take place in community settings, encouraging less confident/harder-to-reach audiences to attend. Focus groups can also be conducted online. The number of groups is often dictated by the budget available, but the average project will have 4 to 6 groups, with larger ones having between 10 and 20. Besides budget, other factors that influence the number of groups used are the number of potential users groups that need to be covered, and the number of geographic areas. Focus groups and 1:1 interviews can be expensive compared with other types of research, while the small samples size means that findings are not statistically significant and it is difficult to generalise across audience groups. Analysis of the sessions can also be time consuming. Moderators also need to build a rapport with respondents. If he or she fails to do this, and can’t control the group adequately, it can result in the sessions being dominated by one or two participants, and biased data being collected.  Participants can be obtained from the general population and hard to reach groups. Focus groups normally consist of 6-12 people. Mini groups are smaller with 4-5 people. Obviously 1:1 depths interviews just have one participant. Normally 6-8 weeks from briefing to results. Analysis period needs to be sufficient to be thorough. To outsource to an external agency the costs are likely to be:","description":"Focus groups, mini groups, and 1:1 interviews involve unstructured interviews or group discussion. ","link":"/service-manual/users/user-research/focusgroupsminigroupsandinterviews.html"},{"title":"Guerilla testing","indexable_content":"This guidance talks about how Guerilla testing can be used to provide user research that can feed into product and service design. Guerilla user testing is a low cost method of user testing. The term ‘guerilla’ refers to its ‘out in the wild’ style, in the fact that it can be conducted anywhere e.g. cafe, library, train station etc, essentially anywhere where there is significant footfall. Guerilla testing works well to quickly validate how effective a design is on it’s intended audience, whether certain functionality works in the way it is supposed to, or even establishing whether a brand or proposition is clear. This approach is quick and relatively easy to set up. Participants are not recruited but are ‘approached’ by those persons conducting the sessions. The sessions themselves are short, typically between 15-30 minutes and are loosely structured around specific key research objectives. The output is typically ‘qualitative’ so insight is often rich and detailed. Anyone on the service team can conduct ‘guerilla testing’ on their site or service but often the best scenario is for a researcher to run the sessions with the designer or developer. The researcher can help with defining the tasks, moderating the sessions as well as provide a level of ‘objectivity’ by not being the person who designed or built what is being evaluated.  Involving the designer / developer in the sessions enables them to see  first hand ‘real’ people interacting with their product, where there are areas for improvement and how they might go about resolving any issues. This approach also does away with any lengthy reporting back. Insights can be observed, taken away and fed back into the design process almost immediately. However, a brief summary with key findings and recommendations can be written up as a more formal record. It is a method that suits the ‘agile framework’ well. Guerilla testing can be used throughout the service lifecycle. As it is cheap to set up, run and report back on, it is a method that can be used frequently. There are a few logistics that should be taken into consideration before conducting any guerilla testing; The key weakness of guerilla testing as a research method it that is not statistically robust and participants may not always match your ‘target’ audience in terms of skills, expertise, knowledge. This can vary from between 6-12 participants in any given round of guerilla testing. It is very much dependent on where and when those sessions are conducted. There is no cost other than time. Depends on the scope of the service but between 1-2 days (incl conducting sessions and any reporting back) 16 teenagers evaluated the National Citizen Service website. Sessions were conducted in the canteen of a further education college in Nottingham. Usability and editorial findings were discovered and quickly fed back to the development teams. The whole process took 2 days.","description":"This guidance talks about how Guerilla testing can be used to provide user research that can feed into product and service design.","link":"/service-manual/users/user-research/guerillatesting.html"},{"title":"Lab based user testing","indexable_content":"This guidance talks about how lab based user testing can be used to provide user research that can feed into product and service design. User testing is a ‘qualitative’ research method that is used to gauge how easy and intuitive a (product, service, website) is to use and whether it supports the needs of it’s intended audience. From a traditional perspective, user testing measures how well participants respond in the key areas of: efficiency, accuracy, recall and emotional response but nowadays this approach is combined with more ‘qualitative’ techniques to help understand the users’ motivations and attitudes as well.  A product or service is usually assessed by asking small samples of the ‘target audience, in one to one sessions, to complete specific but realistic tasks. The facilitator observes how well the participants are able to complete those tasks, noting down the areas or features that cause the participant problems. Often the facilitator will ask participants to think aloud whilst completing those tasks in order to understand their decision processes better. User testing can be conducted on low-fi assets e.g. paper prototypes, wireframes and hi-fi assets e.g. html/css prototypes, live digital services. It’s most effective when conducted early within the lifecycle of a product but can be conducted towards the end of the service lifecycle to validate any usability improvements. A lab environment can provide a more formal setting for conducting sessions, with most spaces offering viewing facilities enabling stakeholders and key team members to watch the sessions in real time.  Usability testing is best suited to finding the big issues, essentially the problems that affect users trying to perform tasks. However, it is qualitative in nature and is not statistically robust due to the small participant numbers, per round. The lab environment may also be considered a weakness. It means testing is conducted in a very different space to that a user will commonly be in. This method is not an appropriate for understanding user behaviour or user needs e.g. what they might want from a product / service.  Between 5-8 participants, per round of user testing is sufficient to highlight the majority of usability issues. The key is to test often. Costs will vary according to your target audience e.g. niche expertise, hard to reach etc. The easiest way to recruit participants is to use a market research recruitment agency. Expect to pay from £150-£200 per recruit (includes finders fee and cash incentive). Lab hire per day varies depending on location and facilities. Expect to pay between £800-£1200 per day. Dependent on project scope, availability of testing assets and resource. Recruitment via an agency will take up to 2 weeks Conducting testing sessions can take between 2-3 days depending on how many participants have been scheduled. Analysis and reporting can take up to a week.","description":"This guidance talks about how lab based user testing can be used to provide user research that can feed into product and service design.","link":"/service-manual/users/user-research/labbasedusertesting.html"},{"title":"A/B and multivariate experiments","indexable_content":"We interviewed Craig Sullivan, an industry guru on conversion optimization. He explains when he uses A/B and multivariate experiments in the design process. This article in Wired shows how A/B experiments were used to good effect in Obama’s election campaign. This article in eConsultancy shows how multivariate experiments were used to improve conversion rates at Lovefilm.","description":"We interviewed Craig Sullivan, an industry guru on conversion optimization. He explains when he uses A/B and multivariate experiments in the design process.","link":"/service-manual/users/user-research/multivariatetesting.html"},{"title":"Online Omnibus surveying","indexable_content":"This guidance looks at online Omnibus surveys and how they can be used in user research. Online Omnibus surveys are an effective way of interviewing a representative number of people, in a short period of time, and for a relatively low cost. Omnibus surveys of this type use an online panel to gather the sample, and as with regular offline Omnibus surveys, costs are kept down by collating questions on a variety of subjects, from a number of clients.  Omnibus panels are ideal when you have key questions that you want answering, and need to reach a representative number of people quickly and cheaply. An Omnibus survey is not appropriate when there are too many questions required. This would result in the survey being too long (combined with questions from other clients). Online panels are ideal for obtaining a representative sample of respondents (2000+), across all demographics, including hard to reach groups. Omnibus surveys are costed by the type of questions clients want to ask. Pre-coded questions are the cheapest (approximately £300 per question), and open ended or questions with video clips the most expensive (approximately £600 per question). A set of ten questions on an Online Omnibus would cost approximately £5,000, depending on question type. An online Omnibus panel can be conducted relatively quickly, with most companies running surveys twice a week with data delivered 4 days after survey goes in to field. Some companies offer an ‘on demand’ service where surveys can start any time, although a minimum of questions is normally required.","description":"This guidance looks at online Omnibus surveys and how they can be used in user research.","link":"/service-manual/users/user-research/onlineomnibussurvey.html"},{"title":"Online research panels","indexable_content":"An online panel is a collection of pre-recruited research participants who have agreed to take part in online research over a period of time. Members of the panels are incentivised to take part, and normally rewarded through vouchers, or points that have a monetary value. As members of the panel are pre-recruited it means that they can be easily targeted by demographic, ownership and lifestyle information. As well as regular consumer panels, some companies also run business panels. Online panels are used to target representative samples of people easily, cheaply and quickly. They can also be useful to target hard to reach groups, who would otherwise be impossible to reach within a realistic timeframe. Respondents completing research in the shortest amount of time is a common problem, as this indicates that they have not given full consideration to the research that they are taking part in. Most panel companies reduce this prospect by monitoring how respondents complete the research (by time completion, and by the way they answer certain questions) and remove any suspicious respondents from their database. In addition to this, most panel companies will ensure that their respondents only complete four to six surveys a year. You can select the participants by demographic, socio-economic group, internet usage etc. It is also possible to target hard to reach groups however fieldwork may be more time consuming and be more costly. Using panel companies can be relatively cheap, with a sample of approximately 400 respondents for a remote usability study costing £1,500 to £2,500. As the sample is ‘on tap’ online panels can be used to turn round projects very quickly. When used for remote user testing, fieldwork is normally complete within three to four days.","description":"An online panel is a collection of pre-recruited research participants who have agreed to take part in online research over a period of time. Members of the panels are incentivised to take part, and normally rewarded through vouchers, or points that have a monetary value. As members of the panel are pre-recruited it means that they can be easily targeted by demographic, ownership and lifestyle information. As well as regular consumer panels, some companies also run business panels.","link":"/service-manual/users/user-research/onlineresearchpanels.html"},{"title":"Remote usability/summative testing (quantative)","indexable_content":"This guidance talks about how remote usability testing can be used to provide user research that can feed into product and service design. Remote usability testing takes place outside the lab with users participating in their own home, using their own computers, and with no third party moderator present. While traditional lab testing focuses of gathering rich and detailed information, remote usability testing aims to test with large numbers of users and produce statistically significant results. It is recommended that remote testing is not conducted in isolation, and face to face testing is also completed. As there is no moderator present, special software is used in order to record the user’s interactions with the website/online service. Each session usually includes tasks being given to see how users interact with the website/online service and are followed up with a series of questions about their perceptions and how easy it was to complete the tasks. Remote usability testing can be used to test both website content, and online services. Testing content normally involves people completing task based on the online content, while online services are normally tested by asking users to complete a task using the online tool or transaction e.g. Jobseekers Allowance, Driving Licence etc. Success is measured on whether the user can complete the tasks/transaction. Benchmarks on new and existing products are gathered so that completion rates (and other success measures) be collected and performance monitored. The key advantage of remote usability testing are the following: Although remote usability provides testing with large numbers of people, findings can lack depth as they focus on what people do, and not why they have done it. When testing new products the tasks can seem artificial, and this is increased when testing is conducted with a panel of users, instead of real users on a live site. Remote usability testing should not be used in isolation and it is recommended that it is used in conjunction with face to face testing with real users.  In order to test new products that are still in development it is necessary to engage an online panel in order to recruit participants. Existing products and services can be testing via the live website. It is recommended that 400 responses are collected so that results are robust. If users are recruited via an online panel the cost will be approximately £1.5k - this is for general internet users. Costs will increase if hard to reach audiences are required, possibly as high as £3k.  When using a user panel the fieldwork normally takes four to five days, with an additional five days for analysis and reporting. Testing on a live site is usually longer as participation is not incentivised.  An example from the second round of Inside Government usability testing (PDF, 2MB)","description":"This guidance talks about how remote usability testing can be used to provide user research that can feed into product and service design.","link":"/service-manual/users/user-research/remoteusability.html"},{"title":"Same day user testing (online qualitative)","indexable_content":"This guidance talks about how same day user testing can be used to provide user research that can feed into product and service design. Like standard remote usability testing, same day user testing also takes place outside the lab with users participating in their own home, using their own computers, and with no third party moderator present. Where standard remote usability testing focuses on large numbers, and providing statistically significant findings, this type of testing enables you to get rich, qualitative data from respondents that would normally only be possible by observing users in person. As there is no moderator present, each session is recorded via screen capture software, that records the whole session, including the user talking about what they are doing, why they are doing it, and how they feel about it etc. All interviews are conducted in one hour, with the video sessions, and the answers to the follow up questions made available immediately.  Analysis is then conducted in house. Rapid 1:1 testing can be used to test both website content, and online services. Each session involves participants completing a series of online tasks to see how they use the website or tool to find information and/or complete transactions. Each participant is then asked a few follow up questions focusing on overall satisfaction with the site and the experience it provided. The key advantages of remote usability testing are as follows: Although this type of remote usability testing can provide depth of content, the lack of moderation can result in respondents veering off topic and the analysis be quite lengthy. Also, respondents can sometimes appear to be ‘professional testers’, and therefore can perform tasks with greater ease than other users. Testing is normally conducted using the online panel that is available through the panel company. As with regular lab based testing, it is recommended that you test with 8-12 users. This type of research is very cheap - approximately £150-£200. Set up, fieldwork, and analysis can be achieved in one day. This has been used to test the User Strategy website.  ","description":"This guidance talks about how same day user testing can be used to provide user research that can feed into product and service design.","link":"/service-manual/users/user-research/samedayusertesting.html"},{"title":"Survey sampling methodologies","indexable_content":"When conducting quantitative research it is essential that all findings are statistically valid so that there is confidence in the findings and inferences can be made from the sample to the population. This process of collecting information from a sample is referred to as ‘sampling’ and enables researchers to understand the views, and needs of a user base, without interviewing the whole user population. Consideration needs to be given to how the sample is collected (who, how, where, when) and the size of the sample collected for each study. A large sample size can be more accurate and provide greater confidence in the data collected, however a large sample is not required for all research projects. The sample method chosen should consider the size and scale of the project, the sub-group analysis required, and balance the robustness of the approach that with the time, money, and resource available to ensure its fit for purpose. Sampling can be a complex process to understand, but on most occasions a sample of 400-500 will be sufficient for most in-house studies - again this is dependent on the level of subgroup analysis needed and also the penetration of the target group in the population. The greater the number of population groups that need to be analysed may increase the sample size needed, as typically for findings to be statistically valid a minimum of 100 in a subgroup is required. A larger overall sample of 1000+ is normally required if findings need to be nationally representative.  This guide to sampling was written by the National Audit Office, and although it was first published in 2001, still provides a helpful introduction to sampling methodologies.","description":"When conducting quantitative research it is essential that all findings are statistically valid so that there is confidence in the findings and inferences can be made from the sample to the population. This process of collecting information from a sample is referred to as ‘sampling’ and enables researchers to understand the views, and needs of a user base, without interviewing the whole user population.","link":"/service-manual/users/user-research/samplingmethodologies.html"},{"title":"Sentiment analysis","indexable_content":"Sentiment analysis is a method used to analyse high volumes of verbatim comments from users in order to help easily understand the attitude and tone of users’ comment. The method is very simple and uses tailored software to analyse user comments and structure them in a manner that can be used to understand ‘sentiment’ towards a product or service. This enables positive and negative comments to be grouped so that actions can be assigned to resolve problems and issues raised by users.  Some of the free tools available can group comments very broadly, and not enable the level of granularity that enables the analysis to be useful and actionable. Furthermore, comments can sometimes be analysed incorrectly. This is especially the case when slang is used, and phrases are not meant literally, and negative words/phrases are meant positively. This is a notoriously difficult technique for anything beyond broad statements, and requires a very large sample size both of seed (already analysed) material and of commentary from each user. It’s not much use on twitter comments, for example, because they’re so short.  Feedback can be gathered from any user feedback point e.g. email, website, survey.","description":"Sentiment analysis is a method used to analyse high volumes of verbatim comments from users in order to help easily understand the attitude and tone of users’ comment.","link":"/service-manual/users/user-research/sentimentanalysis.html"},{"title":"Survey design","indexable_content":"The abundance of free survey tools makes it cheap and easy for user research teams to produce their own surveys. However, thought still needs to be given to the survey design, understanding the goal(s) of the survey , and how the results will be used. In order to increase the effectiveness of the survey it is important to include an introduction that explains the purpose of the survey to potential respondents, and also provide some reasons that will make them want to take part e.g. the results will be published, their feedback will help improve the website etc. As it’s unlikely that it will be possible to include monetary incentives, it’s important the introduction is worded in a manner that increases the likelihood that people will want to take part. It is also recommended that people are informed of how long the survey will take to complete as this will reduce drop out rate (people who start the survey but don’t finish). Questions Screener questions should be placed at the beginning of surveys to ensure that the correct people take the survey e.g. demographic information, reason for visiting etc. Quotas may also be set so that you can control the number of people taking the survey that fall into a specific demographic group or audience type. A typical user survey will be structured in the following manner: Common survey question types Surveys are normally composed of the following types of questions: Single response (these type of questions allow the respondent to select just one answer) e.g. Q. What is your gender? Multiple choice questions (respondents can select more than one answer) e.g. Q. Why did you visit more than one area of the site today (please tick all that apply)? Open ended questions (respondents can write a text response) Q. You rated ‘ease of using the site’ as fair or poor. Please tell us why you gave this rating? Scale questions (respondents are asked if they agree/disagree with a statement) Q. The site was easy to use Once you have decided on the questions that you want to be included, it is necessary to set up the question logic so that respondents are routed through the questionnaire correctly. It is recommended that all surveys are tested thoroughly to ensure the correct data data is collected. GOV.UK Survey (PDF, 213KB) University of Leeds - Guide to design of questionnaires","description":"The abundance of free survey tools makes it cheap and easy for user research teams to produce their own surveys. However, thought still needs to be given to the survey design, understanding the goal(s) of the survey , and how the results will be used.","link":"/service-manual/users/user-research/surveydesign.html"},{"title":"User research briefs","indexable_content":"A Research brief is a document that is written to explain research requirements and enable research to be procured via third party agencies. A research brief will outline the research objectives, and why the work is required. Agencies respond to the brief and recommend suitable ways to conduct the work, and address the research objectives. A Research brief should contain the following information: Background – this should include a background of the organisation that wants to conduct the research, what relevant research has been conducted previously (or is being conducted), and the overall need for the piece of research that’s being procured. Business objectives - this should include an explanation of the business objectives that the research is addressing. This is essential as it explains why the research is required by the business. Research objectives - the research objectives should outline what the research is trying to achieve. Target Audience/Participants - this should identify who you want to talk to, and focus on the target audience for the product/service. It is also common to include key demographic information, and where, geographically, the research is to be conducted. Preferred Approach/Methodology - it is recommended that research agencies consider the objective of the research and recommend the most suitable methodology, however, it is common practice for in-house research teams to provide an idea of how they think the research could be conducted. Cost - this section should state the budget available for the research. It is suggested that research agencies should be asked to provide a range of price options, so that insight teams can choose the most appropriate approach for their needs. Deliverables - this should outline how the research is to be delivered e.g. presentation, workshop etc. Timings - this should specify when the research is to be delivered. Project team - this section should specify who is on the project team and their contact details. IDA research brief (PDF, 177KB) Working with consultants and specialists","description":"A Research brief is a document that is written to explain research requirements and enable research to be procured via third party agencies. A research brief will outline the research objectives, and why the work is required. Agencies respond to the brief and recommend suitable ways to conduct the work, and address the research objectives.","link":"/service-manual/users/user-research/userresearchbriefs.html"},{"title":"User research surveys","indexable_content":"This guidance looks at how user surveys can be used to provide user research to help design new products and services. Surveys can be conducted either face to face by a trained moderator or through self-completion (online, postal etc.) in order to quantify thoughts/beliefs, behaviours, ownership etc. Surveys comprise of closed ended questions with fixed responses such as ‘yes’, ‘no’, ‘very satisfied’ to very dissatisfied’, ‘excellent’ to ‘poor’ etc, and open ended questions that allow respondents to provide verbatim responses. A range of survey methods and delivery channels are available including: Surveys can be a relatively cheap and quick way of gathering and quantifying public opinion and monitoring change over time. Online/email surveys are normally the cheapest method as these can be conducted in-house using inexpensive/free survey software, inviting people to take part from their own website, or using a database of email addresses. Although sometimes a small cost may be incurred if an incentive is used (e.g. competition to win vouchers etc.). Telephone and face interviews are the most costly methodology and they are normally conducted via a third party agency. Cost can, however be kept down by conducting interviews as part of an Omnibus survey which involves questions from a variety of subjects (from a number of clients) being asked during the same interview, as well as the demographic information on each respondent. Omnibus surveys can be conducted online, telephone, and face to face. Surveys can be used to size a market to find out how many people own a product, or take part in an activity e.g. how many people use the Internet in the UK. They can be also used to understand usage and attitudes towards a product or activity e.g. how often people use the Internet, how they access it, why the use it etc. Complex analysis can be also be conducted to understand key drivers of attitude/behaviour, segmentation of audiences, and trade offs between different opinions etc. Surveys provide a snapshot of opinion, and are a useful method for monitoring uptake, usage and attitudes over time. Although surveys can be a quick and cheap manner in which to conduct user research, there are certain aspects, which should be considered when they are used: All surveys, regardless of methodology should use a robust sample that is nationally representative/representative of audience of interest. Most nationally representative surveys are 1,000 people or more, but can be significantly larger if there are a number of distinct user groups that are being targeted. It is important that the delivery channel used considers the target audience. For example in the recent Digital Landscape research an online and face-to-face methodology was used as online and offline audiences needed to be interviewed - an online only methodology would have excluded those who are offline, and the opinions on this audience omitted from the research It is also important to note that reaching hard-to-reach groups can be more time consuming and costly (e.g. BME, people with disabilities). The costs can vary depending on scale, reach and method – as detailed previously, online surveys are generally cheap, while face to face interviews are more costly. Cost ranges from free (just in-house resource) to £100k+.  Again this depends on research method and reach. Face to face interviews are more time consuming, and typically take 6-8 weeks from assigning an agency to delivery of results.","description":"This guidance looks at how user surveys can be used to provide user research to help design new products and services.","link":"/service-manual/users/user-research/userresearchsurveys.html"},{"title":"User research tools","indexable_content":"This section looks at research tools that are available (free and subscription) to enable users research teams to conducts research projects in-house. There are number of good tools that can help user research teams conduct in-house research quickly and effectively. These are listed below Survey tools - These enable the easy creation of online surveys. Features include scripting tool that allows easy creation of different question types, branding, advanced branching, automated reporting, automatic activation/pop script, email links etc. Some good tools are available free, however, these often have restrictions on the number of questions you can ask, the number of completes you can collect, and advance features such as API, and branding. Available tools are below: Free tools - Survey Monkey, Survey Expression, Subscription tools - Smart Survey, Fluid Surveys Remote user testing tools - These tools enable user research teams to conduct remote usability testing in-house. These tools allow you to script the surveys easily, while also conduct the analysis to be conducted relatively simply and quickly. Most of these tools also enable you to conduct standard online surveys within the same software package. Available tools are below: Keynote Webeffective, User Zoom Card sorting tools - These tools enable user research teams to conduct online card sorting in-house. Card sorting tools usually come as part of a larger survey package. Available tools are below: User Zoom, Optimal Workshop Online focus groups - Enable in-house research teams to conduct focus groups online. Most also enable you to run 1:1 sessions, auto ethnography, diary studies, and micro communities. Available tools are below: LiveMinds, VisionsLive Community groups - Enables in-house research team to manage a community of users and/or stakeholders. These normally come part of a larger online research tools package. Available tools are below: LiveMinds, Bloomfire Text analysis tool - Enables the analysis of large amounts of written feedback - via email, helpdesk etc. This software enables you to sort user feedback into themes, and make it easier to action. Available tools are below: Feedback Ferret, Atlas Online tools are a good way in which to conduct in-house research cheaply and effectively. Conducting research in-house enables user research teams get closer to the data, and have a good understanding of the user. This is, however, dependent on teams having adequate resource to enable analysis to be conducted properly and fed back to the relevant teams. Using online research tools in-house can be cheaper than assigning a third party agency to conduct the work, however, as pointed out above, this is dependent of user research teams having adequate resource. Participants can be obtained internally through websites, emails, and other user channels, or via third party sample companies. Some tools are free, but licences for others can range from £15k to 20k. Research can be conducted quickly as little set up time is required.","description":"This section looks at research tools that are available (free and subscription) to enable users research teams to conducts research projects in-house.","link":"/service-manual/users/user-research/userresearchtools.html"}]
