[{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Accessibility testing","indexable_content":"Carrying out accessibility testing When not to use it Accessibility audits Types of participants Cost Timescales screen readers voice recognition software trackball devices cognitive and learning disabilities, eg dyslexia or attention deficit disorders visual impairments, eg total and partial blindness, colour blindness, poor vision auditory disabilities, which can also affect language motor skills impairments, eg those affected by arthritis, strokes, RSI they’ll often have things set up to suit their individual needs it’ll make the whole process less stressful for them, eg not having to travel or change environment real content is in place, rather than ‘dummy text’ – this is so it can be properly assessed by those with any cognitive or learning difficulties interactive elements, such as calls to action, hyperlinks, forms etc are in place – this is so it can be properly assessed by those with motor skill impairments participants are travelling to your testing location participants require specialist assistance with carers or travel recruitment via an agency can take up to 2 weeks, depending on the target audience testing sessions can take between 2-3 days depending on the number of participants analysis and reporting will take up to a week Accessibility testing is very similar to usability testing, in that it is about making sure that a product or service is easy for its intended audience to use. That audience includes users who access the service via a range of assistive technologies like: It’s important to consider a range of disabilities when you are testing any product or service, including those with: The Equality Act 2010 came into force in 2010 and is intended to stop discrimination against members of society who may be unfairly treated due to age, disability, race, religion or sexual orientation. You have a duty to make adjustments for disabled persons so that they’re not excluded from your service, making every reasonable attempt to provide access. The Royal National Institute of Blind People (RNIB) has a useful guide to the UK law and how it affects the accessibility of your site or service. Most accessibility testing is typically done after an accessibility audit has been conducted. If a participant has a range of needs it’s best to do accessibility testing in the participant’s own home. This is because: It’s often difficult to carry out accessibility testing early on in the process of service design. Your service must be fairly stable for it to be evaluated by people using assistive technologies, eg a screen reader will read out the contents of a web page, so the code needs to be well structured. Only do accessibility testing when: Full lab-based accessibility testing is not necessary for every project. An accessibility audit may be a more efficient and cost-effective way to review your service, depending on your typical user needs. Accessibility audits are an alternative to standard accessibility testing. An accessibility audit involves an accessibility expert reviewing your site or service, highlighting all accessibility issues and making recommendations for fixing them. They would typically use assistive software used by disabled web users (eg a screen reader) to effectively carry out the audit. See the W3C accessibility guidelines for further information. Accessibility audits are cheaper and quicker than accessibility testing, but rely primarily on the expertise of the person doing them. Include disabled participants as part of a wider user testing recruitment process. The numbers will be small, but aim to capture a range of disabilities and assistive technologies. Disabled participants are a harder to reach audience, so the cost of recruiting participants can be relatively expensive. Recruit participants through specialist organisations or agencies like the RNIB. Expect costs to increase if: Timings will vary, eg whether sessions are lab-based or structured in a ‘home environment’, but generally: These estimates are dependent on your project’s scope, and the availability of testing assets.","description":"Accessibility testing is very similar to usability testing, in that it is about making sure that a product or service is easy for its intended audience to use. That audience includes users who access the service via a range of assistive technologies like:","link":"/service-manual/user-centred-design/user-research/accessibility-testing.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Accessibility skills","indexable_content":"Accessibility leads Related links strong analytical skills a methodical approach to testing empathy for users with different, often contradictory needs an in-depth knowledge of modern accessible development best-practices the ability to communicate at all levels Accessibility is everyone’s responsibility. Provide training and resources to help your team build accessibility into everything they do. That includes planning, designing, building and managing. Include at least one person with strong accessibility knowledge in your team. Give them overall responsibility for educating and supporting your team’s accessibility goals. The key skills required by an accessibility-focused developer are: Accessible, usable products are the heart of everything we do at the GDS. Your service needs to be just as thoughtful to the needs of all possible users so that no one is excluded on the basis of disability. Read the guidance on accessible design","description":"Accessibility is everyone’s responsibility. Provide training and resources to help your team build accessibility into everything they do. That includes planning, designing, building and managing.","link":"/service-manual/the-team/accessibility.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Addresses","indexable_content":"1. Free text box 2. Multiple fields 3. Address finder On this page: Guidance Guidance Guidance Free text box Multiple fields Address finder it can handle any possible address format people can copy and paste addresses from the clipboard people don’t have to work out which part of the address goes in which field parsing addresses for sub-parts (region, street etc.) is hard, and impossible to do with 100% accuracy many legacy back-end systems expect multi-field addresses you can easily extract the parts of an address and do things with them you can give help for or validate each part of the address separately works well with browsers that have auto-complete enabled hard to find a single format that works for a broad range of regions no guarantee that people will use the fields as you intended can’t easily paste addresses from the clipboard ‘postcode’ is one word let people enter postcodes with or without spaces avoid making individual fields mandatory (but warn users if they don’t fill in any fields) Royal Mail do not need a county as long as the town and postcode are included include a county field though – it lets people who don’t know the postcode give a valid address make the field lengths appropriate – it helps people understand the form people entering UK addresses don’t have to enter as much information reduces the chance of mis-typed UK addresses requires greater effort to implement make it clear that the address finder only works for UK addresses provide a manual option for people with international adresses or addresses that are missing or badly formed in the Royal Mail database If you need to ask for an address on a form, consider where the addresses will be from and what you need to do with the data. A single, multi-line text box where users write out the address in full. The good: The bad: Use when you’re expecting a very broad range of address formats and you don’t need to use specific sub-parts of the address. The address is broken down into multiple fields. Here’s an example that works for simple UK addresses: The good: The bad: Only use multiple address fields when you know which regions the addresses will come from and can find a format that supports them all. UK addresses: Sometimes referred to as ‘postcode lookup’. An address finder lets users specify a UK address by inputting their postcode (and optionally street name or number) and selecting the address from a list. Here’s how this pattern was implemented on the Lasting Power of Attorney service. The good: The bad: Discuss this page on Hackpad","description":"If you need to ask for an address on a form, consider where the addresses will be from and what you need to do with the data.","link":"/service-manual/user-centred-design/resources/patterns/addresses.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Alpha and beta banners","indexable_content":"When to use the alpha and beta banner Get the banner code Things on the service.gov.uk subdomain Things on GOV.UK This guide explains when and how to use the GOV.UK alpha and beta banners. You have to use an alpha banner if your thing is in alpha, and a beta banner if it’s in beta. If your service is in beta or alpha, you must show the relevant banner in the header. It should sit directly underneath the black GOV.UK header and colour bar, as in this example: If your GOV.UK content is in beta or alpha, show the relevant banner below the page title, but above the body content. You can see what they should look like in this blog post. All the code and assets you need to implement the alpha and beta banners are available via the GOV.UK frontend toolkit on GitHub. Discuss this page on Hackpad","description":"This guide explains when and how to use the GOV.UK alpha and beta banners.","link":"/service-manual/user-centred-design/resources/patterns/alpha-beta.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Alpha phase","indexable_content":"The objective of an alpha What should be in your alpha Alpha phase duration Team requirements Outputs An ideal alpha gain greater understanding of a service test design approach test some technologies begin to form the team gain shared understanding of the service at a coding and integrations level understand what or who you will need to deliver a beta is the solution appropriate? is your approach viable? do you have enough understanding of your users’ needs to meet them? high level story cards plan for beta and running of the live service (decreasingly detailed) working basic system that provides limited functionality that can be shown to a number of users understanding around legacy systems to replace or wrap or integrate with cross-functional requirements decision to progress to beta phase final analysis on the research you have commissioned on user needs options for the assisted digital support for your service Next phase: beta Previous phase: discovery When designing a service it’s impossible to predict everything upfront. Each project features many challenges, and in your alpha you will start exploring solutions for these. You may need to bring more developers and designers into the team, who will help you to build and test prototypes and possible solutions for your users’ needs. By the end of the alpha you should have a clear idea of what’s required to build a beta. The whole phase should not last longer than about 6 to 8 weeks. The objective is to build a working prototype. This will be used by stakeholders or a closed group of end users to: Continue to build upon and analyse the research you have commissioned on user needs and use this to set up an open, engagement process with your stakeholders. Involve a wide range of stakeholders from the private, voluntary and other parts of the public sector. Run a series of workshops with these stakeholders to develop your options. Following demonstration of your alpha, you may choose to discard the code and start fresh in the beta. If, however, your code is effective you may continue to iterate against your prototype. Develop options for the assisted digital support for your service. To help develop the options, continue to build upon and analyse the research you have commissioned on user needs. Bring in the expertise of organisations working with people who are offline and users themselves. Run workshops to develop your options. The alpha doesn’t need to be a complete, end-to-end transaction. You’re looking to demonstrate just enough so users gain some understanding of the service. Think of it as a proof of concept: If not, find out more and make a new prototype. The alpha phase is another relatively short phase. At GDS, we try to limit these to about 2 months, running in week-long sprints over a 6 to 8 week period. This phase involves a relatively small core team, who will be capable of rapidly iterating solutions. It will probably expand and contract in size as different specialisms are required. This core team will be a mix of stakeholders, and makers (designers and developers) particularly those familiar with user research. It will be led by the service manager. The outputs for the alpha phase are: For a worked example, we have written up some information on an ideal alpha.","description":"When designing a service it’s impossible to predict everything upfront. Each project features many challenges, and in your alpha you will start exploring solutions for these.","link":"/service-manual/phases/alpha.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Analytics tools","indexable_content":"High-level requirements Privacy Vendor comparison Configuring analytics tools Further reading the total cost of ownership, as well as the cost in comparison to turnover of service the volume of data being sampled who owns the data (it should be your organisation!) the cost of additional profiles and/or custom variables the admin system users have access to whether it is hosted by the vendor or in-house whether it tracks offline channel usage whether it provides a comprehensive set of standard reports (including social interactions and multimedia capturing) if it can measure transactions through funnel analysis and measure goals support and training the cookies it requires won’t collect and process any personal information (the terms and conditions of your analytics provider will probably expressly forbid you from doing this) turn off any data sharing (some suppliers may collect data anonymously for internal benchmarking) anonymise IP addresses that your analytics provider collects by removing the last octet of the address meets the EU privacy directive and the European Commission’s Directive on Data Protection has data centres that meet EU/British data security standards allows you to restrict your vendor’s employees access to your data as appropriate – also make sure there are adequate administration tools to control appropriate access for your own staff allows you to own export analytics data (pay attention to terms and conditions for any free products) where collected data is held how long data is held for? what happens to the data on termination of the contract - can you export it? if your vendor mines data (analyses data from different perspectives, turning it into useful summaries of information) for cross-customer benchmarking/trends or to provide usage data to any advertising channels have you installed web analytics software? have you configured your web analytics software with the appropriate conversion funnels? do you have the capability to run user satisfaction surveys? do you have the capability to do A/B testing and multivariate testing? AboutAnalytics Web Analytics Tools Comparison: A Recommendation - old, but thoughtful Analytics tool comparison Enterprise Web Analytics: A Buyer’s Guide Occam’s Razor by Avinash Kaushik has a wealth of useful information on analytics that’s easy to understand Web Analytics Demystified and the Digital Analytics Association have free whitepapers Conversion Funnels What is a Conversion Funnel? Blog post by Morgan Brown with a good discussion on user flows and conversion funnels There are various web analytics tools available to help you measure how people are using your service. Be sure to assess how well a particular tool meets your needs before deciding on which tools to use. It’s essential that your chosen analytics tools provide an open Application Programming Interface (API) with no restrictions on exporting data. Also, when deciding which analytics tool is most appropriate for your service, consider: For each of these criteria, identify which ones are fulfilled as part of the standard quoted package and what is charged for any additional features. The privacy and security of data is of absolute importance, so review your vendor’s privacy and security policy. Make sure your analytics solution and processes: As part of keeping your users’ data secure, find out: There are many digital/web analytics vendors in the marketplace, along with open source solutions. A search for ‘analytics tools comparison’ provides a number of useful resources where you can compare the capabilities and strengths of different services. See some more examples. Install and configure analytics tools that meet your needs. Where possible, use platforms that enable the data to be piped automatically into other systems. Use APIs (Application Programming Interfaces) as they will stop you from having to input data manually and allow the grouping of data across multiple platforms. You will need to answer the following: Sites that provide vendor comparison information: Sites that provide information on digital analytics: Sites that provide information on funnels (important for transactions):","description":"There are various web analytics tools available to help you measure how people are using your service. Be sure to assess how well a particular tool meets your needs before deciding on which tools to use.","link":"/service-manual/making-software/analytics-tools.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: APIs","indexable_content":"Guidance Consuming and using APIs Further reading Build an API by building with the API Just use the web Give each thing a bookmarkable URL Use HTTP methods as Tim intended Representations are for the consumer Names reinforce conventions Document by discovery… and example Explicitly set expectations Be public by default Practice service evolution Code Integration Testing Service agreements and resilience JSON for convenient processing in most programming languages JSONP and JSON with CORS for client-side JavaScript CSV for importing into spreadsheets Atom for feeds iCalendar for events vCard for name and addresses KML and geoRSS for geographical data m3u for playlists API Craft is a reasonably active public forum for discussing publishing APIs The Open Web Application Security Project (OWASP) maintains a large repository of security information applicable to building APIs, including a REST Security Cheat Sheet The White House are developing API standards which are largely compatible with this guide Martha Lane Fox’s report called for government to act as a “wholesaler, as well as the retail shop front, for services and content by mandating the development and opening up of Application Programme Interfaces (APIs) to third parties.” This section is a set of guiding principles for exposing a digital service as an API. When building an API there is always a danger of building the wrong thing in the wrong way for the wrong people. This is especially a risk in the absence of a developer community driving the needs behind the API. The simplest way to ensure your API is useful and consumable is to build a website using your own API. Building a website leads to considering how to best model content and data in terms of bookmarkable resources, and ensures data is presented in human as well as machine-readable representations. Becoming a consumer of your own APIs not only validates your API, but exposes services on the web. Consider an API to be a part of a website. Provide links to machine-friendly formats from human readable pages, and enable agents to easily construct URLs which link to human-friendly representations of pages. Use standard formats for content, and follow established web patterns for authentication. Building a service to enjoy mass adoption and support from a wide, disparate community of developers and programming environments while being able to reach a worldwide audience is difficult. While proprietary and open technologies abound for machine-to-machine communication, none of them have the web’s interoperability, reach and ability to scale. Standards are powerful agreements, and nowhere are agreements more quickly established and adopted than on the web. Using HTTP (Hypertext Transfer Protocol) and URLs (uniform resource locator), the core technologies of the web, along with emergent standards such as JSON and OAuth changes a website from a retail shop window into a wholesaler, meeting our design principle to build digital services, not websites. Expose data as a set of resources, offering a clean URL for each thing, and each collection of things. Only use query strings for URLs with unordered parameters like options to search pages. Consider creating URLs for different granularity of resources. For example, /members.json could return a list of names, whilst /members.json?detail=full could return detailed information about each member in a list. These principles enable network effects which arise through linking and allow information published beyond the web, sent in alerts email, SMS, XMPP and other messages, to link back to the canonical content on the web. Ensure all HTTP GET requests are safe and actions which change state are conducted using a POST, PUT or DELETE method. Use PUT and DELETE with caution, as they are commonly blocked by firewalls, intranet proxies, hotel Wi-Fi and mobile operators: always offer a POST alternative. Avoid HTTP methods which are not well defined, such as PATCH. Offer content for each thing as a human-readable HTML, with links to content in alternative machine-readable representations: Where possible, also offer other formats most suited to a specific domain, such as: This advice builds on our more general guidance on data and content publication formats. Include hyperlinks to alternative representations as link headers as well as in content. Consider also encoding metadata inside HTML content using semantic markup: Microformats, RDFa or schema.org. The representations supported by an API for input will vary depending upon the complexity of the action, but where possible should include application/x-www-form-urlencoded to allow the construction of simple POST forms. Use names for fields, formats and path segments in a URL path consistently across your API. Establish conventions others may easily follow and anticipate. Where possible, reuse names widely used elsewhere on the web, as with the Microformats naming policy. Building a website which exposes the data through links, and services through HTML forms encourages exploration and leads to discovery through hypertext. Provide documentation for your API using examples. Collect how people are using your API, especially link to any open source projects for projects, wrappers and programming language libraries. Provide simple ways to experiment, as with The Guardian API explorer. Be clear in the web pages and other documentation as to the security, availability, rate-limiting, expected responsiveness of the platform and the provenance of data, so consumers can plan their commitment to using your API. Lower the barriers to others using your data: don’t demand registration or API keys for public data. Open data increases the number of people able to use your data and service, and leads to feedback loops where consumers become motivated to resolve issues at source, feeding back issues and correction to your service and its data. Where content is sensitive, or requires authentication, use HTTPS encryption (Hypertext Transfer Protocol Secure) and a standard authentication such as basic authentication or OAuth, depending upon the sensitivity of your content. Build for forwards compatibility by gracefully handling content that is unexpected. The robustness principle – Postel’s Law – explains the ability for the web and internet to evolve, though you shouldn’t ignore protocol errors, corrupted, or invalidly formatted content. Preserve backwards compatibility with existing consumers of your API, by sending expected fields and employing sensible default values for missing fields. Eschew changes to the semantics of content, eg don’t change a title field from meaning the title of the page, to meaning the prefix for a name to the person’s job title. Where a revolutionary change is unavoidable, communicate a breaking change by changing the URL. When changing URLs, continue to honour old consumers, possibly use a redirection. Cool URIs don’t change. Don’t do everything yourself (you can’t). Sometimes the functionality your service needs will be provided by other parts of your organisation, other government departments or by reliable third parties via APIs. Most modern digital services are built on top of a wide range of APIs. This allows each part of the service to focus on its core responsibility rather than constantly reinventing the wheel. When consuming APIs you should be careful to keep the integration with your code clean and distinct. This is important to ensure that you can swap between providers or update to new versions of an API without making substantial changes to your core code. At GDS we encourage the use of adapter code that’s entirely focused on interfacing with the system and mapping code. This will provide the linkage between your code’s domain model and the concepts and services provided by the API. You should consider carefully how you intend to test your integration with the service. In day-to-day development you’ll want to be able to test your code without making (computationally or potentially financially) costly calls out to third party services, so you should come up with a way of providing mock versions of those APIs. For full system tests, however, you’ll want to test the full flow including the third-party service so an automated mechanism should be built for that. Many of the GOV.UK publishing applications send emails to provide alerts for content designers. When running tests we don’t want to send lots of fake emails so we swap the normal email adapter for one that logs the emails it would have sent. This lets us test our code is doing the right thing without depending on external services. Parts of our Performance Platform code involve significant interactions with Google Analytics. It wouldn’t be practical to test this by sending events to Google, waiting for them to be processed, and then reviewing the results. Our developers therefore built a mock service that can be run alongside tests and provides a dummy version of Google’s API that lets us check the right data is being sent. Our publishing systems make use of a single sign-on service. In most of our tests the interaction with that service are mocked so the applications’ tests can be run in isolation, but we also have a suite of smoke tests that run in our preview environment and use dummy accounts to ensure that the full authentication and authorisation flow is working. The Licence Application Tool integrates with a number of third-party payment services. It makes use of test accounts with those services to verify it is able to communicate with them and is sending the right data to complete payments. By depending on a third party API you could very easily be tying your service’s availability to that of the third party. In some cases that may be acceptable, but often you will want to ensure that you have a fallback plan in place. The details of that fallback will vary according to your service. It may be that you’ll need to offer the user the opportunity to use an alternative service, or queue the action to take place later. That could be an automated queue with software that monitors it and retries transactions, or it could be a manual queue where someone follows up to collect further details. You should be clear with your users about what’s happening. If a third-party payment provider isn’t available you might queue the transaction to try again later. That will mean you can’t offer users the same guarantee that their payment will be processed correctly and you should tell them so.","description":"Martha Lane Fox’s report called for government to act as a “wholesaler, as well as the retail shop front, for services and content by mandating the development and opening up of Application Programme Interfaces (APIs) to third parties.”","link":"/service-manual/making-software/apis.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Asset hosts","indexable_content":"Asset hosts To provide the fastest possible experience for our users it’s usually a good idea to serve assets (stylesheets, JavaScript, images) from a separate domain name. That will enable the web browser to fetch more elements of the page in parallel, and also make it straightforward to remove cookies and other extraneous information from the HTTP responses for assets. You should not use the asset host that we use for www.gov.uk to load your assets. The files provided there are only guaranteed to work for www.gov.uk and could change without notice in ways that would break other services that are using them. Instead you should host any assets you rely on within your service. We recommend a hostname such as:","description":"To provide the fastest possible experience for our users it’s usually a good idea to serve assets (stylesheets, JavaScript, images) from a separate domain name. That will enable the web browser to fetch more elements of the page in parallel, and also make it straightforward to remove cookies and other extraneous information from the HTTP responses for assets.","link":"/service-manual/domain-names/asset-hosts.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Assurance for digital services","indexable_content":"Who’s involved in assurance How assurance is done review each others’ work challenge and refine ways of working through transparent communications reflect on how the service could be improved in the light of feedback departmental assurance teams specialist reviewers, eg technical, legal, content, design  the Major Projects Authority — mandatory for certain projects the Government Digital Service (GDS) — via Digital by Default Service Standard assessments (mandatory for services with more than 100,000 transactions per year)      don’t slow down delivery — assurers need to work at the same pace as delivery teams, using the data teams are already producing to manage delivery         decisions when they’re needed, at the right level — assurers should have the authority to make decisions and provide recommendations to solve problems before they affect delivery         do it with the right people — assurers need recognised skills and experience in what they’re assuring so they can be effective and credible         go see for yourself — face-to-face communication with delivery teams and people who govern is the most effective way of assuring         only do it if it adds value — assurance should be focused where it can add the most value and this will be different for each service         trust and verify — mutual trust between assurers and those they assure is vital    Assurance is a range of activities and processes put in place to make sure services meet user needs and organisational goals. It’s important because it helps successful delivery of services by improving people’s understanding of progress, helping them make informed decisions about the work being done. Assurance is an ongoing process that helps maintain quality through the life of a service. Everyone working on a service has responsibility for assurance. It’s built in to Agile ways of working with regular checkpoints and opportunities for feedback that help keep governance and delivery work on track. This self assurance gives teams the opportunity to: This way of working helps assure the quality of the services being delivered and provides reassurance to those responsible for governance, helping to build trust. Sometimes, services benefit from an independent view provided by someone not directly involved in day-to-day delivery. This provides extra confidence that everyone is seeing a clear picture of progress. It can also be useful where there are multiple teams with interdependencies or higher levels of investment, complexity and risk associated with the service.  External assurance could be done by: Read more about assurance from those outside the service team. Assurance should be done in line with the governance principles for digital services: don’t slow down delivery — assurers need to work at the same pace as delivery teams, using the data teams are already producing to manage delivery decisions when they’re needed, at the right level — assurers should have the authority to make decisions and provide recommendations to solve problems before they affect delivery do it with the right people — assurers need recognised skills and experience in what they’re assuring so they can be effective and credible go see for yourself — face-to-face communication with delivery teams and people who govern is the most effective way of assuring only do it if it adds value — assurance should be focused where it can add the most value and this will be different for each service trust and verify — mutual trust between assurers and those they assure is vital More on assurance for digital services Self assurance by Agile teams Assurance from those outside the service team Get involved To give feedback, make a suggestion or share your experience, use the governance guidance hackpad.","description":"Assurance is a range of activities and processes put in place to make sure services meet user needs and organisational goals. It’s important because it helps successful delivery of services by improving people’s understanding of progress, helping them make informed decisions about the work being done.","link":"/service-manual/governance/assurance-for-digital-services.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Assurance from those outside the service team","indexable_content":"Assurers from within the organisation Digital by Default Service Standard assessments Cabinet Office spend controls Major Projects Authority National Audit Office provide extra confidence to the delivery team and people who govern that they have a clear understanding of progress identify improvements that could increase the chances of successful delivery look at delivery activity across multiple teams, including governance, and assess how effective it is proportionate to the service phase and scale aligned to the governance principles for digital services based on mutual trust between the team and those involved in the assurance activities open and transparent, with any observations or recommendations available to everyone help ensure the service as a whole is healthy and set up for success take part in coordination activities (eg show and tells or stand up of stand ups — where representatives of each team meet to talk about their upcoming work) to observe interactions and assure that cross-team dependencies, risks and opportunities are being managed properly assess whether underspend is leading to problems with the service delivery or whether overspend means the service can still generate sufficient benefits bring together the outcomes from assurance activities across the service to get a complete picture look for common problems across the service or organisation and help ensure they are addressed observe drift from an overall vision and goals that people who govern and individual teams might not notice at the end of the alpha stage when the service is ready to go to public beta on GOV.UK at the end of beta when the service is ready to have its beta branding removed and be fully live need investment above a department’s expenditure limits introduce new policy that requires primary legislation to be passed are particularly innovative or contentious and may have higher delivery risk Sometimes people not directly involved in day to day delivery of a service can spot things that people in the team are less likely to see because they’re too close to the work. This type of external assurance can: Assurance should be: Assurers should actively seek feedback on their approach and adapt in response to that feedback. Sometimes, assurance is provided by people elsewhere in the organisation. This could be a departmental audit team or specialists in particular disciplines (eg legal, technical, content, design). Where more than one team is involved in developing a service, these assurers are useful because they can: If assurers are involved with the service on an ongoing basis they can add more value than if they’re only involved periodically. They should engage early with the service manager and people who govern. The Digital by Default Service Standard is a set of criteria that has to be met by all new or redesigned transactional government services.  Services with more than 100,000 transactions per year are assessed by the Government Digital Service (GDS). Services with under 100,000 transactions a year are assessed by a team within their own department.  Assessments happen at 3 points in the development of a service: This is an assurance and approval process: if the GDS assessment panel doesn’t pass a service, it won’t be awarded the standard or appear on GOV.UK. After an assessment, the panel provides feedback to the service team, including about where they might need to improve in order to meet the standard. Teams can use the criteria in the standard and the service manual to help focus and prioritise their work. If assurers from within the organisation are available, they can also use the service standard as a basis for their review activities. Under Cabinet Office spend controls GDS assures all technology related spending over published thresholds. You should speak to GDS early on to agree appropriate points for approval and to get advice on shaping your expenditure request.  The Major Projects Authority (MPA)  provides independent assurance on major projects, which are defined as those which: The MPA assures projects through Gateway Reviews or Project Assessment Reviews. Gateway Reviews take place at significant points during a procurement process, whereas Project Assessment Reviews are bespoke, focussed and timed to meet the specific assurance needs of your service.  The National Audit Office (NAO) audits most public sector bodies in the UK and produces value for money reports on the implementation of government policies, which could include your service. The NAO also publishes reports on good practice to help government improve public services. In recognition of the growing importance of Agile delivery of digital services, the NAO published a report in 2012 on governance for Agile delivery, drawing on experiences of governance in the private sector. More on assurance for digital services Assurance for digital services Self assurance by Agile teams Get involved To give feedback, make a suggestion or share your experience, use the governance guidance hackpad.","description":"Sometimes people not directly involved in day to day delivery of a service can spot things that people in the team are less likely to see because they’re too close to the work. This type of external assurance can:","link":"/service-manual/governance/assurance-from-those-outside-the-service-team.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Beta phase","indexable_content":"The objective of a beta How to publish a beta Duration of the beta phase Team requirements Outputs Private beta Public beta gives more control over the audience demographic that gets to use the beta allows you to restrict the volume of transactions that go through the beta lets you start small and get feedback faster before rolling it out to a wider audience use something like AB-testing to funnel some traffic to the beta invite people with a separate call to action to use the beta delivered a (private or public) end-to-end service a collection of prioritised work to be done (your backlog) a user testing plan accurate metrics and measurements to monitor your KPIs tested the assisted digital support for your service a working system that can be used, for real, by end users Next phase: live Previous phase: alpha You’ve tested your solutions to user needs and built up a clear picture of what it will take to build and operate your service. Now you will build an end-to-end service, test it in public and prepare to go live. The objective of this phase is to build a fully working service which you test with users. You’ll continuously improve on the service until it’s ready to go live, replacing or integrating with any existing services. This is achieved by providing the user stories in the backlog created in the alpha phase. This is the time to resolve any outstanding technical or process-related challenges, get the service accredited and plan to go live. You’ll also be resolving technical and process challenges, meeting for the first time many of the technical criteria outlined in the service standard. You should be rapidly releasing updates and improvements into the development environment, and measuring the impact of your changes to the key performance indicators (KPIs) established in your discovery and alpha phases. You’ll also test the assisted digital support for the digital service. You might test one or more of the options you developed in the alpha phase. There are various ways of running the beta phase. In all instances, it should involve interacting with a full, end-to-end version of the service. This is a beta that is not open to everyone – either regional, or invite only etc. You might want to choose this option because it: A public beta is made open to everyone. It can exist alongside an existing version of the service and you might: The exact duration of your beta will depend on the scope of your project, but an appropriately sized team shouldn’t take more than a few months to create a beta. Following the release of your beta you’ll spend some time iterating on the service until it is ready to go live. You’ll now know what size team you need to create the service, scoping it in response to the findings of your alpha prototype(s). It will be run by a single, suitably skilled service manager, and will include designers, developers, web operations specialists and performance analysts as appropriate. At the end of the beta phase, you’ll have:","description":"You’ve tested your solutions to user needs and built up a clear picture of what it will take to build and operate your service. Now you will build an end-to-end service, test it in public and prepare to go live.","link":"/service-manual/phases/beta.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Blogging about a service's progress","indexable_content":"Publish, don’t send Get to the point Little and often Illustrate everything Use the style guidance… …but remember you’re a person Always get someone to read it first Blogs are a simple, effective way of keeping a record of the work a team is doing on a service. Updates can be as short as you like, allowing you to tell readers about subtle changes to a service’s design or new features added to a tool, while also offering you a platform for longer pieces of writing that describe significant changes to policy or approach. The Inside GOV.UK blog and the MOJ Digital blog are great examples of how teams are already talking about their work in the open. A good blog post can do the work of dozens of emails. Finding the right way of sharing a piece of information once – describing a change to how a feature works for example, or talking about upcoming work – can save a huge amount of effort in finding new ways of saying roughly the same thing. More importantly, it means there’s a consistent frame of reference for talking about a subject. If people have been told slightly different things then they might come away with a different understanding of how something works – not something likely to make your life simpler, clearer and faster. Whatever you’re writing about, make sure the most important information is in the title and the opening sentence. Readers may only see the summaries of your posts on the home page, or their feed reader, or in the email alerts, and it’s important they have a clear understanding of what you’re doing. Short updates can still go into a bit of detail about what you’re doing and why you’re doing it, whether that’s changing the colour of a button or re-writing a whole section of guidance. Keeping posts short makes them simpler to write, and also means it’ll be easier to get input from the people you work with. Though the frequency with which you blog will often vary you should aim to publish a new post at least once a week. A well-chosen illustration or photo can add important context or information for your readers. The GOV.UK blogging platform is fairly flexible. You can embed tweets, slide decks and videos as well as images, all media that make your message just a little bit easier to comprehend. The GOV.UK content design guidance is an invaluable tool for writers. They’re full of carefully researched tips for clear, concise prose which seeks to make your writing easier to understand. In particular, make use of subheadings and bullet points within your blog posts to make them quicker to read. And make use of the ‘words to avoid’ list – it’s there to make sure writers stay clear no matter the audience. The style guide doesn’t mean you can’t be warm, candid or personal. You should be all of these things. It’s important that you’re accountable for the things you write. If your name appears on a post then you should be prepared to respond to comments and engage in dialogue both on your blog and on other social networks. It’ll improve your work, and ultimately improve the experience for your users It’s a given that writers will be professional and mindful of things like the Civil Service Code when they blog, but you should always get someone to read your work over before you hit PUBLISH. For starters, they’ll probably spot a few mistake that have slipped your notice, but they might also be able to offer a different perspective on the things you’re writing about.","description":"Blogs are a simple, effective way of keeping a record of the work a team is doing on a service.","link":"/service-manual/communications/blogs.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: CAPTCHA","indexable_content":"Why you shouldn’t use them Alternatives to CAPTCHA Further reading Usability – they put the burden of detecting bots on the user rather than the system. As CAPTCHAs are designed to be hard to read and understand, this makes the service much more difficult to use. Accessibility – they are inaccessible by design. This effectively makes the service unusable by people with certain disabilities. Even CAPTCHAs that provide audio versions do not completely resolve this issue. Privacy – 3rd party CAPTCHA services set cookies, collect analytics and can track users across multiple sites. This introduces significant privacy concerns Performance – use of a 3rd party CAPTCHA service ties your performance to theirs. If their service goes offline, so does access to your service Security – the security of your service is tied to that of the 3rd party. If they are compromised, your service and its users may also be rate and connection limiting use of honey pots protective monitoring CAPTCHA and the BBC Ticketmaster ditches CAPTCHA for something simpler CAPTCHA stands for Completely Automated Public Turing test to tell Computers and Humans Apart. They are used to prevent bots (automated software) from completing a form or accessing a system and usually take the form of jumbled up text for the user to decipher and enter before submitting a form. CAPTCHAs introduce significant problems to online services: Additionally, if a 3rd party CAPTCHA service is used, there are further problems to consider: Many of the risks that CAPTCHAs are aimed to mitigate can be addressed in other ways: It’s important to note that even with a CAPTCHA in place bots will still get through due to advances in computer imaging and the use of CAPTCHA farms. A combination of different approaches generally gives the best results.","description":"CAPTCHA stands for Completely Automated Public Turing test to tell Computers and Humans Apart. They are used to prevent bots (automated software) from completing a form or accessing a system and usually take the form of jumbled up text for the user to decipher and enter before submitting a form.","link":"/service-manual/user-centred-design/resources/captcha.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Card sorting","indexable_content":"How to carry out a card sort Where/how you might use it Weaknesses/when not to use Online tools Closed sorting Card sorting is a research method used to understand the way that the intended users of a website naturally organise or think about different types of information or content. It’s also a method service teams can use to sort and arrange user needs. The method is very simple to use: it involves simply writing the needs or topics on pieces of card (maximum of around 50) and asking representative users to organise these into groups, which are then given a meaningful label. 15 to 30 users are recommended: a minimum of 15 users will give reasonable confidence in the results, but any more than 30 gives diminishing returns. It is also possible to conduct a closed card sort, whereby the users are given pre-determined categories and are asked to allocate each card to one of these categories. Open card sorts are most suitable for identifying how users think about information, in order to build an information architecture. Closed card sorts are most suitable for validating an information architecture. The analysis of card sorting can be difficult if no clear categories emerge. It can also be difficult to conduct a card sort on a very large website with a broad scope of content (eg GOV.UK) since the maximum number of cards may not be representative of all of the content. Card sorts can also be conducted using an online tool such as OptimalSort. This is a cheap and effective way of reaching a large sample of users without needing to bring them into a dedicated lab facility for the study.","description":"Card sorting is a research method used to understand the way that the intended users of a website naturally organise or think about different types of information or content. It’s also a method service teams can use to sort and arrange user needs.","link":"/service-manual/user-centred-design/card-sorting.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Check your answers pages","indexable_content":"Code Guidance Accessibility Security Research increased completion rates – users are reassured that the correct data has been captured so are less likely to drop out of the transaction before completing it reduced error rates – users are given a second chance to spot and correct errors before submitting the data what effect does the wording, styling and position of the ‘Change’ links have on their usage? for long, multi-section transactions, do people need one of these pages at the end of each section, or just at the end of the whole transaction? Use these to play back to users the information they have provided, before they submit it. Status: Research required  The two main benefits of summary pages are: There’s a coded example of this page in the GOV.UK prototyping kit. Make it clear that there is a task to perform on this page and that the transaction is not complete until they confirm that the information is correct. Provide links back to the pages containing information they can change. The user should be returned to the ‘Check your answers’ page once they’ve updated the information (and any other questions triggered by their updated response). Don’t just write the questions and answers out as they were originally given. Reword them for this particular format and context. For example, the individual elements of an address do not need labelling, and long questions can often be rewritten as shorter statements. The submit button should fully state the action it performs, eg ‘Change your tax details’ or ‘Send your claim form’. You don’t need an explicit declaration checkbox as long as it’s made clear what will happen. Add hidden text to the ‘Change’ links, so that they make sense when read out of content by screen readers. These pages play back information entered by your users. In some cases you might need to obfuscate parts of the data to reduce the risk that it’s read by the wrong people. Each service is different. Work with your security team to make sure that there’s an acceptable level of risk. You can help improve this pattern by researching the following: Update the Hackpad with your findings","description":"Use these to play back to users the information they have provided, before they submit it.","link":"/service-manual/user-centred-design/resources/patterns/check-your-answers-pages.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Choosing appropriate formats","indexable_content":"Choose the format to fit the content Don’t assume your users can read proprietary formats      For written reports, the native format of the web (HTML) should be your default option. PDF can be an excellent display format, but without additional effort it can be inappropriate for users of screenreading software. It’s faster and easier to make accessible HTML that’s suitable for every platform and device. If you must publish PDFs, you should provide accessible alternate formats for the document, and invest effort in accessibility tagging your PDFs.         For data, use CSV or a similar ‘structured data’ format (see also JSON and XML). Don’t publish structured data in unstructured formats such as PDF.         If you’re regularly publishing data (financial reports, statistical data etc.) then your users may well wish to process this data programmatically, and it becomes especially important that your data is ‘machine-readable’. PDFs, Word documents and the like are not suitable formats for data publication. In addition, you should consider making your data available through an API (application programming interface) if this will simplify your users’ interactions with your publications. For more information on APIs, and for more detailed technical guides on publishing data, please see our guidance on APIs and formats.         If you’re publishing a written report that contains statistical tables, provide the tables alongside or in addition to your report in suitable data formats.         For textual reports, provide HTML, plain text (.txt), or PDF rather than formats that require proprietary software to view, such as Word documents (.doc/.docx).         For tabular data, provide CSV or TSV rather than Excel spreadsheets (.xls/.xlsx).         For other structured data, see our guidance on representations for the consumer. Wherever possible, choose an open format over a proprietary one.    Almost all content relating to the policies or publications of government departments should live on GOV.UK. Where exceptions to this rule are required, content and data should be provided in formats that appropriately reflect their purpose and intended audience. You should publish documents in file formats that reflect the nature of the information they contain, and the uses to which they will likely be put. For written reports, the native format of the web (HTML) should be your default option. PDF can be an excellent display format, but without additional effort it can be inappropriate for users of screenreading software. It’s faster and easier to make accessible HTML that’s suitable for every platform and device. If you must publish PDFs, you should provide accessible alternate formats for the document, and invest effort in accessibility tagging your PDFs. For data, use CSV or a similar ‘structured data’ format (see also JSON and XML). Don’t publish structured data in unstructured formats such as PDF. If you’re regularly publishing data (financial reports, statistical data etc.) then your users may well wish to process this data programmatically, and it becomes especially important that your data is ‘machine-readable’. PDFs, Word documents and the like are not suitable formats for data publication. In addition, you should consider making your data available through an API (application programming interface) if this will simplify your users’ interactions with your publications. For more information on APIs, and for more detailed technical guides on publishing data, please see our guidance on APIs and formats. If you’re publishing a written report that contains statistical tables, provide the tables alongside or in addition to your report in suitable data formats. In summary, consider your users, and the uses to which they’ll put your published data and content. If in doubt, treat the native format of the web, HTML, as a good fallback option. Web browsers are available on all platforms and devices, and web pages tend to be both passably accessible and machine-readable. Wherever possible, publish in accessible, patent-free, open formats, for which software is widely available on a variety of platforms. If publishing in proprietary formats, you should always make a non-proprietary alternative available. For textual reports, provide HTML, plain text (.txt), or PDF rather than formats that require proprietary software to view, such as Word documents (.doc/.docx). For tabular data, provide CSV or TSV rather than Excel spreadsheets (.xls/.xlsx). For other structured data, see our guidance on representations for the consumer. Wherever possible, choose an open format over a proprietary one. Again, if in doubt, you should treat the native format of the web, HTML, as your best default option.","description":"Almost all content relating to the policies or publications of government departments should live on GOV.UK. Where exceptions to this rule are required, content and data should be provided in formats that appropriately reflect their purpose and intended audience.","link":"/service-manual/user-centred-design/choosing-appropriate-formats.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Cloud security","indexable_content":"Cloud first Cloud security principles Implementing the cloud security principles Not all principles are relevant to all services Further reading Data in transit protection Asset protection and resilience Separation between consumers Governance Operational security Personnel security Secure development Supply chain security Secure consumer management Identity and authentication External interface protection Secure service administration Audit information provision to consumers Secure use of the service by the consumer A description of the principle – what it is and why it is important A number of implementation objectives – how the principle should be satisfied A number of approaches that can be taken to meet the implementation objectives, attracting different residual risks ISO/IEC 27001 – information security management Cloud Security Alliance – cloud controls matrix Cloud security guidance home page    public sector organisations should consider and fully evaluate potential cloud solutions first – before they consider any other option     This approach is mandated to central government and strongly recommended to the wider public sector. Departments will remain free to choose an alternative to the cloud if they can demonstrate that it offers better value for money.     Which security principles are relevant?  Government currently has a Cloud First policy for public sector IT. This policy specifically states that: public sector organisations should consider and fully evaluate potential cloud solutions first – before they consider any other option It is worth noting as well that: This approach is mandated to central government and strongly recommended to the wider public sector. Departments will remain free to choose an alternative to the cloud if they can demonstrate that it offers better value for money. With the move to using third party hosted and managed services comes a series of new security challenges. With that in mind CESG, the national technical authority, have produced a set of guidance. The introduction  covers how the guidance was developed and how it can be used to build trust in cloud based systems and suppliers. This guidance begins with outlining 14 principles through which we can analyse the security of a cloud based system. The guidance also includes an implementation guide which contains for each of the above principles Rather than replicate the good work of existing standards bodies this guidance references existing work wherever possible. In particular extensive reference is made to: The guidance also includes recommendations for integrating this with existing risk management processes. In particular this recommends asking Which security principles are relevant? The 14 principles should not be viewed as a checklist to be used in all cases.","description":"Government currently has a Cloud First\npolicy for public sector IT. This policy specifically states that:","link":"/service-manual/operations/cloud-security.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Technology code of practice","indexable_content":"The technology code of practice Using the Technology Code of Practice £5 million – for technology expenditure including all sub components in a request, eg a website and service supported by a series of contracts such as hosting, data centres, voice & video, security, securing third party support. Includes expenditure related to networks. £1 million – for expenditure on services also delivered by Independent Shared Service Centres (current or new back office/administrative systems, including ERP systems, HR systems, finance/accounting systems, procurement systems). £100 thousand – for digital expenditure. £0 (no lower limit) – for any digital projects using ID assurance for the general public, domain registration, and any external facing digital transaction, website or mobile apps. Ensure systems, information, processes and networks are designed around the needs of the service user, providing as simple and as integrated an experience as possible. Be very clear who the users are and how to engage with them and ensure their needs are met. Demonstrate value for money in your business case and articulate the options considered in a full and objective appraisal. All new or redesigned digital services, both public facing and for internal use meet the Digital by Default Service Standard. All new or redesigned networks adopt the Network Principles Ensure a level-playing field for open source software when you choose technology. Demonstrate an active and fair consideration of using open source software – taking account of the total lifetime cost of ownership of the solution, including exit and transition costs. Use open standards, complying with any that are compulsory for use in government, unless you’ve been granted an exemption. Use common government platforms (eg GOV.UK, Performance Platform, GOV.UK Verify (identity assurance), Digital Marketplace, shared services) where available. Make data open by default, while minimising and securing personal data, or data restricted for national security reasons. Public data should be proactively published in a manner consistent with the Open Data principles: structured, machine-readable, and discoverable through data.gov.uk. Users should have access to, and control over, their own personal data. Establish the sensitivity of information held in accordance with the Security Classification Policy, establish legal responsibilities, develop user friendly, proportionate and justifiable security controls according to the Security Policy Framework. Separate commodity from niche needs and purchase accordingly. Use cost-effective commodity services for infrastructure and utility business activities like office productivity (word processing, spreadsheets and presentation software, email, scheduling and collaboration). Identify and acquire capabilities rather than infrastructure where services required are bespoke/innovative. Ensure that any procurement is designed to encourage competition and follows published Government Procurement Policy. Programmes will be ‘disaggregated’ to meet business and operational needs – broken down into components supported by the market to enable many suppliers to bid. Contract lengths for services should be kept to the minimum level necessary to ensure commercial flexibility. Objectively evaluate potential public cloud solutions first – before you consider any other option. In order to do this you will need to identify the capabilities and services that make up your technology design, and demonstrate that the solution chosen represents best value for money. Purchase networking and telephony services through the soon to be announced Network Services framework or its predecessors PSN Connectivity framework and PSN Services framework. Purchase Data Centre capacity through Crown Hosting Service. Applications should be accessible through a browser to ensure your solution will work for any end user device. Ensure best environmental practices, whether in-house or via external suppliers, including compliance with Greening Government ICT. Any software licence agreements must evidence actual user needs – there should be no default continuation of enterprise licence deals or specification of products or brands. Share resources: services, information, data and software components must be shared in order to encourage reuse, avoid duplication and prevent redundant investments. Reuse includes the use of existing services and capabilities that already exist outside of government where they provide best value for money, eg identity verification, fraud and debt management, cloud-based commodity services. Align to the shared services strategy for HR, procurement, finance and payroll. Plan on using an agile process, starting with the user need. Waterfall should only be used by exception and where it can be shown to better meet user need. Projects may need the best of both formal and agile methods, playing to their respective strengths: producing successful IT services is about knowing when to use the right tool at the right time. Demonstrate that adequate capability is available in your organisation – you shouldn’t outsource strategic decision-making or service accountability. If the necessary capability does not exist in-house, then you need to evidence a plan for developing or recruiting people with the right skills and experience. Implement effective procedures for the use and management of information (both structured and unstructured) through its entire lifecycle. Adhere to The National Archives (TNA) expert guidance on information management. no IT contract will be allowed over £100 million in value – unless there is an exceptional reason to do so, smaller contracts mean competition from the widest possible range of suppliers companies with a contract for service provision will not be allowed to provide system integration in the same part of government there will be no automatic contract extensions; the government won’t extend existing contracts unless there is a compelling case new [cloud] hosting contracts will not last for more than 2 years GDS will examine and challenge all technology-related spending over a certain threshold – no public commitment or expenditure should be made above these thresholds without prior approval. The thresholds are: For your project to proceed, you must demonstrate that you have met all applicable elements of this code – this will be verified through the controls process: There are a number of aspects, which indicate that a project or programme is significantly far away from meeting the Technology Code of Practice and being run in the wrong way. These are known as the ‘red lines’ and they are: Further information can be found in our blog post about red lines. However, there will always be exceptions to these red lines and we are pragmatic, where there is genuine need. If you have any doubt please contact GDS on gdsapprovals@digital.cabinet-office.gov.uk.","description":"GDS will examine and challenge all technology-related spending over a certain threshold – no public commitment or expenditure should be made above these thresholds without prior approval. The thresholds are:","link":"/service-manual/technology/code-of-practice.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Testing code","indexable_content":"Approaches Types of testing When to write tests Test early and often Acceptance testing Unit testing make sure that code does what it’s supposed to make sure that code is protected against malicious attacks provide assurance that iterating the code for better design or new features won’t introduce bugs Dan North “Introducing BDD” Wikipedia on BDD acceptance testing unit testing allowing developers to verify that complex calculations are performed correctly making sure that code handles bad input from users properly making sure that optimisations to the code won’t break its behaviour everyone involved understands the objective of a piece of work progress can be demonstrated through the current user story happy path tests confirm that the system can be used as intended sad path tests confirm that it handles errors (whether bad input from a user, a vital API being unavailable, or some other issue) smoothly GDS use automated testing to: –GDS also add manual testing as an extra check, where appropriate. Automated testing is an important part of our overall approach to quality but only one part of it. There are various approaches to writing automated tests. In particular there are differences in when people expect to write tests, and in the ways that they’re expressed. Many practitioners insist that automated tests should always be written before the code they seek to test (to ensure careful design and ‘just enough’ code) while others are happier writing tests after the fact. Tests that are written before the code offer a number of advantages and that approach should be encouraged, but the most important thing is that the whole team works to ensure there are automated tests, that those tests are understood as an asset of the product and that they help you ensure the quality of your code. It is common to talk about behaviour-driven development (BDD) as an alternative approach to test-driven development. BDD is an approach to automated testing that focuses on expressing tests in the “ubiquitous language” that the whole team should share when discussing problems. There are various tools that have been created to facilitate BDD but it is an approach that can be implemented using most traditional tools. Any code written for your service should have a set of tests operating at 2 levels: This requires broad tests that run through high-level functionality end-to-end, making sure that the pieces of the system come together to provide the right service. A developer in your team should be able to describe the steps in any acceptance test to the product/service manager in a way that makes sense to them and matches how they expect the service to be used (or abused!) This concentrates on the specific details of the code, making sure that each separate unit of code does what is expected of it by: Write tests whenever a bug is discovered. Then write a test to reproduce the bug before it’s fixed, so you can confirm that the bug has been fixed and make sure that it isn’t reintroduced later. GDS aim to write a first set of tests at the start of working on a feature. Acceptance tests that describe the expected end-to-end behaviour of code make sure that: These tests are often described as ‘happy path’ or ‘sad path’: GDS start with happy path tests and a few simple sad path tests, and then add more sad path tests as understanding of the code and its dependencies develops. Developers are expected to run tests regularly, especially before sharing new code. Examine the code in tests as part of the code review process, and regularly run tests in a shared continuous integration system. This gives the whole team a chance to see how they’re performing.","description":"GDS use automated testing to:","link":"/service-manual/making-software/code-testing.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Community user groups","indexable_content":"How research communities work Where and how you might use it When not to use Participants rapid speed (questions are answered in real time, research team can react rapidly to internal demands) cost effective when up and running flexibility availability rich outputs (visual content such as video is regularly used and is impactful) deep insights raises profile of research internally when insight is relevant and timely An online research community is a website that allows pre-selected respondents to interact with each other and complete a series of tasks. The types of community differ, and vary in openness, permanency and size. A typical community will be private, often have between 300-500 members and focus on building relationships between participants. This type of community offers a great source of deep insight over long periods of time. The tasks given to a research community differ in format and can include; online forums, blogs, polls, surveys, instant chat and more. Tasks are usually creative, and ask different questions relating to the research objective/s. Research communities can be used for a wide range of projects such as audience understanding, diary studies, concept development or simply idea generation. They allow a business to get close to users and offer meaningful dialogue between an organisation and its customers. The key benefits are: Communities require constant management. Participants need to be kept engaged, which means daily interaction, new questions and inventive approaches to maintain their interest. The community needs constant moderation, usually by a research manager or third party research agency. The quality of community members is paramount. Their feedback needs to be quality checked and panel attrition needs to be monitored on an on-going basis to ensure fresh ideas. Participants can be obtained using public postings on websites, email groups and social media, or via third party sample companies.","description":"An online research community is a website that allows pre-selected respondents to interact with each other and complete a series of tasks.","link":"/service-manual/user-centred-design/user-research/community-user-groups.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Confirmation pages","indexable_content":"Code Guidance Implementation Research a reference number (if there is one) details of what happens next and when contact details for the service links to related information and services a prompt to give feedback on the service a way of saving a record of the transaction (eg as a PDF) Where further action is required by the users, should this pattern be used? If so, how do we ensure people are aware of those actions? If not, what pattern should be used? A confirmation page lets someone know they’ve completed a transaction and explains to them what will happen next. Status: Research required  There’s a coded example of this page in the GOV.UK prototyping kit. The page should include: Some users might bookmark confirmation pages as a form of receipt. Make sure your service responds in a helpful way when they return to the page. The following areas require further research: Update the Hackpad with your findings","description":"A confirmation page lets someone know they’ve completed a transaction and explains to them what will happen next.","link":"/service-manual/user-centred-design/resources/patterns/confirmation-pages.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Content designer","indexable_content":"The importance of content designers Skills and attributes Guidance Job description identify user needs – based on:            legacy content       source material provided by policy colleagues       feedback from users and stakeholders       analytics data both from the site and from search engines         legacy content source material provided by policy colleagues feedback from users and stakeholders analytics data both from the site and from search engines gain an in-depth knowledge of a wide range of subjects – so they can make informed decisions about the best way to present information to users develop content plans and strategies – high-level plans showing how the identified user needs will be met write great content – in plain English, optimised for the web and according to house style edit content – making sure the site remains accurate, relevant, current and optimised both for users and search engines make tough decisions and work hard for the user – grappling with complicated legislation and turning it into clear, clean, crisp web content (that still has enough depth to be useful) work with developers and designers to create better solutions – for example, writing logic and content for smart answers understand and incorporate the results of user testing review the work of other editors – to make sure consistency and excellence is maintained across the site publish content – using various systems communicate the principles of good content design to others in the organisation advocate for the user and act as a guardian for the site – pushing back on change requests that don’t contribute to meeting user needs and incorporating change requests that do build positive relationships with others inside the team and in the wider organisation innovate and anticipate – excellent content designers are excited about the possibilities of web content and contributing to the digital sector’s future Content designers make sure that the writing on the site or service meets the needs of the user as clearly, simply and quickly as possible. You can add up, but it doesn’t mean you’re an accountant. You can write, but it doesn’t mean you’re a content designer. For many reasons, in the past the government has often published content that’s difficult to understand and difficult to act on. What gets published can be more about what the government wants to say than what the user needs to know. At best this results in a frustrated user. At worst, citizens and businesses get into trouble because they can’t understand (or can’t face wading through) difficult content. The content designer’s job is to make sure that doesn’t happen. Content designers must be able to: Read guidance in the manual of particular interest to content designers. Click either of the options below to download a template Content designer job description. Download as OpenDocument Format / Download as MS Word doc Cabinet Office will help departments to recruit suitably skilled individuals through the Recruitment Hub.","description":"Content designers make sure that the writing on the site or service meets the needs of the user as clearly, simply and quickly as possible.","link":"/service-manual/the-team/content-designer.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Continuous delivery","indexable_content":"Deployment The deployment pipeline The commit stage Shared sandbox environment Specialist testing environments Production environment add improvements fix bugs test expectations about your end product configuration management (how you maintain consistency with your product’s performance and functionality) the automation of your build, deploy, test and release processes commit stage shared sandbox environment specialist testing environment production environment the commit stage the shared sandbox environment any necessary specialist testing    Test all iterations of your software, either through public user testing or automated testing (using separate software to perform the testing).  Continuous delivery is about producing regular iterations of your software that’s ready to be released (deployed), though you don’t have to release these iterations to the public. Producing regular iterations makes it easier for you to: If your software isn’t ready to be used then it’s not creating any real value. Treat it as stock piling up – and stock is waste. The Lean software development philosophy tells us to: Test all iterations of your software, either through public user testing or automated testing (using separate software to perform the testing). Automate your deployment process so you are forced to fully understand it. Then any issues with moving code from your version control system into production (when it’s gone live) can be dealt with early on. Automating it early also means that code will be tested and any bugs fixed so that releases become frequent, low-risk, almost boring events. Don’t plan production release slots far in advance. You can’t be certain what will be ready in 6 months’ time. Make your release planning simpler – make sure it’s flexible enough that any feature or update to your software can be deployed when it’s ready. Then, if a feature needs to miss a release slot, it can easily be rescheduled for tomorrow or next week, rather than in 6 months, or more if the next slot is already full. Another advantage is being able to quickly respond to security patches or similar changes in underlying  libraries or frameworks used by your application. You can quickly make a change and watch the update flow through the various gates in your deployment pipeline, confident that nothing has been broken. What happens to code between it being written by a developer and deployed to production is known as the deployment pipeline. Understand your end-to-end deployment pipeline. Knowing how it works and how each element works together will have implications for: The deployment pipeline has 4 stages: When your developer checks into version control (where all code, including previous versions, are stored), a range of tests should be run against the latest version of the code. Any quick, easy-to-identify defects, like compile errors or unit test failures will be found at this stage. If the tests pass, the code progresses to the next stage and should be considered for release into production. Send the code to a shared sandbox (testing) environment. This is the first environment where the application is deployed and run. It’s also the first stage where it can be visually inspected for quality by anybody on the team. Make the sandbox environment as similar to the production (live) version as far as is practical. For example, if production uses Postgres, the sandbox should also use Postgres and not another database like MySQL or SQLite. The purpose is to find any defects in the code. If a defect is found, stop the version of the code at this stage. If it passes the sandbox environment it can move on to further specialist testing environments. You may need to perform additional testing for specialist requirements, like load and performance testing, penetration testing, or accessibility testing. The amount of environments you’ll need will depend on the requirements and conditions of your individual projects. When you are satisfied with the quality of the code, move it to the live production environment. Your code is ready to go live if it has passed: Deploy to production the same way as you deploy to any other environment – use the same scripts, same configuration management tooling, and the same version of the code.  This means you’re not releasing code for the first time – you’re performing a task that’s been validated at each stage throughout the deployment pipeline.","description":"Continuous delivery is about producing regular iterations of your software that’s ready to be released (deployed), though you don’t have to release these iterations to the public.","link":"/service-manual/agile/continuous-delivery.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Cookies","indexable_content":"Cookies explained Privacy and Electronic Communications Regulations Using cookies Types of cookies Cookie information and warnings Cookie scoping and attributes Further reading First party cookies Third party cookies Exempt cookies what to keep in mind when including cookies into your services how and why you must notify users about cookies on your service get explicit, informed consent from the user before storing cookies on a user’s computer be satisfied that the user understands that their actions will result in cookies being stored (implied consent) be satisfied that the cookie is “absolutely essential” to the operation of the website (eg cookies used for operating a shopping cart) first party cookies third party cookies exempt cookies cookies for storing logged in status cookies for storing user preferences some types of analytics cookies what data is stored within them how long the data is stored for what the data is used for under the control of the third party service, not the website operator can be accessed on any website that makes use of the service can be used to track a user from one site to another cookies from social media sharing services cookies from advertising campaign management services cookies from embedded document sharing services (eg dropbox) cookies from some analytics services being used for load balancing being absolutely essential to website functionality (eg used to store shopping cart contents)    “GOV.UK uses cookies to make the site simpler. Find out more about cookies.”  This short guide about cookies covers: Cookies are small data files that are sent from a website and stored on a user’s computer. They are used to store information that can be retrieved later in the visit or in future visits to the website. Many uses of cookies are harmless, but sometimes they are used to track users and their browsing habits across multiple websites and target them with relevant advertising. In May 2011, the Privacy and Electronic Communications Regulations were updated to require website operators to gain consent before storing or retrieving data from a user’s computer (or other device). This change directly affects the use of cookies and other similar technologies, like HTML5 local storage. Before using cookies, your website operator needs to either: Responsibility for complying with these regulations lies with your website operator. This guide covers how to use cookies on government services, but the principles also apply to other technologies, like HTML5 local storage. Minimise the use of cookies in your service, storing the smallest amount of information that you need for as short a time as necessary to provide a good service to users. If your service needs to store cookies, you need to make sure that this can be explained simply and clearly, in a way that the majority of users can understand. You must notify users that cookies are being stored. This information covers 3 types of cookies: These cookies are set by the website that the user is currently viewing. They’re under the control of the website operator and can only be accessed by the website. Data stored in the cookie is not shared with other websites. Examples of first party cookies include: First party cookies are minimally intrusive as the website owner has complete control over: These cookies are set by external services used on the website and are: Examples of third party cookies include: Third party cookies are intrusive as the website owner usually has no control over what data is collected or how it is used. These cookies do not need to gain consent from a user. Exempt cookies have many uses, like: Still notify users that these cookies are in use, even though they are exempt from the Privacy and Electronic Communications Regulations. All services on the service.gov.uk subdomain must include a cookie information page. This page must contain information about the cookies used throughout the site, followed by an explanation of each cookie’s purpose and how long it’s stored for. The GOV.UK cookies page is an example of how to do this. You must link back to your service’s cookies page from the footer of your website. Your service information page must also link to the GOV.UK cookies page. Your service must also tell users on their first visit that cookies are used and regularly remind them of this. This is particularly important when your service relies on implied consent. GOV.UK does this with a blue information banner that is displayed at least once every 3 months with this message: “GOV.UK uses cookies to make the site simpler. Find out more about cookies.” Where explicit consent is required, your service must notify their users before the cookie is set. To do this, use the sets a cookie text linked to the appropriate details on your cookie information page. Cookies must only apply to your originating domain name, eg www.servicename.service.gov.uk not .gov.uk. Don’t use cookies on domains that host only static assets (they introduce a browser overhead that slows down the response time for users without providing any benefit). Cookies must be sent with the Secure attribute and, where appropriate, be sent with the HttpOnly attribute. These flags provide additional assurances about how cookies will be handled by browsers. This blog post by GDS developer Dafydd Vaughan explains how cookies were used on the beta version of GOV.UK.","description":"This short guide about cookies covers:","link":"/service-manual/making-software/cookies.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Create a password","indexable_content":"1. Help users to create good passwords 2. Help users to enter passwords securely and accurately 3. Help users who have forgotten their password 4. Examples On this page: Password constraints Password strength indicators Hide passwords by default Don’t disable paste Reset passwords, don’t email them Avoid password reset questions Avoid password reminders Help users to create good passwords Help users to enter passwords securely and accurately Help users who have forgotten their password Examples choose constraints appropriate to your users security needs explain to users what those constraints are are distracting to some users encourage some users to create minimally secure passwords occasionally report easy-to-guess passwords as being strong easier to mistype a password harder to meet strict password contraints letting users see their password if they want to showing the last typed character of their password making users enter their password twice and comparing them  Help people create secure passwords. You can force stronger passwords by setting minimal constraints on them relating to length, characters used etc. Make sure you: Overly strict or confusing constraints on password formation make it harder for people to use your service.  Some users may give up. Others may regularly forget their password and have to rely on whatever support you’ve provided. Others will end up storing their password in some non-secure place. We are not currently recommending use of password strength indicators because some evidence indicates that they: When users are creating a password they need to know that they’ve entered it correctly, but they also need to know that no-one has seen what they’ve entered. Users might be in a public space when entering or creating a password, so hide passwords by default. However, if they can’t see what they’re typing, it’s: You can help by: This last approach is only appropriate when creating a password. It should not be implemented in combination with the other two approaches and is not guaranteed to block all mistypes. Don’t disable paste on password fields. People may have very good reasons why they want to paste their password – they’re using a password manager for example. Passwords that are hard to guess can also be hard to remember. Help users who have forgotten their password to reset it. Email is not a secure channel. Sending passwords by email is a bad idea. Instead, email a link or code to the address or phone number registered on the account that the user can use to create a new password. These are not particularly effective. The information they ask for is either too obscure and therefore just as hard to remember as a password, or it’s easy to find out (mother’s maiden name), or it’s unstable (favourite colour). Password reminders are a bad idea because they encourage users to reveal information about their password and they don’t work for very strong passwords involving random strings of characters. An example with a ‘show typing’ toggle: An example with a ‘re-type password’ field: Discuss this page on Hackpad","description":"Help people create secure passwords.","link":"/service-manual/user-centred-design/resources/patterns/create-password.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Create a user account","indexable_content":"1. Consider alternatives 2. Don’t make people create an account until they have to 3. Use consistent and unambiguous language 4. Use effective signposting 5. Minimise distractions 6. Help people choose good usernames and passwords On this page: Consider alternatives Don’t make people create an account until they have to Use consistent and unambiguous language Use effective signposting Minimise distractions Help people to choose good usernames and passwords Help people to quickly and painlessly create user accounts. Account management systems are hard to build and hard to use, so consider other ways of giving people access to their data first. For example, delivery companies often let people track the status of a parcel by entering a unique order number and their email or name. Alternatively, use the existing GOV.UK Verify sign-in service. Let people use as much of your service as possible before they need to create an account. For example, most shopping cart sites let you browse without having to create an account first.  If people can avoid creating an account at all – even better. For consistency with other GOV.UK services, use the phrase ‘Create an account’ on the appropriate screens rather than ‘Register’, ‘Sign-up’ or something else. Similarly, use labels like ‘Create a username’ and ‘Create a password’ rather than ‘Username’ and ‘Password’, so people understand that they’re not being asked to enter existing ones. Creating an account is rarely a user goal. People come to your service to use it not to create an account – your calls to action should reflect this. Make the choice between creating an account and signing in explicit. Presenting the options side by side is not enough – people will miss one of them or not understand the difference. If you can, ask people if they’ve used the service before and then direct them to either sign in or create an account. If users fail to create an account they might not be able to use your service at all. Account creation screens should be about this one task, so strip out any distracting content or links. Read our guides on creating good usernames and passwords. Discuss this page on Hackpad","description":"Help people to quickly and painlessly create user accounts.","link":"/service-manual/user-centred-design/resources/patterns/create-user-account.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Create a username","indexable_content":"1. Use the phrase 'Create a username' 2. Explain what the username is for 3. Try asking for username and password on separate pages 4. Examples On this page: Use the phrase ‘Create a username’ Explain what the username is for Try asking for username and password on separate pages Examples people with high digital skills did not notice that the two questions were separated, and completed them easily people with low digital skills found the questions much easier when they could concentrate on them one at a time Help people to create a memorable and appropriate username for their user account. Use a label like ‘Create a username’ rather than ‘Username’, so people understand that they’re being asked to create a new username, not recall an existing one. Alternatively, if you’re using email addresses or phone numbers as usernames, use these as the label. The concept of usernames may not be familiar to people with low digital skills or low confidence online. Help these people by explaining what the username is for and how it will be used. This also helps people decide on a username. It’s particularly important to tell people if their username will be visible to other people, or if it can’t be changed. If your service has users with low digital skills or low confidence online, then consider asking for the username and for the password on separate pages. In research for GOV.UK Verify we found that: Discuss this page on Hackpad","description":"Help people to create a memorable and appropriate username for their user account.","link":"/service-manual/user-centred-design/resources/patterns/create-username.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Creating accessible PDFs","indexable_content":"In Microsoft Word In Adobe Acrobat Before publication Acknowledgements Use headings Use lists Create a table of contents Use readable body text Use good colour contrast Use data tables Provide text descriptions Set the document language Check the tag tree Check the tab order Check the reading order Check the reflow order Check text descriptions Remove empty tags Set decorative content Check data tables Check active links Check high contrast Display document title Full Adobe accessibility check Quick screen reader check Use NVDA Use VoiceOver from the top of the PDF (with the Numlock off), use Numpad 0 + Numpad 2 to read the PDF from top to bottom and check the reading order use the tab key (repeatedly) to move through the PDF and check the tab order use the h key (repeatedly) to move through the PDF and check the heading structure use the g key (repeatedly) to move through the PDF and check for text descriptions from the top of the PDF use a double finger down swipe, or Control + Option + A to read the PDF from top to bottom and check the reading order use the tab key (repeatedly) to move through the PDF and check the tab order The best way to create an accessible PDF is to create an accessible source document. Well structured Microsoft Word documents make good source documents for conversion to PDF. When a source document is converted into PDF it is tagged. The PDF tag tree reflects the structure of the document, and it’s this structure that assistive technologies like screen readers use to navigate the document. Use the styles and features available in Word to format your content and give it structure. This will make it easier to convert your source document into PDF because it lays the groundwork for the PDF tag tree. Use the heading styles in Word to create a logical document structure. Don’t increase the size of text or make it bold to create the appearance of headings. Treat your document like a book: It should have one title (level one heading) and multiple chapters (level two headings). Within each chapter there may be multiple sections (level three headings) and sub sections (level four headings). Use the list styles in Word to group together related items. If the items follow a specific sequence, use a numbered list instead. Don’t use punctuation or other markers to create the illusion of a list. If your document is longer than a few pages, use Word to automatically create a table of contents based on your heading structure. Don’t use lists and links to manually create a table of contents. Use left aligned text (unless the language of your document is read right to left). Don’t use justified text in your document. Choose a sans serif font and use the styles in Word to set it as the default, with a minimum size of 12pt. If you need to include footnotes or other text of a smaller size, increase the size of the body text to 14pt rather than reducing the size of any text below 12pt. Don’t use chunks of italicised or capitalised text, and don’t underline text unless it’s a link. Use foreground/background colours for text that have a good contrast ratio. The 4.5:1 ratio recommended by the Web Content Accessibility Guidelines 2.0 is a good minimum. Don’t use colour or shape as the only way to identify something in your document. Use text labels or descriptions instead. Use tables with column headings to display data. Don’t use tables to make cosmetic changes to the layout of the document. Use Word to add text descriptions to all important images in the document. Make sure the text description includes all the information contained within, or conveyed by, the image. Use Adobe Acrobat Pro to convert your Word document into PDF. Use the Convert to PDF option under the Adobe menu in Microsoft Word to do this. This will ensure that Acrobat picks up the accessibility you have built into your source document. Set the language of the document. Go to File > Properties > Advanced and select a language from the Language menu. If the PDF is written in Welsh, type CY into the box. All content must be tagged, marked as an artefact (background content), or removed from the tag tree. Use the Tags panel to review and edit the tag tree. If the PDF was converted from a well structured Word document, the tag tree should require little editing. If the PDF contains form fields, use Advanced > Accessibility > Touch up reading order to check they can be navigated with the tab key in a logical order. If the tab order needs improving, use the Order panel to drag and drop the fields into the correct order. Use the Tags panel to review and edit the reading order of the PDF. Don’t rely on the visual order of the PDF. The reading order is based on the structure of the PDF tag tree, which may not match the visual content order. Use View > Zoom > Reflow then check that the PDF still has a logical reading order. Note: It can sometimes be difficult to guarantee a logical reflow order for PDFs with complex content. Go to Advanced > Accessibility > Touch up reading order and check that all images have text descriptions. If the text descriptions were present in the source Word document and the Convert to PDF option was used, the text descriptions should already be present in the PDF. Remove empty tags from the tag tree. Use the Tags panel to highlight and delete any empty tags from the tag tree. Tag decorative content elements as artefacts. Use Advanced > Accessibility > Touch up reading order to select a decorative element, and use the Background button to make the element an artefact. Use the Tags panel to check the structure of data tables. The <table>, <tr> and <td> tags should be used to give data tables the proper structure. Use the Tags panel to check that links are active. Active links should be tagged with the <link> tag. Use File > Preference > Accessibility to set a high contrast colour scheme, and check the PDF remains readable. It may not be possible to make high contrast mode work in all PDFs, in which case you should be prepared to make a high contrast version available on request. Display the document title instead of the file name. Go to File > Properties > Initial view and select Document title from the Show drop down box. Once all the above steps have been taken, the PDF should be checked before it is published. Go to Advanced > Accessibility and select Full check. The PDF should pass the full check for WCAG 2.0 Level AA without any warnings. Ask a screen reader user to read through the PDF. If no-one is available to do this, use one of the following options instead. Non Visual Desktop Access (NVDA) is a free open source screen reader for Windows. It can be installed to the desktop or run from a portable USB thumb drive. With NVDA running, open the PDF and use the following commands to check the PDF: These commands will also work with the JAWS screen reader from Freedom Scientific. VoiceOver is the integrated screen reader with Mac OS X and all iOS devices. In Mac OS X turn VoiceOver on (or off again) using Command + F5. With VoiceOver running open the PDF and use the following commands to check the PDF: VoiceOver does not provide shortcut keys for navigating PDFs by headings or graphics. Thanks to the Department for Work and Pensions (DWP) for enabling us to incorporate its accessibility best practice guidance into this document.","description":"The best way to create an accessible PDF is to create an accessible source document. Well structured Microsoft Word documents make good source documents for conversion to PDF. When a source document is converted into PDF it is tagged. The PDF tag tree reflects the structure of the document, and it’s this structure that assistive technologies like screen readers use to navigate the document.","link":"/service-manual/user-centred-design/resources/creating-accessible-PDFs.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Creating a culture that supports change","indexable_content":"What good looks like What you might need to change Breaking down barriers to technology change Understanding fear, uncertainty and doubt Why you need an environment that supports change Example: hosting changes for GOV.UK Further reading a team that is knowledgeable about existing and emerging technology options a governance and management structure that is an enabler, not a bottleneck or barrier to change flexible commercial models that meet your needs, eg commodity or utility provision a diverse, competitive market from which to choose your suppliers or products transparency and trust in your supplier relationships pricing competition and the ability to switch between alternative suppliers Your digital services and the technology on which they’re built should enable change, giving you the freedom to evolve your services according to changing user needs, expectations and technology innovation. Improving the way that you produce digital services is more about cultural and behavioural change than it is about IT. Technology alone will not be able to improve public services. Your digital services are based on user needs, not on organisation structures and processes. You have: To create better public services you’ll need to change existing behaviours and cultures, and successfully transition to new ways of working. You may need to develop skills within your team and keep up to date on new methods and technologies. It may also be necessary to change your physical working environment to get the best out of your team. You may need to radically change governance and management structures and will need to run projects in a more agile way. You’ll need an effective approach to information management, ensuring you adhere to the high level direction and guidance provided in the Information Principles. You’ll need to build relationships with new suppliers that you may have found through new procurement routes, such as the G-Cloud framework. It may be necessary and more cost-effective to write-off previous financial investments, so you’ll need courage and conviction to stop spending on old, legacy systems. The Cabinet Office, through spending controls, has a strong preference against extending contracts to support legacy systems. At the very least you’ll need an exit management strategy that sets out a plan for how you will significantly reduce any dependence on outdated technology. The cost of maintaining out-of-date technology that isn’t fit for purpose could far outweigh the cost of changing to a more suitable technology that better meets user needs. If you’re in this position, it’s also likely that the architecture and hosting of your existing estate may need to be rethought. You may come across inertia or resistance when you try to make necessary changes to your technology or how your service is provided. Your existing supplier base will tend to resist change – and the more successful a supplier has been working under the old model, and the more entrenched it is, the greater this resistance is likely to be. The changes needed will often be more successfully initiated and produced by those not encumbered by past success – new market entrants and those traditionally closed out from government work, like small or medium-sized businesses. You may find that you have become locked-in to a particular contract or technology. As part of your consideration of the total cost of ownership of a particular solution, you should have estimated the cost of exit at the start of implementation. If you try to move and find that cost is a barrier, any unlocking costs that you have identified must be associated with the incumbent supplier/system and not be considered as part of the cost of a new solution. Building on open standards or selecting products which use them will help you to avoid this problem in the future. If you have incumbent suppliers, it’s important that you understand and anticipate their likely reaction as they struggle to maintain the status quo and resist change. Incumbent suppliers will have data that reflects the past success of their business models and the way their business worked. They will be less certain about the future and may be concerned about the weakening of their market share and their exposure to genuine, open competition. The rewards and culture of these companies are likely to be built on their current business model. That will reinforce their internal resistance to change. It’s difficult for them to persuade shareholders and financial investors to replace a well-understood, if obsolete, business model with an approach that favours the treatment of some IT products as commodities. The IT as utility market is not yet fully established, so they will be uncertain about it even though this model might bring them higher profits on lower revenues. You’ll also see similar resistance in-house, where many staff may have long grown accustomed to old ways of working. It’s important that as a leader you provide the assistance and mentoring they need to help update their skills and experience. Some may have become dependent on external suppliers’ advice. Some may even have built their career on accreditation in a single supplier’s technologies. Flexible technology and services give service managers the ability evolve services and make them responsive to users’ needs. It also avoids costly and more risky big bang changes. Reducing the risk of lock-in to suppliers, software, service and support, or too old and inefficient IT, means that you have the ability to choose what best meets the outcomes you are looking for at a sustainable and competitive cost and at a timescale that suits you. GOV.UK is an example of how this works in practice. On the technology side, we set up hosting to build the beta of GOV.UK and then transferred it to a new supplier on the G-Cloud framework before launch. In early 2014, the infrastructure team at GDS migrated GOV.UK to a new hosting environment. Sam Sharpe wrote a blog post on the technical aspects of the migration and Carl Massa explained the project in a video. G-Cloud is the procurement framework intended for purchasing Infrastructure as a Service, Platform as a Service and Software as a Service products. Guidance about the team you’ll need to have in place to build a successful service. A blog post from GDS about making better choices for the technology we use.","description":"Your digital services and the technology on which they’re built should enable change, giving you the freedom to evolve your services according to changing user needs, expectations and technology innovation.","link":"/service-manual/technology/culture-that-supports-change.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Data visualisation","indexable_content":"Introduction Telling the story Choosing your visualisation and templates Creating your visualisation Be open and honest Further reading Best practice GDS example Checklist Column chart Bar chart Line chart Pie chart Scatter chart Checklist Worked example Checklist Worked example: GDS performance dashboard Checklist Worked example Before After Before After Who are your audience? How much detail do they need? What story does the data tell? Do you need a visualisation? Negative values below the x-axis. If needed, the target should be a single line that is visible but not too thick. Limit stacked columns to 3 segments. Arrange bars in size order, from biggest to smallest (unless there’s good reason, ie data needs to be represented alphabetically). Negative values to the left of the y axis. Limit number of data sets to 4. Keep axis labels horizontal. Use line points to differentiate between data types (use line dots for projections and estimates only). The largest segment should be at 12 o’clock going clockwise. Label the chart directly and avoid text inside segments. Limit items to two to avoid confusion. Include trend line if required. This should be a single solid line. What visualisations are available? Have you chosen the right visualisation for the data? What about infographics? Too many segments make them hard to compare. Too many colours made the chart confusing. The chart includes cost recovery as a negative which cannot be represented properly in a pie. This stacked chart is much clearer. Comparisons can easily be made and sorting the data provides quick insight. The stacked chart provides additional information which could not be visualised in the pie. start axes at zero unless there’s good reason not to (ie data is clustered at high values) don’t say too much, limit the number of data sets if needed, put legend at the top of the chart in the same order as the data in the chart maximise the space available to the chart and remove chartjunk include units of measurement in the chart title or directly on the axis, avoid doing both keep colours simple, do not repeat/alternate or use opposites - use the GDS Colour palette use the same colour when reporting a single data set 3D effects borders unnecessary axes lines random colours or backgrounds unnecessary text Have you removed the chartjunk? Have you got the right amount of supporting information? Have you got the right amount of data? Have you used the GDS colour palette? Will the colours work for people with colour blindness or greyscale? Are your colours appropriate for the data? Have you referenced data with a URL? Have you provided contact details (eg a mailto link)? Is it clear whether data is internal or public? Have you been open and transparent with data? Chart only shows a select few data points The small range on y-axis has exaggerated the fluctuations in data Y-axis starts at 60 but chart suggests data has reached lowest possible value Y-axis has been formatted to show the full range of data The data fluctuations can be seen in the context of the wider data series.    “We want transparency to become an absolutely core part of every bit of government business.” - Francis Maude  As we surface more data about government services, we need to make sure that the visualisations of it are easy to understand, visually compelling and prompt action. To do that, we need to have a consistent visual grammar, for use both within GDS and across government. This guide sets out 4 principles of good data presentation, with easy to follow checklists to help you achieve this. For context, we’ve added examples of how the principles have been employed at GDS. The principles and examples found in this guide are likely to evolve as we find new challenges and applications for them. There are many examples of best practice style guides already in place. For example, The Economist has a clearly defined house style that allows its readers to readily identify and understand their visualisations. They publish a new visualisation every day in their Graphic Detail. This guide attempts to build on the best practice from a range of organisations. The GOV.UK Performance Platform helps the government make decisions based on data, often presented through innovative visualisations (built using D3.js). The example below compares weekly visitors to GOV.UK with the two main websites it replaced.  To effectively tell the story behind the numbers, you need to understand both your audience and the data. Only use visualisations if they make the story clearer. In many cases, a good table or words may communicate better than a visualisation. If there are very few data points (eg top rate income tax down 5%, all other rates unchanged), it’s clearer to write a sentence than draw a picture. If every item must be read precisely (to several decimal places) then a table is best. A good table will be clear and uncluttered. The data should be easy to read with the same decimal places or rounded and sorted into a logical order. Don’t use too many different types of font, and make sure your data is referenced. But visualisations often are the right answer and the data is the most important feature. It should tell its own story and it’s best to not try to say too much in one go. Keep charts simple, cutting down on unnecessary items and jargon.  Explanatory text will be needed in some cases, but it should not simply repeat the story being told in the visualisation. A well written chart title can reinforce the story of the data and reduce the amount of additional text needed. Choosing the right visualisation will help the data tell its own story and give powerful insight. There are many ways of displaying information visually.  Most computer programmes come with a range of visualisations. There also visualisation tools available online: for example, this blog showcases some free ones and GDS has produced a guide to infographics Each chart has its own strength. Below are the core 5 with templates (a Google spreadsheet of these is available):  Strengths - comparing items, or a small number of time periods.  Strengths - comparing items, especially if they have long names or many items.  Strengths - comparing over time or between variables for a single item (eg site traffic vs site performance)  Strengths - simple share of total. Use with caution as column or bar charts are often better. Limit to 2 segments to avoid confusion.  Strengths - relationships between variables where there are many items (eg volume vs cost for numerous transactions) There is more help on choosing the right chart here. It is important to not confuse your audience. Choosing the correct visualisation is important and at GDS we reviewed what was being used in the performance dashboard. As the example below shows, Pie charts with many items are not clear. We used a stacked bar chart to better represent the data.   Keep in mind these useful tips when creating your charts: Keeping your visualisation simple will help the data tell its own story. Chartjunk is anything in your visualisation using ink that actively reduces clarity. Avoid: Know your audience so you give the right amount of supporting information. External or non-technical audiences will need more explanation but internal or expert audiences may find this tedious. Don’t use the text to simply repeat what’s being said by the data. Visualisations should avoid too much data. Only include what’s relevant. If the trend is obvious, don’t include a trendline. Sometimes it may be more effective to focus on high-value items only (if you’re being selective, be open and clear about this). Poor colour choice can change how the data is perceived in a visualisation. For example, red is strongly associated with negative performance so is best avoided for positive/normal figures. Colour blindness makes it difficult for a user to differentiate between data sets. Labelling charts directly and different line styles can help. If your visualisations are likely to be printed it’s important the colours work in greyscale as not all users will have high quality printers. The example below from the GDS senior management dashboard shows how avoiding chart junk and limiting the number of datasets can enhance your visualisation.  The legend accounts for a quarter of display space. The Y axis quotes £ and not £m. The segments are profiles and proportionate for each time period, so the bright colouring adds no extra detail. The mix of bar and line is confusing with so much information in the chart.  The stacked column gave a level of detail which wasn’t needed. This has been rationalised to best suit the audience. Axes have been standardised. The legend has been relocated giving the chart more space. Heavy grid lines and axes have been removed to give a clearer display. “We want transparency to become an absolutely core part of every bit of government business.” - Francis Maude Being open and transparent supports the Open Data White Paper. Similarly, our Open Public Services agenda is built on transparency. Sourcing data builds trust and credibility. Providing contact information shows ownership but also helps collaboration and information sharing. When presenting data be aware of axes and scales. Data can be misrepresented by only showing a selection if it isn’t clear why an extract has been chosen. Consider where the visualisation might be published. For example, if published alongside other visualisations, the reader is likely to assume the scales are consistent. This might change how your data is perceived.   More information about the Performance Platform. This chart chooser from Andrew Abela builds on the work of Gene Zelazny’s classic book Saying it with Charts. This interactive tool from Juice Analytics helps guide your chart choice through filters. Brain Suda’s A Practical Guide to Designing with Data provides a comprehensive understanding of how to best engage the audience with your data. Here is a video of Brian Suda presenting on a section of his book at the 2012 DIBI conference. Dona M. Wong’s The Wall Street Journal, Guide to Information Graphics details the do’s and don’ts of presenting data. Edward Tufte’s The Visual Display of Quantitative Information is a seminal work on data visualisations and introduces the concept of chartjunk. Here is a video of Edward Tufte discussing his theories on visual thinking and analytical design. This article from the Peltier Tech blog covers the ten chart design principles. The Flowing Data blog is a source of data visualisation news.","description":"As we surface more data about government services, we need to make sure that the visualisations of it are easy to understand, visually compelling and prompt action. To do that, we need to have a consistent visual grammar, for use both within GDS and across government.","link":"/service-manual/user-centred-design/data-visualisation.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Dates","indexable_content":"1. Memorable dates 2. Copied dates 3. Approximate dates 4. Relative dates 5. Calendar controls 6. Formatting dates 7. Validating dates On this page: Memorable dates Copied dates Approximate dates Relative dates Calendar controls Formatting dates Validating dates 8 July 2014 8 July to 9 August date ranges - check the dates are consecutive past dates - check the date is in the past future dates - check the date is in the future mistyped dates - if a date is obviously mistyped, warn users Not all dates are the same. Choose a format appropriate for the kind of date you’re asking about. For capturing memorable dates like dates of birth, the simplest option is to provide text fields for users to type the date in. For example: Calendar controls are not particularly useful for known dates and some users struggle with select boxes. We’re currently recommending using three fields as it’s easier to reliably validate than a single field. Don’t automatically tab between fields, as this can be unexpected behaviour that clashes with normal keyboard controls, and confuses users. Tip: If you want to trigger the num-pad on iPhones, add a pattern attribute to the input element like this: pattern=\"[0-9]*\" If you require a date to be provided EXACTLY as it’s given on another document (a passport or credit card for example), then it’s easier for users to copy the date across if the format is the same. If you don’t need an exact date and users are likely to struggle to remember it (for example the date you started your first job), make sure you allow users to enter approximate dates like ‘June 1996’. Some dates make most sense relative to today’s date or another date. This is particularly common when booking appointments of some kind. In these cases it helps if you let users enter or select relative dates like ‘tomorrow’ or ‘4 days later’. If the day of the week is important, show this as well.  Calendar controls are only appropriate for near past or future dates, where the day of the week is relevant. Their main use case is for appointment booking. If you also need to show information like availability, embed the calendar in the page and make it large enough for the information to be readable. A calendar control that depends on JavaScript should never be the only input option. The default date format to use is this one: Periods of time should be formatted like this: When you’re validating dates, remember to check the following: Discuss this page on Hackpad","description":"Not all dates are the same. Choose a format appropriate for the kind of date you’re asking about.","link":"/service-manual/user-centred-design/resources/patterns/dates.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Delivery manager","indexable_content":"The importance of delivery managers Skills Job description templates Further reading strong estimation and budget scoping skills experience in Agile Project Management methodologies familiarity with structured programme and project management environments experience delivering digital services experience in open source and cloud technologies and their sourcing good communication skills strong organisational and communication skills collaborative approach to working good at prioritising time-critical work an understanding of the wider digital landscape Good health check from the Scrum Alliance for delivery managers. A day in the life of a delivery manager - blog The delivery manager sets the team up for successful delivery. Remove obstacles, or blockers to progress, constantly helping the team become more self organising. They enable the work a team does rather than impose how it’s done. Skilled delivery managers remove obstacles, or blockers to progress, constantly helping the team become more self organising. They enable the work a team does rather than impose how it’s done. It’s not about micro managing! Equally important in an agile team – and particularly important to the delivery manager – is ongoing effort to improve products, services or processes. Their role in this is to facilitate project meetings- including daily stand-ups, sprint planning meetings, and retrospectives. They also track progress and produce artefacts for showing this, like burn down/up charts. They must be able to enable the team to produce estimates of how much effort is required to produce features that the Product Manager wants.  Delivery managers need to have: A delivery manager will also need the following skills: Click either of the options below to download a template Delivery Manager job description.  Download as OpenDocument Format / Download as MS Word doc  Cabinet Office will help departments to recruit suitably skilled individuals through the Recruitment Hub.","description":"The delivery manager sets the team up for successful delivery. Remove obstacles, or blockers to progress, constantly helping the team become more self organising. They enable the work a team does rather than impose how it’s done.","link":"/service-manual/the-team/delivery-manager.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Deploying software","indexable_content":"Principles for software deployment Techniques and tools to achieve these ideals Little and often Quality software Optimise for cycle time Repeatable, auditable deployments Zero downtime deployments Single artefact Ordered environments Repeatable deployments of infrastructure configuration Repeatable deployments of code Management of environment-variable configuration Zero downtime deployments Smoke tests Emergency deployments Deploying configuration management code Avoiding configuration management code Secrets Database migrations Service dependencies Making writes asynchronous little and often quality software optimise for cycle time repeatable, auditable deployments zero downtime deployments a .jar file for JVM languages for languages without compilation artefacts it may even be a tag in the source control system an entire virtual machine image with the application pre-deployed. construct your artefacts as operating system packages (.debs or .rpms) and install using your infrastructure configuration management tool from a local package repository (apt or yum) use a push-based system to deploy such as fabric, capistrano, or similar create a new immutable server for each deployment at a coarse-grained level, secrets cannot be accessed outside of the environment which uses them at a fine-grained level, secrets are known only by those machines in an environment which need to know them. We’ve identified some common principles for software deployment which we’ve applied in a number of different projects, with different technology stacks and needs. These principles underpin a software deployment process which meets user needs. Those principles are: Deploying software should be a low-risk activity. By deploying software frequently and in small increments, the risk is reduced in a number of ways. See Regular Releases Reduce Risk from the GDS blog for more on this. Deploying software frequently makes life better for the product managers in your organisation. Frequent deployments allow the product managers to get things right in a timely fashion: both fixing bugs and releasing new features.  Roo Reynolds, GOV.UK mainstream product manager, said that “Deploying once a week would be frighteningly slow.”  The GOV.UK site design has changed radically 4 times since its public release in October 2012. This was enabled in part by frequent releases enabling rapid gathering of feedback and responding to change. The software that you deploy to production should be of a consistently high quality. The user impact of bugs is obvious; less obvious is that the earlier you identify bugs, the easier and cheaper they are to fix. The deployment itself should not be a risky process. By the time a version of the software is deployed to production, you should have confidence that it will work smoothly and seamlessly. How long does it take from a developer making a code change to that change hitting production? The shorter this time is, the faster a product can respond to change. The quicker you can release the next iteration, the faster you will converge on an ideal solution. You should know at any moment what version of your service is running in each environment. When a deployment hits production, you should be able to trace the changes that it introduced all the way back to the initial code commits in the source code repositories which went into that deploy. Combined with small, frequent releases, if any problem does hit production, you will be able to immediately narrow down the cause to a small number of commits.  Rolling back to a previous version is less onerous as less of the system has changed. And “rolling forward” – with a code change to fix the production issue – is achievable because the deployment process is automated and the lead time is short. An additional benefit of having a repeatable deployment process is that scaling and recovering from failure become easy. Suppose you want to add more application servers to host a particular application, either to respond to higher demand, or to replace failed instances.  Once you have provisioned the required machines, you can just re-run your deployment process on the new machines to deploy the software. Without a repeatable deployment process, adding machines becomes manual and error-prone. Many deployment processes incur a downtime cost. The more frequently you deploy, the more downtime you will experience as a result of deployment. This may be acceptable depending on the needs of your particular project,  or you may need to consider how your deployment process needs to change to achieve zero downtime. This isn’t a one-dimensional problem. Achieving zero downtime for read operations is easier than doing so for write operations. Whether or not your project has a business need for zero downtime deployments, it’s worth considering the tools and processes which make it possible, as the constraints of zero downtime deployments can result in better engineering practices generally. An antipattern in deployment processes is building a different application artefact for each environment. Examples of this might include controlling the presence of debugging symbols in the binary or variations in the use of optimisation flags. The problem is that the testing you do of code artefacts in preview environments may not be applicable to the artefact you deploy to production. The better alternative is to build a single artefact which gets deployed to all environments. With the same code running in each environment, you can deploy to production safe in the knowledge that this code has been tested in every other environment and has not been found wanting. Note that the exact nature of an artefact is intentionally vague. It may be You should have multiple environments to deploy to. At the very least, you will have a development environment running the latest version of the software, and a production environment being used by live users. You may also have other environments dedicated to exploratory testing, user testing, performance testing or a staging environment prior to production. The environments should be ordered so that a version of the software cannot be deployed to a later environment before it’s been deployed and tested in an earlier stage. That way, the software cannot be deployed to production without having been tested in every previous environment first. This does not need to be a strict linear ordering. Some sets of tests may be run in parallel – such as user acceptance testing and performance testing. However there is very often one single production environment which is later than all others, and one single entry point which precedes all others. One of the principles of good deployment is repeatable deployments. This does not just apply to application code. Applications don’t run in a vacuum, and often have particular requirements of the underlying system in which they run. The configuration of that system should be automated and repeatable. There are two main issues: ensuring that new builds of machines are repeatable, and ensuring that once built, machines do not suffer from configuration drift, in which small manual configuration changes are made over time, resulting in a system which is not in a reproducible state. Scripting the configuration of a new machine is not a difficult process. It will always start from a known state and can have a number of tasks to install packages, put configuration files in place and start services until the machine is in a good state. Managing configuration drift is more tricky, as to counteract manual changes to configuration, your system must be robust enough to take a machine from an unknown state to a known state. There are a number of tools for managing your infrastructure configuration, such as CFEngine, Chef, and Puppet. Each of these is designed such that they can be run repeatedly on a machine to alleviate configuration drift. If you’re using one of these tools, you need to provide a means to deploy new versions of the infrastructure code. There are two means of distributing infrastructure code: Using a server (Chef Server, Puppetmaster) In this kind of system,   you’ll have a central server that distributes configuration code   to each machine in your environment. Deploying new versions of code   requires only deploying to the server, that will then distribute it   to the clients. Serverless (Chef Solo, Masterless Puppet) Here, you’ll need to   distribute the configuration code to each individual node and ensure   that each node runs the code. An alternative strategy to avoid configuration drift is to use the immutable server pattern, in which once a machine is configured it’s never touched again. In order to deploy a new version of the software, an entirely new machine is provisioned and the old one discarded. Configuration drift is avoided because servers have short lifespans and are frequently replaced by new instances. This is a natural fit in virtualised environments and where the application artefact is a virtual machine template with the app pre-deployed, but can also be achieved using containerisation technology such as lxc. There are a number of options for deploying your code: You should think about how you’ll discover hosts that you deploy to. In a simple scenario, your deployment script may have a hard-coded list of application servers that it deploys to.  In this situation, there’s a risk that the hard-coded list of servers drifts to differ from the number of servers which actually exist in reality. This risk grows more likely with larger and more dynamic infrastructures.  There are more involved host discovery mechanisms, such as internal DNS, Zookeeper, or using a message-queue based system such as MCollective. Since you should be deploying the same artefact to each environment, both for infrastructure configuration management code and for application code, you’ll inevitably find a need to inject configuration which varies between environments, such as URLs of dependent services. For application configuration, your deployment mechanism should provide a way of injecting environment-specific configuration files into each environment. For infrastructure configuration, your infrastructure tool should provide some means of achieving this. For example, Puppet 3 provides Hiera, a hierarchical datastore for managing these values. Extra care must be taken when managing secrets such as database passwords or SSL keys. You want to ensure: For example, in a three-tier app with database, application and web servers, the database server does not need to know the  SSL (secure sockets layer) private keys for the site, nor does the web server need to know the database credentials. If you are using hiera, then hiera-gpg provides a solution to this problem. It allows the injection of values from GPG-encrypted files. Only those with an appropriate private key can access the contents. By creating a GPG key for each host in an environment, you can decide on a host-by-host basis which host can access which sets of secrets. If you are using chef, then chef data bags provide a similar solution. In projects which have high availability requirements, the process of deploying small code changes to production frequently may incur an unacceptable loss of service, if each deployment results in a short period of downtime. Therefore, it’s important to consider what engineering is necessary to enable deployments which do not result in any downtime at all. This is all subject to what your definition of downtime is. Maintaining uptime for read-based operations is relatively simple: a caching layer which can serve from stale can hide the absence of application servers; a database is easy to migrate from one master to another if it is placed into read-only mode first. Maintaining uptime for write-based operations is trickier, and requires up-front thought and design. If you know that you’ll have high uptime requirements for write-based or transactional operations, you’ll need to consider how that will affect your architecture and infrastructure. As applications evolve over time, so do the requirements that they place on their databases. Database migration scripts are short pieces of database code which transform the database in some way for the benefit of the application. To achieve zero downtime deployments, you should decouple application deployments from database migrations. If you’re performing zero downtime deployments, you’ll necessarily end up with multiple different versions of the application running concurrently. Conversely, the application will need to be tolerant to the eventuality of a database migration script running concurrently within the application lifetime. Note that database migrations should be subject to the same rigorous deployment pipeline as application code. They should be deployed to testing environments first, and only go to production once they have been applied and verified against all other environments. Services which depend on one another via an application programming interface (API) can experience similar deployment problems as applications which depend on a database. For example, a frontend application which communicates with a backend application over an API of some sort.  Once again, the answer is to decouple deployments of the applications to make sure that the frontend application is tolerant to additions to the backend API, and that similarly the backend API can add functionality without disrupting the frontend application’s operation. Another method of avoiding failures during deployments is to make write operations asynchronous by posting them to a message queue. That way, when the backend system which consumes from the queue is disabled during a deployment, the frontend does not start seeing errors; rather, it just sees an increase in the time taken to see a write reflected in further read operations. Once you have deployed your application, you should determine whether the application is working as expected. If it’s not working, the deployment can be cancelled or rolled back. The test used to determine this is often referred to as a “smoke test”. A good smoke test is simple and fast, and exercises not just the application but also all of its essential dependencies. For example, if an application needs a database to be present to operate effectively, the smoke test should exercise an application code path which will fail if the database is not present or returns an error. If and when the smoke test fails, you should know what your response will be. The simplest option is to manually roll back to a previous version of the application – which should be easy enough if you have a versioned artefact repository to draw the application from. A solution with more sophistication may automatically detect the smoke test failure and cancel the deployment or roll back to the previous version. An ideal solution would not even add the new version of the application to the production load balancer until it has been smoke tested and verified good. If the application fails the smoke test, it is simply discarded; no rollback is necessary, and no interruption in service happens. This works particularly well with the immutable server pattern. From time to time, there may come a situation where you wish to deploy to production right now. This may be due to a published security vulnerability in a library you are using, or because a bug has hit production which has broken the system for a number of users. It may be the case that you subvert your usual deployment pipeline to fix things, then back-port the change you made in production (or “hotfix”) to your development environment and push it through the normal deployment process once the crisis is over. Should this be the case, then your cycle time is too long. In the ensuing post-mortem analysis of what went wrong, you should ask questions about why the deployment pipeline was not streamlined enough to accommodate a rapid deployment of a fix to production.","description":"We’ve identified some common principles for software deployment\nwhich we’ve applied in a number of different projects, with\ndifferent technology stacks and needs. These principles underpin a\nsoftware deployment process which meets user needs. Those principles\nare:","link":"/service-manual/making-software/deployment.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Design skills","indexable_content":"How designers work How to hire designers Job description templates Further reading Specific guidance for designers working on digital by default services. Those with the title of designer may have a particular focus on one or more specific design disciplines – interaction, graphic, UX – but a good digital service needs talented, flexible designers to help build user-centred products. Designers, user researchers and front-end developers should work together in one team, designing in-browser. This is a better way of working, avoiding silos and ensuring that decisions are made with complete awareness of the implications. As a result, the people you hire should already have worked like this, or at least understand it. Depending on the types of project you are tackling you may require a team of designers with a range of different skills. A good first hire for a team tends to be a strong interaction designer, however adding designers with graphic design skills and designers who also have a background in undertaking user research can also give your team additional flexibility and capabilities. We strongly believe that design and user experience is the responsibility of the entire team and must be considered from the outset of the project through to and beyond going live. UX includes how fast the servers are, to how the copy is written, to how the layout is implemented in code, and what the structure of the URLs is. It’s worth looking at Frances Berriman’s talk on this. When building a team ask to see examples of work and ask the designers to explain their practical contribution. You should look for the designer to share stories and documentation that demonstrates how they have worked closely with other members of the team including developers, content designers, user researchers and stakeholders in an agile and iterative way. When evaluating their design work, it is important that the designer can explain a strong rationale for their design decision making that is based in supporting user needs. This ability to explain their rationale convincingly is more important than their ability to show polished wireframes or designs in their portfolio. Often a lack of polished designs in a portfolio can indicate that a designer is highly collaborative or has strong development skills, so take care not to focus only on polished comps or layouts. You should also ask the designer to talk about any significant differences between the design they present in their portfolio and the actual live design. It is important that designers create appropriate work and are able to persuade stakeholders and their team to make good design decisions. Click either of the options below to download template Designer job descriptions.  Designer – Download as OpenDocument Format / Download as MS Word doc  Junior Visual Designer – Download as OpenDocument Format / Download as MS Word doc  Cabinet Office will help departments to recruit suitably skilled individuals through the Recruitment Hub.","description":"Those with the title of designer may have a particular focus on one or more specific design disciplines – interaction, graphic, UX – but a good digital service needs talented, flexible designers to help build user-centred products.","link":"/service-manual/the-team/designer.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Developer skills","indexable_content":"The importance of developers Skills and attributes Developers in the team Guidance Job description builds software with a relentless focus on how it will be used seeks collaboration and early feedback designs software they expect to operate and maintain leaves code simpler and better tested than when they started looks for opportunities to share progress and knowledge is always hoping to learn from colleagues and the wider community distinguishes the important from the urgent uses data to make decisions, building tools to gather that data have deep skills in at least one programming language be aware of the differences between a few languages and frameworks, and be pragmatic at picking the right one understand the core concepts of the internet and web – they should be able to give a good answer to the question ‘what happens when I click a link in a web browser?’ be deeply committed to testing their work with automated tests and exploratory testing be able to explain their work to people without particular technical skills Developers build software with a relentless focus on how it will be used. They continually improve the service by identifying new tools and techniques and removing technical bottlenecks Good digital services will require code to be written, adapted, maintained and supported. A team of skilled developers will be able to make sure that happens in an efficient and transparent way, and to help continually improve the service by identifying new tools and techniques and removing technical bottlenecks. No digital service can be effectively built, delivered, owned and operated without the technical skills to understand and improve the software enabling it. In order to provide the best service to your users it is vital that you are in a position to rapidly tailor that software to their needs and to the efficient running of the underlying systems. Developers will be able to work directly on those services, but are also an important part of service innovation as they bring ideas, generate prototypes and contribute to a rounded team.  Once a service is live the need for developers continues. There will always be technical optimisations that can be made – faster and more responsive systems, acting to mitigate the risks of a constantly changing security environment – but as you respond to new or more clearly understood user needs the software will need to adapt. As the policy and other context around a service changes the software may need to integrate with new systems or provide new features and a development team can help ensure that work is pro-active. A great developer: You would expect any developer to: It’s really important that your team is clear as to who makes technical decisions. Everyone on the team will have useful knowledge, skills and experience to bring to bear and you will need an agreed way to understand that input and make a call. Some teams accomplish that by appointing a Tech Lead or delegating certain decisions to a Technical Architect, others will put that responsibility in other roles. The important thing is that clear technical decisions be made by people equipped to understand the trade-offs involved. As your team grows it is likely that you will find a range of skills and experience. You should be ensuring a balance within the team, making sure that more junior team members are well supported and coached by more experienced team members, but that everyone’s ideas are considered and engaged with by the team and its decision maker. Read guidance in the manual of particular interest to developers. Click either of the options below to download a template Developer job description.  Download as OpenDocument Format / Download as MS Word doc  Cabinet Office will help departments to recruit suitably skilled individuals through the Recruitment Hub.","description":"Developers build software with a relentless focus on how it will be used. They continually improve the service by identifying new tools and techniques and removing technical bottlenecks","link":"/service-manual/the-team/developer.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: DevOps","indexable_content":"Why DevOps Good habits Warning signs Related guides in the Service Manual Further reading Culture Automation Measurement Sharing development quality assurance operations business based in different locations work for different organisations under completely different management structures culture automation measurement sharing release management (releasing software) provisioning configuration management systems integration monitoring orchestration (the arrangement and maintenance of complex computer systems) testing those who build and test software those that run it in production quality release management performance have a shared sense of ownership of the service have a shared sense of the problem develop a culture of making measurable improvements to how things work cross-functional teams – make sure your teams are made up of people from different functions (this helps with the team owning the end-to-end quality of service and makes it easier to break down silos) widely shared metrics – it’s important for everyone to know what ‘good’ looks like so share high and low level metrics as widely as possible as it builds understanding automating repetitive tasks – use software development to automate tasks across the service as it:            encourages a better understanding of the whole service       frees up smart people from doing repetitive manual tasks         encourages a better understanding of the whole service frees up smart people from doing repetitive manual tasks post-mortems – issues will happen so it’s critical that everyone across different teams learns from them; running post-mortems (an analysis session after an event) with people from different groups is a great way of spreading knowledge regular releases – the capacity for releasing software is often limited in siloed organisations, because the responsibilities of the different parts of the release are often spread out across teams – getting to a point where you can release regularly (even many times a day) requires extreme collaboration and clever automation DevOps tools (nearly always marketing) a DevOps team (in many cases this is just a new silo of skills and knowledge) DevOps as a job title (you wouldn’t call someone “an agile”) Configuration Management Monitoring Release Management What DevOps Means to Me What is this DevOps thing anyway? What is DevOps? (and the wall of confusion) There’s no such thing as a “DevOps Team” DevOps is a cultural and professional movement in response to the mistakes commonly made by large organisations. Often organisations will have very separate units for: In extreme cases these units may be: Communication costs between these units, and their individual incentives, leads to slow delivery and a mountain of interconnected processes. This is what DevOps aims to correct. It is not a methodology or framework, but a set of principles and a willingness to break down silos. Specifically DevOps is all about: DevOps needs a change in attitude so shared ownership and collaboration are the common working practices in building and managing a service. This culture change is especially important for established organisations. Many business processes are ready to be automated. Automation removes manual, error-prone tasks – allowing people to concentrate on the quality of the service. Common areas that benefit from automation are: Data can be incredibly powerful for implementing change, especially when it’s used to get people from different groups involved in the quality of the end-to-end service delivery. Collecting information from different teams and being able to compare it across former silos can implement change on its own. People from different backgrounds (ie development and operations) often have different, but overlapping skill sets. Sharing between groups will spread an understanding of the different areas behind a successful service, so encourage it. Resolving issues will then be more about working together and not negotiating contracts. The quality of your service will be compromised if teams can’t work together, specifically: The root cause is often functional silos; when one group owns a specific area (say quality) it’s easy for other areas to assume that it’s no longer their concern. This attitude is toxic, especially in areas such as: High quality digital services need to be able to adapt quickly to user needs, and this can only happen with close collaboration between different groups. Make sure the groups in your team: DevOps isn’t a project management methodology, but use these good habits in your organisation. While not unique to DevOps, they help with breaking down silos when used with the above principles: Like agile, the term DevOps is often used for marketing or promotional purposes. This leads to a few common usages, which aren’t necessarily in keeping with what’s been said here. Watch out for: Those interested in DevOps are often also interested in:","description":"DevOps is a cultural and professional movement in response to the mistakes commonly made by large organisations. Often organisations will have very separate units for:","link":"/service-manual/operations/devops.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Digital leaders","indexable_content":"Digital by default GOV.UK Digital leaders act as the single point of contact for the department’s strategic interactions with GDS, including co-ordinating digital activity for departmental agencies, arm’s length bodies and non-departmental public bodies. They are senior civil servants (generally executive or director general level) drawn from every department of government and the devolved administrations, and they have experience of leading large scale transformation programs inside and/or outside government. Digital leaders promote and encourage take up of ‘Digital by Default’ within their department, with stakeholders and customers. This means actively participating in the delivery of the Government Digital Strategy and Departmental Digital Strategy actions, ensuring these are fully embedded in the Department’s Business Plan. Digital leaders also act as the strategic governance mechanism for the GOV.UK domain, informing policy and strategic direction of digital.","description":"Digital leaders act as the single point of contact for the department’s strategic interactions with GDS, including co-ordinating digital activity for departmental agencies, arm’s length bodies and non-departmental public bodies.","link":"/service-manual/the-team/digital-leader.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Discovery phase","indexable_content":"Timescales Objectives The team Outputs workshops simple mock ups paper prototypes plenty of whiteboard diagrams a prioritised list of user needs a prioritised list of story cards to feed into project teams an understanding of the team and the capabilities required to complete the project the ability to scope and plan an alpha a decision to progress to next phase maybe some rough prototypes maybe some user personas a list of stakeholders and input from them about existing services understanding of existing services, including those run by non-government sources understanding of how many of your users will need assisted digital support, and what their user needs are Next phase: alpha What are the needs of your users? What services currently meet those? How are they performing? What technological or policy related constraints might there be? Before you start building a service you need to build up a picture of what the context for that service is. That means lots of user research, close analysis of policies, laws and business needs, and workshops and interviews which establish the criteria for success of your service. The discovery phase will give you a high-level understanding of user needs, what the existing service landscape looks like and a sense of what your initial prototypes will explore. You’ll need to think about user needs for digital and assisted digital users. As a start, you’ll need to understand what proportion of your users you think will need assisted digital support. The high-level business context will become clear, and you’ll begin setting targets for your KPIs. You’ll also get a better understanding of the legacy interfaces and infrastructure you must deal with, and what existing processes are in place for replacing or decommissioning these. This information is found through: A small team will be required, consisting of your stakeholders and any core team members that have been identified, including the service manager. The phase should not take longer than 4 to 8 weeks. During the final week you should be setting up the broad scope of a project and an initial set of user stories (also known as a backlog) to work to. This is known as an ‘inception’. At the end of the phase a decision should be made whether to proceed to the alpha phase. You will leave the discovery phase with:","description":"What are the needs of your users? What services currently meet those? How are they performing? What technological or policy related constraints might there be?","link":"/service-manual/phases/discovery.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Discussion Guides","indexable_content":"Where and how you might use them Example Discussion guides are used in order to ensure that focus groups and 1:1 interviews cover the required topics, and information is obtained from the sessions that will address the needs of the research. In preparation for a focus group or 1:1 interview it is helpful to generate a list of questions that address the information that you’re interested in obtaining from the sessions. These questions should be open ended, and structured in a manner that will help elicit information from respondents in a sensible flow. As suggested by the name, discussion guides should be used as a guide to the discussion, and in comparison with a structured questionnaire, questions and areas for coverage should not read out verbatim. This enables the discussion to be led by respondents’ own experience. Typically each section of the guide would include time guidance to ensure all areas can be covered and it should also indicate when stimulus is being used, and specify participants’ tasks. Writing a discussion guide should be an iterative process, and once the initial draft has been written it is helpful to get input from other people on the project team. A discussion guide written for testing that was conducted with BIS experts for the development of GOV.UK (PDF, 157kb).","description":"Discussion guides are used in order to ensure that focus groups and 1:1 interviews cover the required topics, and information is obtained from the sessions that will address the needs of the research.","link":"/service-manual/user-centred-design/user-research/discussion-guides.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Email addresses","indexable_content":"1. Make the field long enough 2. Make it clear why you're asking for someone's email 3. Help people to enter their email address correctly 4. Check that the user has access to their email account On this page: Validate it Don’t disable paste Ask for it twice Play it back Make the field long enough Make it clear why you’re asking for someone’s email Help people to enter their email address correctly Check that the user has access to their email account How to ask people for their email address. Some email addresses are very long. Your email field should be able to fit at least 95% of email addresses. Two-thirds of the page width should be enough to do this, but analyse your user data to be sure. An email address is personal data that can easily be abused. Help to reassure users by making it clear what you will and won’t do with their address. This will also help users with multiple email addresses choose which one to provide. If a valid email address is required then you should help your users enter it correctly. The following techniques can help. Check that what they’ve entered conforms to the basic email address format. Check for common typos like ‘gnail’ instead of ‘gmail’ (read a case study here). Users may have a perfectly good reason for pasting an email address (for example if it’s very long). By doing this you may capture some accidental mis-types. It’s not failsafe though – users can make the same mistake twice, especially if they’re typing quickly (or just misremembered their email address). It also adds additional effort on to your users. An alternative way of helping users spot a mistyped email address is to play it back to them. Make sure it’s obvious how to correct any errors they might spot. If email is an integral part of your service, you can confirm whether the user has access to the email address they give you using an email confirmation loop. Find out more about email confirmation loops. Discuss this page on Hackpad","description":"How to ask people for their email address.","link":"/service-manual/user-centred-design/resources/patterns/email-addresses.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Email confirmation loops","indexable_content":"1. Consider alternatives 2. Confirmed email addresses don't prove identity 3. Set an expiry time on the email 4. Blocking versus non-blocking loops 5. Activating your account On this page: Consider alternatives Confirmed email addresses don’t prove identity Set an expiry time on the email Blocking versus non-blocking loops Activating your account tell them where the activation email has been sent explain that they need to click the link in the email to proceed let them resend the activation email let them use the service remind them that they need to activate their account tell them where the activation email has been sent let them re-send the activation email If email is an integral part of your service then consider confirming whether users have access to the email address they give you. Email confirmation loops can be a very disruptive user experience, forcing people to wait and then switch from a website to email and back again. They may not have access to their email at that point in time. Confirmation loops tend to increase drop-off rates, because some users struggle to complete the loop. They don’t notice the email, or it goes in to their spam folder, or they give up on the service before the email arrives. You decision should be based in part on what information you intend to send by email. Consider just helping users enter their email correctly instead. An email confirmation loop verifies that the address supplied by the user is a real one, but doesn’t verify that they alone own that address, or that they check it regularly.  Some users may supply temporary addresses as a way of getting around confirmation loops. Other people may share their account with someone else, or the account may have been hacked. For added security you can set an expiry date on the email, so that the link can’t be used after a certain time period. If the user attempts to use an expired link then they should be told this and given an opportunity to re-send a new link. There are two versions of the email confirmation loop. In the ‘blocking’ version the user cannot use the service until they have completed the loop. In the ‘non-blocking’ version they can continue to use the service, but will be reminded regularly that they need to confirm their email. Some functionality may not be available until they have done this. Blocking loops have a slightly simpler flow, but if the users can’t complete the loop they’re unable to use the service at all. It’s especially important that the emails are sent instantly. Non-blocking loops require more careful design, and you can’t guarantee that all users emails are confirmed, but there’s no risk that the loop will stop people from accessing your service. Some services start users in a non-blocking loop initially and then transition to a blocking loop. Use the phrase ‘Activate your account’ to describe the process of confirming an email address. The activate your account page should be shown immediately after the user provides their email address, or if they try to sign in before confirming their email. It should: For blocking loops this should be the only page the user sees if they try to sign in before activating their account. For non-blocking loops, if a user signs in before activating their account, you should: When a user clicks on the link in the activation email take them to a page that confirms that they have activated their account. You may or may not require them to sign in at this stage, depending on where in the flow the activate your account screen appears. Discuss this page on Hackpad","description":"If email is an integral part of your service then consider confirming whether users have access to the\nemail address they give you.","link":"/service-manual/user-centred-design/resources/patterns/email-confirmation-loop.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Handling email","indexable_content":"Handling email Getting emails to users ensure there is a mail exchanger (MX) record set up for the domain from which you send email implement the Domain-based Message Authentication, Reporting and Conformance (DMARC) standard to improve the likelihood that your email will be delivered correctly Emails to users of your service should be sent from a human-monitored email address that originates from the domain servicename.service.gov.uk (and not the dept/agency or any other domain name). Users are interacting with the service and that is where they will expect communications to come from. In order to protect users from spam email providers put in place a variety of checks. It is often a good idea to use a trusted specialist third-party to dispatch email as they will have tools and expertise to help ensure that you pass those checks. As a minimum you should: Before releasing your service you should test your email delivery. As a minimum you should use your service with registered email addresses from a range of popular email providers and ensure that emails arrive as you expect. For more on good practice on email in government, see the Common Technology Services email blueprint","description":"Emails to users of your service should be sent from a human-monitored email address that originates from the\ndomain servicename.service.gov.uk (and not the dept/agency or any other domain name). Users are interacting\nwith the service and that is where they will expect communications to come from.","link":"/service-manual/domain-names/email.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: End user devices","indexable_content":"The mandate for change Start with user needs overly expensive end user devices and associated services an inability to change user IT to meet user needs a growing divergence from the modern user-friendly consumer IT being exploited in other sectors an entrenchment of incumbent products and suppliers through technical and commercial dependencies or “lock in”. This guidance aims to address historic issues with poor user experiences of device technology, including user tools that do not meet user needs, degrade civil service productivity, and provide poor user experiences. These issues have been compounded by: The Civil Service Reform Plan requires departments to address “frustrating” IT tools for civil servants. A workplace transformation programme will streamline working practises, supported by effective IT. The Government Protective Marking Scheme Review and resulting Security Classification Policy, together with the Open Standards policy, require a reform of the principles by which IT tools for civil servants are designed, procured and managed. Civil servants are expected to take responsibility and apply reasonable judgement when dealing with information, removing the need for overbearing technical controls. The government and departmental digital strategies establish user needs as central to digital services and tools. This requires a step change in agility to meet changing user needs, together with an excellent user experience. We need to get 4 basic things right: The end user device strategy will help departments and suppliers understand what kinds of design decisions they should now be making. There’s specific guidance on the standards that need to be met to ensure that digital services don’t become heavily dependent on particular products or suppliers, and new security guidance has been developed by CESG (the government’s specialist technology security advisors) for those working with government information across all types of mobile devices.","description":"This guidance aims to address historic issues with poor user experiences of device technology, including user tools that do not meet user needs, degrade civil service productivity, and provide poor user experiences.","link":"/service-manual/technology/end-user-devices.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Ethnographic research","indexable_content":"How ethnographic research works Where and how you might use it When not to use Participants Timescales Ethnographic research usually involves observing target users in their natural, real-world setting,  rather than in the artificial environment of a lab or focus group. The aim is to gather insight into how people live; what they do; how they use things; or what they need in their everyday or professional lives. Ethnographic research relies on techniques such as observation, video diaries, photographs, contextual interviews, and analysis of artefacts such as for example devices, tools or paper forms that might be used as part of a person’s job. Observations can be made at home, at work, or in leisure environments. People can be studied with their family, on their own, with work colleagues, or as part of a group of friends. Often one participant may be recruited, but several more may be studied as part of that person’s family or friends. Data collection can range from a 4-5 hour contextual interview, through to following a participant for several days, or even a longitudinal study over several weeks or months to investigate, for example, how a particular product or service might be used over time. It doesn’t necessarily involve ‘full immersion’ in a person’s life: it can involve a depth interview in a person’s home or it might involve a person simply maintaining their own video diary over a period of time. Ethnographic research can provide extremely rich insight into ‘real life’ behaviour, and can be used to identify new or currently unmet user needs. This approach is most valuable at the beginning of a project when there is a need to understand real end user needs, or to understand the constraints of using a new product or service by a particular audience. Ethnographic research can provide a significant amount of qualitative data, and analysis can be time consuming. NOTE: The term ‘ethnographic’ can be misused, it’s currently a bit of a ‘buzzword’ with some agencies who may not fully understand the approach.  It is recommended that a specialist agency is used, who can demonstrate successful case studies (collecting and analysing the data). In principle, anyone could participate in this type of research. As with any user research, the recruitment of suitable participants is key. The full implications of the research should be fully explained to potential participants, as some may not feel comfortable with this level of intrusion in their lives. Depending on the study needs and the approach, but 6-8 weeks from briefing to results can provide rich insight. It may take time to build trust with participants, and the analysis period needs to be sufficient to be thorough. Ethnographic research can be expensive and time consuming, but this depends on the needs of a particular project. The benefits derived can be extremely valuable.","description":"Ethnographic research usually involves observing target users in their natural, real-world setting,  rather than in the artificial environment of a lab or focus group. The aim is to gather insight into how people live; what they do; how they use things; or what they need in their everyday or professional lives.","link":"/service-manual/user-centred-design/user-research/ethnographic-research.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Expert reviews","indexable_content":"How expert reviews work Where and how you might use it Weaknesses and when not to use Timescales Cognitive walkthrough Expert reviews – also known as heuristic evaluations – are low cost usability methods that don’t involve participation of real end users. An ‘expert’ usability evaluator can assess a product (or web site) against a known set of ‘heuristics’, or usability guidelines (best practice). An alternative approach is to conduct a ‘cognitive walkthrough’ against specific use cases or scenarios. Ideally two usability experts will conduct the review independently, and then meet to discuss and agree the findings before making recommendations to the service manager. A list of widely accepted (although not necessarily validated) heuristics are provided by Jakob Nielsen or an alternative set can be found in the International Standard ISO 9241. A ‘cognitive walkthrough’ is a usability inspection that aims to identify usability issues by focusing on how easy it is for users to accomplish specific tasks with the system (or website). This method starts with identifying the user goals, then conducting a task analysis to specify the sequence of steps or actions required to achieve each task. The usability expert, along with designers and developers, then walks through these identified steps to assess the extent to which a user can achieve their goal. Both of these approaches can be used to evaluate an existing product or website, or as a quick, low cost method of evaluating a product in development. These approaches can be considered to be ‘better than nothing’, but will never provide the same quality insight as testing with real end users. However, a competent usability specialist will often identify issues that are subsequently seen in traditional user testing or in actual use. Some agencies conduct expert reviews as the first step of all usability projects. The fundamental weakness of any expert review is that it doesn’t involve use by real end users. Some people may therefore consider this to be purely opinion, but input from an experienced usability expert is better than no user testing at all. For a simple website application this could be turned around in 1-2 days.","description":"Expert reviews – also known as heuristic evaluations – are low cost usability methods that don’t involve participation of real end users. An ‘expert’ usability evaluator can assess a product (or web site) against a known set of ‘heuristics’, or usability guidelines (best practice). An alternative approach is to conduct a ‘cognitive walkthrough’ against specific use cases or scenarios.","link":"/service-manual/user-centred-design/user-research/expert-review.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Feedback pages","indexable_content":"GOV.UK Feedback pages Further discussion Discuss this page on Hackpad Discuss user satisfaction on Hackpad Blog post: How good is your service? How many users give up? (August 2015) All digital services that government provides must have a Feedback page on GOV.UK. Every service must link to their Feedback page to collect user satisfaction and feedback.  Feedback pages are hosted on GOV.UK. This is so satisfaction scores can be compared across services. To get a Feedback page on GOV.UK – request a content change.","description":"All digital services that government provides must have a Feedback page on GOV.UK. Every service must link to their Feedback page to collect user satisfaction and feedback.","link":"/service-manual/user-centred-design/resources/patterns/feedback-pages.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Focus groups, mini groups, and 1:1 interviews","indexable_content":"How they work Where and how you might use them Weaknesses and when not to use Participants Focus groups, mini groups, and 1:1 interviews involve unstructured interviews or group discussion. A focus group is normally made up of 6-12 people, although sometimes mini groups are favoured (4-5 people) as they can lead to a greater depth of discussion. 1:1 interviews are conducted by a moderator with a single respondent, and sometimes these are conducted over the phone. Interviews and discussion groups are both facilitated by a trained moderator using a specially designed topic guide in order to ensure the discussion is focused and keeps on topic. Sessions normally last 1-2 hours and will often involve participants being shown stimulus – eg communications materials to inform discussion, and sessions may include interactive techniques eg role play scenarios. Focus groups and 1:1 interviews are useful techniques for exploring and mapping reasons for attitudes and behaviour, understanding how target audiences approach issues or may be encouraged to change. They also enable participants reactions to be monitored, and the moderator to probe interesting issues when necessary. Dedicated viewing facilities are often used that enable sessions to be recorded, and interested parties can observe the sessions (often via a two way mirror) in a separate room to the participants. This often results in the research having more credibility as interested parties will have been able to view the sessions, and witness the findings first hand – this also means key issues can be acted upon quicker as observers can feed straight back to their teams. Some sessions take place in community settings, encouraging less confident/harder-to-reach audiences to attend. Focus groups can also be conducted online. The number of groups is often dictated by the budget available, but the average project will have 4 to 6 groups, with larger ones having between 10 and 20. Besides budget, other factors that influence the number of groups used are the number of potential users groups that need to be covered, and the number of geographic areas. Focus groups and 1:1 interviews can be expensive compared with other types of research, while the small samples size means that findings are not statistically significant. There is, therefore, a risk of generalising across audience groups. Analysis of the sessions can also be time consuming. Moderators also need to build a rapport with respondents. If he or she fails to do this, and can’t control the group adequately, it can result in the sessions being dominated by one or two participants, and biased data being collected. Participants can be obtained from the general population and hard to reach groups.","description":"Focus groups, mini groups, and 1:1 interviews involve unstructured interviews or group discussion.","link":"/service-manual/user-centred-design/user-research/focus-groups-mini-groups-interviews.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Form structure","indexable_content":"1. Know why you’re asking every question 2. Design for the most common scenarios first 3. Start with one thing per page 4. Examples 5. Further reading On this page: Tips Know why you’re asking every question Design for the most common scenarios first Start with one thing per page Examples Further reading why does the service need that information? who uses the information and what for? which users need to provide the information? how will you check that the information is accurate? how will you keep the information up to date? have one thing per page only merge pages when you have clear evidence from user research that this would improve the user experience one piece of information to understand one decision to make one question to answer understand what they’re being asked to do  focus on the specific question and its answer find their way through an unfamiliar process use the service on a mobile device recover easily from form errors save people’s work automatically as they go capture analytics about each question handle branching questions and loops How to structure web forms for GOV.UK services. Before you start, make a list of all the questions you want to ask. For each question, you need to know: We call this list a ‘question protocol’; it’s different from the form itself, because it’s about how you use the answers. A question protocol forces you (and your department) to question why you’re asking users for each item of information, and gives you a way of challenging and pushing back if you need to. Once you have a question protocol you can start to decide how to order the questions. Structure the form so most users have a simple, quick path. Use branching questions to hide questions from people who don’t need to answer them. You’ll need to make difficult decisions about which users to prioritise, so make sure you have good data from the business about them. Most public-facing government services are used infrequently, by people with a broad range of digital skills and confidence. Because of this, we recommend that you: ‘One thing’ could be: We’ve found that this approach works for both high and low confidence internet users, and that people are generally very tolerant of clicking through multiple pages. Having only one thing on a page helps people to: It also helps you to: ‘One question’ doesn’t necessarily mean one form field. For example, date of birth is best captured with three text fields. For page titles you can use either a question or a statement. For example - ‘What is your date of birth?’ or just ‘Date of birth’. Use questions or statements consistently to help users get into a rhythm of answering. This lets them focus on the content of the questions rather than their presentation. Try to ask questions that might result in the user being ineligible for the service as soon as possible, so you don’t waste people’s time. For a good example of this approach, try the Register to vote service on GOV.UK (you can get to the last page without submitting any data). Joe Lanman has written more about the things we learned designing ‘Register to vote’. The Question Protocol: How to Make Sure Every Form Field Is Necessary Discuss this page on Hackpad","description":"How to structure web forms for GOV.UK services.","link":"/service-manual/user-centred-design/resources/patterns/form-structure.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Funding your digital service","indexable_content":"Discovery and alpha Beta and live Managing business cases for digital services      investment approval from your department         Cabinet Office spend control approval to spend on external resources, eg on suppliers         investment approval from your department         Cabinet Office spend control approval to spend on external resources, eg on suppliers         any other approvals specific to your service eg headcount or salary/grade changes         your anticipated total spend on the entire programme (including non-digital aspects) is higher than your organisation’s delegated authority as set by HM Treasury         your digital spend is above £10 million         typically enables smaller, more frequent funding approvals         may form part of a larger programme of transformational change    There are 5 phases of service delivery — discovery, alpha, beta, live and retirement. You’ll need to get approval at different phases when you build your service.  Discovery is right at the start of your project — where you research user needs, find out about any policy or technological constraints and get a picture of what your first prototypes will explore. During the alpha phase, you start building and testing those prototypes. To start discovery, you need approval for discovery and alpha. This means: investment approval from your department Cabinet Office spend control approval to spend on external resources, eg on suppliers You don’t normally need a detailed business case for discovery and alpha because they inform the development of the bigger investment case for beta and live. As a guide, discovery and alpha together shouldn’t cost over £750,000. If you plan to go above this, you should question whether you’ve sized the discovery and alpha correctly — if you do exceed this figure you’ll need a detailed business case. The beta phase is where you build a fully working prototype that you’ll test in public. The live phase is when your public beta has been tested and is ready to release. To go to beta you’ll need approval for beta and live. This means: investment approval from your department Cabinet Office spend control approval to spend on external resources, eg on suppliers any other approvals specific to your service eg headcount or salary/grade changes You need to submit a business case to HM Treasury (HM Treasury published guidance) if any of following apply: your anticipated total spend on the entire programme (including non-digital aspects) is higher than your organisation’s delegated authority as set by HM Treasury your digital spend is above £10 million If you need to approach HM Treasury and your digital spend is below £10 million, just include it concisely within your programme business case rather than writing a separate business case for it. HM Treasury has introduced the ‘programme business case’, where detail can be added by phase. This is because agile delivery: typically enables smaller, more frequent funding approvals may form part of a larger programme of transformational change The programme business case can be used for projects or programmes that have agile delivery elements. It replaces the traditional approval process (strategic outline case followed by outline business case and full business case). However, these still apply for large investment decisions involving lengthy public sector OJEU (Official Journal of the European Union) procurement processes.  There’s more information about this in the HM Treasury clarification on business cases for agile. Get involved To give feedback, make a suggestion or share your experience, use the governance guidance hackpad.","description":"There are 5 phases of service delivery — discovery, alpha, beta, live and retirement. You’ll need to get approval at different phases when you build your service. ","link":"/service-manual/governance/funding-your-digital-service.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Gender and sex","indexable_content":"1. Avoid asking about gender and sex 2. Don't say 'sex' when you mean 'gender' 3. Do not try to infer gender from title 4. Allow users to declare an unspecified gender 5. Don't assume that 'male' first is the correct order 6. Asking about sex instead of gender 7. Further reading On this page: You may be breaking the law You may offend or upset your users UK Germany Australia Avoid asking about gender and sex Don’t say ‘sex’ when you mean ‘gender’ Do not try to infer gender from title Allow users to declare an unspecified gender Don’t assume that ‘male’ first is the correct order Asking about sex instead of gender Further reading some titles are not gendered (Dr, Rev, Major) title can be changed by deed poll to a title contrary to one’s gender/sex female male unspecified intersex prefer not to say Think very carefully before you decide to ask your users about gender or sex. Sex is a protected characteristic in the Equalities Act 2010. Gender isn’t, but the closely related concept of gender reassignment is. Treating people differently because of protected characteristics may be discrimination and may therefore be illegal. The act applies to the public sector and to services. Many people consider that questions about gender or sex are sensitive or inappropriate in a government context.  For people who are coming to terms with differences between their sex and gender, being asked about sex or gender may be particularly difficult and stressful.  Sex is biologically determined, and can be male, female, or intersex. Gender is a social construct about how people choose to present themselves in the world.  Trying to guess someone’s gender from a title is a bad idea because: Do not collect title as a required answer, due to the huge variety of titles in use. Gender is not a clearly defined ‘female’ or ‘male’ construct. It can include a range of identities in-between, and labels identifying these identities are still in a state of flux.  Some people may also be in transition, or move between different genders frequently. This makes it difficult to provide sensible options on a form.  For a few people, being forced to declare themselves as ‘female’ or ‘male’ may lead to dropping out of a form.  Historically, the assumption was that ‘men’ came before ‘women’ so the traditional order of options has been ‘male first’. This is no-longer appropriate as an automatic assumption. Please test with ‘female’ as the first option and report your findings on the wiki. It is only appropriate to ask for someone’s sex in a context where the biological data is necessary – typically, only for medical transactions. Unfortunately, we also have to recognise that we have to deal with legislation that may require ‘sex’ as a legal identifier. If you must ask for ‘sex’ then ask for ‘female’, ‘male’ and at least 1 of the remaining options, depending on circumstances: In the UK an Early Day Motion has been submitted in support of giving legal recognition to those who do not associate with a particular gender. In Germany, the Personal Status Law (paragraph 22) states that if a child’s sex cannot be determined at birth then the law allows for birth to be registered with the ‘sex’ box left blank. The Australian Government guidelines on sex and gender specify that when a government form asks about sex or gender, then it must provide these options: M (male), F (female) or X (Indeterminate/Intersex/Unspecified). Discuss this page on Hackpad","description":"Think very carefully before you decide to ask your users about gender or sex.","link":"/service-manual/user-centred-design/resources/patterns/gender-and-sex.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Governance across the life of a service","indexable_content":"Discovery Alpha Beta Live Retirement      establish conditions for delivering a successful service         decide whether to proceed to alpha         review and adjust your team and governance for alpha based on what you learn         decide whether to proceed to beta         review and adjust your team and governance for beta based on what you learn         complete a business case and get approval for it         get Cabinet Office spend control approval, if necessary         prepare for and pass a digital by default service standard assessment    There are 5 phases of service delivery — discovery, alpha, beta, live and retirement. Governance is important across all these phases and ensures the transition between them is seamless. People who govern must anticipate problems that could affect delivery and make sure it isn’t slowed down. During discovery your team will be uncovering new information rapidly and you’ll need to support it by making decisions about the direction of the product and removing blockers. You’ll use this phase to: establish conditions for delivering a successful service decide whether to proceed to alpha review and adjust your team and governance for alpha based on what you learn You may need approval from Cabinet Office to spend money on suppliers and should start work on your business case. Find out more about governing the discovery phase Your team will be testing prototypes with users and you’ll start to get a feel for what the service could look like. You’ll use this phase to: decide whether to proceed to beta review and adjust your team and governance for beta based on what you learn You may need approval from Cabinet Office to spend money on suppliers and continue to work on your business case. To progress to beta you’ll need to: complete a business case and get approval for it get Cabinet Office spend control approval, if necessary prepare for and pass a digital by default service standard assessment You must ensure you do these things in enough time to enable the delivery team to move from alpha straight into the beta phase. You’ll slow down delivery if you don’t. Find out more about governing the alpha phase. The purpose of beta is to develop the service and test it with users. As you do this, you may need to scale to achieve your objectives. You should adjust governance arrangements to make sure that they fit your scale and structure for this phase. You’ll also need to update your business case and get further Cabinet Office spend control approval for the service if you’re spending money on external resources (eg suppliers). If you move outside agreed tolerances for costs and benefits you will need investment re-approval. To move into live service operations, you’ll need to pass a service standard assessment. Find out more about governing the beta phase. You’ll need to make sure that you have the resources and practices in place to continuously and iteratively improve the service. This will make sure it responds to changing user needs and policy development.  The service standard assessment process will apply if the service undergoes a major change. Find out more about governing the live phase. An existing service may be withdrawn or replaced. If you do this, you’ll need to indicate what, if anything, users are expected to do instead, or signpost them to the replacement service. As part of shutting down a service you must ensure that data is held according to regulatory or policy requirements. Get involved To give feedback, make a suggestion or share your experience, use the governance guidance hackpad.","description":"There are 5 phases of service delivery — discovery, alpha, beta, live and retirement. Governance is important across all these phases and ensures the transition between them is seamless. People who govern must anticipate problems that could affect delivery and make sure it isn’t slowed down.","link":"/service-manual/governance/governance-across-the-life-of-a-service.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Governing a service after scaling up","indexable_content":"Governing more than one team Get your teams working together Plan for scaling      make sure everyone understands the service vision and goals — especially new starters         have teams plan together and think about what they’ll individually be responsible for         where possible look at splitting work by theme, product or feature sets to minimise dependencies between teams         think about whether it makes sense to share specialist people across teams, eg software architects and people working in security, policy or finance         there are communication overheads         there’s less informal communication and learning         it’s harder to establish a shared culture         video communication         social media         regular face-to-face meetings         what has your team done since we last met?         what will your team do before we meet again?         is anything slowing your team down or getting in their way?         are you about to do something that will affect another team’s plans?         consulting with the delivery team on who should be added when         making sure the team can devote time to integrating new people         making sure new people will have the tools they need to be productive on the day they arrive    When scaling your team make sure you continue to follow the GDS governance principles to keep governance to a minimum. As soon as you have more than one team you’ll need shared planning, coordination and management of dependencies. This can mean a small increase in the amount of work, but don’t lose sight of the aim to have self organising, self managed teams. Scaling up doesn’t mean you need to add more people who govern; don’t complicate governance processes as you scale. You need to: make sure everyone understands the service vision and goals — especially new starters have teams plan together and think about what they’ll individually be responsible for where possible look at splitting work by theme, product or feature sets to minimise dependencies between teams think about whether it makes sense to share specialist people across teams, eg software architects and people working in security, policy or finance It’s better for multiple teams to be sitting in the same location — just as a single team benefits from this. When your teams don’t sit together: there are communication overheads there’s less informal communication and learning it’s harder to establish a shared culture This is particularly problematic when ‘business’ and delivery team members don’t sit together. If you can’t find a suitable shared space or team members live and work in different parts of the country, split the team. There are ways to minimise the problems separation causes, eg: video communication social media regular face-to-face meetings You can also get team representatives together in a cross team stand up (sometimes called a ‘stand up of stand ups’), where the following questions are explored: what has your team done since we last met? what will your team do before we meet again? is anything slowing your team down or getting in their way? are you about to do something that will affect another team’s plans? The last question helps teams to gain early sight of any dependencies or cross team issues that will need joint remedy. The frequency of these meetings is best determined by the teams and the pace of delivery. It’s important to plan for changes in team composition. Planning activities include:  consulting with the delivery team on who should be added when making sure the team can devote time to integrating new people making sure new people will have the tools they need to be productive on the day they arrive When you have more teams, don’t force them to work in exactly the same way as each other. Delivery teams work best when they can control how they work.  More from the service manual on scaling Scaling a service team When to scale up Get involved To give feedback, make a suggestion or share your experience, use the governance guidance hackpad.","description":"When scaling your team make sure you continue to follow the GDS governance principles to keep governance to a minimum.","link":"/service-manual/governance/governing-a-service-after-scaling-up.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Governing the alpha phase","indexable_content":"Delivery and assurance Funding and approvals Team      delivery and assurance         funding and approvals         the service delivery team         what the service is         why it’s important         who it’s important to         making it easier and quicker for users to register for and use a service          reduction of print and post costs         more accurate user data         cost reduction through labour saving         people         software         technical infrastructure          team environment (facilities) and support         continuous improvement         assisted digital    When you’re governing the alpha phase of a service, you’ll need to consider: delivery and assurance funding and approvals the service delivery team Revisit your vision and goals The service vision will probably have evolved as you learned more during discovery. Even if it hasn’t changed, it’s important to revisit it in this phase as your team is likely to change or expand.  Doing this helps ensure everyone understands: what the service is why it’s important who it’s important to Review your governance arrangements Your team may take on a different shape as you start to build a prototype. As the team and goals change, make sure your governance team is still closely involved in supporting the team delivering the service. Identify and mitigate risks A crucial outcome of the alpha phase is that the team can identify the biggest risks to the service early on. Things that may help manage risks include using iterations to test incrementally and adding critical risks to the team wall. Find out more about identifying risks during alpha. Create a roadmap based on the outcome of your discovery We use roadmaps to describe the high level direction a service will take. Roadmaps need a firm vision with goals that are focused on outcomes.  It’s important to establish goals during discovery and create the roadmap with the team. The roadmap will inform the business case. Your roadmap may identify that the service is broken up into a number of segments, each of which have their own beta phase and then get separately put into live service. Find out more about identifying goals during alpha. Measure your progress You should use the measures you agreed to use during discovery. If you decide to measure progress in a different way, you need to make sure this is really necessary and get everyone involved in the decision to change. The team wall is how delivery teams manage their work — it’s essential everyone understands how the wall can help show progress. Go to show and tells A show and tell is where the team demonstrates progress by showing what they’ve been doing. It’s an opportunity to ensure user needs are being met.  The team may ask for help in unblocking something, or need a decision on something that needs your steer. Clarify benefits By testing prototypes with users you will verify whether or not the expected benefits can be realised. Some examples of benefits from digital services include: making it easier and quicker for users to register for and use a service  reduction of print and post costs more accurate user data cost reduction through labour saving Identify costs for beta and live Consider the expected lifetime of the service and costs for: people software technical infrastructure  team environment (facilities) and support continuous improvement assisted digital Revisit procurement needs You need to evaluate the size of team you need for beta and get costs for things like cloud services and tools needed for development so you can build a set of costs for spend approval and the business case. Rate cards and price lists available in procurement frameworks can help you estimate costs, as can actual spend data of similar services. Develop your business case and get approval If you haven’t already appointed a senior responsible officer (SRO), should appoint one now to sponsor the business case. This could be the service manager or someone else senior in the business. Seek funding for your digital service with a business case for further investment approval and Cabinet Office spend control approvals. You can’t start beta until you have an approved business case. Digital by default service standard assessment The Digital by Default Service Standard is a set of criteria for digital teams building government services to meet. Meeting the standard will mean digital services are of a consistently high quality.  This includes creating services that are easily improved, safe, secure and fulfill user needs. The result of an assessment could mean you continue to build towards a beta service, make some changes and come back for re-assessment or stop what you are doing. Plan for assurance In addition to the in built assurance activities at the team level (eg pairing, show and tells, user research and retrospectives), you can use your department’s internal assurance team to provide an independent view. A valuable side-effect of this assurance can be identifying potential opportunities and dependencies with other departmental initiatives that could impact your service development. If your service is, or is part of, a major project then you’ll have developed an Integrated Approval and Assurance Plan that defines appropriate assurance activities. In these circumstances you’d also expect to have a Major Projects Authority (MPA) Project Assessment Review towards the end of alpha to help the shaping of beta and the development and approval of your business case. Even if you’re not subject to MPA assurance, you should include assurance and approval points in your service roadmap. Unblock and support the team Teams need help with things that slow them down and are outside of their control. During alpha they’ll need you to be closely involved on a daily basis, so that you can help them with decisions, encouragement and ideas. You’ll need to anticipate problems and make decisions promptly to avoid slowing down delivery and ensure a seamless transition from the alpha to beta phase. Revisit the size and shape of the team You’ll probably need more user researchers, developers and designers to help deliver your alpha, particularly as you start building prototypes of the service. You may also need finance people, economists and analysts to help with your business case. During alpha you’ll also need to work out the team structure and composition you need for beta and beyond.  Consider if you need to scale At this point your team may need to think about adding more people and how that will affect delivery. Review team needs As your team changes, it’s important to revisit their needs. New skills may mean new tools, and new people will certainly mean more space and equipment. Remember to keep the service team seated together so that they can work closely. Find out how to get the right working environment. Previous: governing the discovery phase Next: governing the beta phase Get involved To give feedback, make a suggestion or share your experience, use the governance guidance hackpad.","description":"When you’re governing the alpha phase of a service, you’ll need to consider:","link":"/service-manual/governance/governing-the-alpha-phase.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Governing the beta phase","indexable_content":"Delivery and assurance Funding and approvals Team      delivery and assurance         funding and approvals         the service delivery team         people         software         technical infrastructure          team environment (facilities) and support         continuous improvement         assisted digital    When you’re governing the beta phase of a service, you’ll need to consider: delivery and assurance funding and approvals the service delivery team After a successful alpha, the service vision and goals will have evolved into something more concrete. You’ll have investigated risks and gained a more thorough understanding of your users and their needs. Review governance You should review the governance arrangements you’ll need for the beta service, bearing in mind that you may have multiple teams working towards a common set of objectives. Keep your governance arrangements as lightweight as possible whilst still supporting development of the service. Plan for known risks A successful alpha will have investigated most of the areas of risk and uncertainty in your service. Use this to inform your roadmap and try to identify and plan for risks that are specific to beta. Including risks in your roadmap will help you focus on the risks that are relevant to delivery. Measure progress The most important measures of progress are seeing the service being delivered and user satisfaction. The team wall is another useful way of seeing how delivery is progressing — it’s essential everyone understands how the wall can help show progress. Find out more about how to decide what to measure. Keep going to show and tells This is where the team demonstrates progress by showing what they’ve been doing. It’s an opportunity to ensure user needs are being met. The team may ask for help in unblocking something, or need a decision on something that needs your steer. Test benefits In beta you’ll be delivering a working, public facing version of the service. This phase is an opportunity to test expected benefits with users. Review costs for beta and live Consider the expected lifetime of the service and costs for: people software technical infrastructure  team environment (facilities) and support continuous improvement assisted digital As you progress in beta you should get a much clearer picture of live service costs. Revisit procurement needs As your understanding of what is needed for live service improves, you may discover, for example, that you need more people, different skills or changing cloud service requirements. Adjust your procurement accordingly. Get department investment approval and Cabinet Office spend approval for beta and live Seek further approval for funding your digital service as necessary. Digital by Default Service Standard assessment The Digital by Default Service Standard (DbDSS) is a set of criteria for digital teams building government services to meet. Meeting the standard will mean digital services are of a consistently high quality. This includes creating services that are easily improved, safe, secure and fulfill user needs. The result of an assessment could mean you continue to build towards a beta service, make some changes and come back for re-assessment or stop what you are doing. Assurance and audit As you move through beta towards live, the typical assurance processes to get your business ready to run the live service are applicable. There is a strong link to the DbDSS assessment at the beta to live point and it’s worth considering any additional assurance that may be beneficial beyond the scope of that assessment.  This is likely to focus more on the business impact of introducing the new service, or new digital service channel, and any associated transition arrangements. Any audit of beta should focus on the team and governance process and behaviour, not the actual decisions taken. The team and its governance bodies can only be expected to make decisions based on what’s known at the time. A decision may be found to be wrong as better information becomes available, eg through testing with users.  If your service development team is working well, it will quickly learn and modify its approach. Unblock and support the team Teams need help with things that slow them down and are outside of their control. During beta they’ll need you to be closely involved on a daily basis, so that you can help them with decisions, encouragement and ideas. You’ll need to anticipate problems and make decisions promptly to avoid slowing down delivery. Revisit the size and shape of the team You’ll probably need a larger team with a broader range of skills to help as you start building the beta service. You should consider if you need business analysis, product management, operations or programme delivery management skills. Find out more about beta team requirements. Scale as necessary Scaling to more than one team brings overheads. It’s important to start small and stay small unless you really need to get bigger. And if you do, plan for these overheads and you’ll get better results. Review the team environment As your team changes it’s important to revisit their needs. New skills may mean new tools, and new people will certainly mean more space and equipment. Remember to keep the service teams seated together so that they can work more closely. Find out how to get the right working environment. Previous: governing the alpha phase Next: governing the live phase Get involved To give feedback, make a suggestion or share your experience, use the governance guidance hackpad.","description":"When you’re governing the beta phase of a service, you’ll need to consider:","link":"/service-manual/governance/governing-the-beta-phase.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Governing the discovery phase","indexable_content":"Delivery and assurance Funding and approvals Team      describing the initial vision for the service         assigning a service manager         assembling a discovery team         agreeing the outcomes you want from discovery         delivery and assurance         funding and approvals         the service delivery team         develop a service vision laying out the overall intention of the service, including who the users are and why the service is important          agree a handful of discovery goals and how to measure progress         find out who the stakeholders are         set up initial governance and communicate what people can expect from it         the team can demonstrate progress towards meeting discovery goals         you can see how the well they are working together         you can support and help unblock things that are slowing them down         people         software         infrastructure         team environment and facilities         design and development tools         a user research lab (possibly)          a large stock of whiteboards, sticky notes and pens         what the service is and who it is for         key benefits it provides         how success will be measured          how it fits in with organisational strategy         stop         extend discovery (if you haven’t found out enough yet)         restart discovery (if you need to investigate something completely different)         proceed to alpha    Discovery is more successful when you begin by: describing the initial vision for the service assigning a service manager assembling a discovery team agreeing the outcomes you want from discovery You need to get investment approval within your department and spend approval from Cabinet Office if you’re going to spend money on a supplier.  If your service is in response to a new policy, make sure you consider the digital service during the policy impact assessment.  You need to set up proportionate governance for discovery – you’ll need to consider: delivery and assurance funding and approvals the service delivery team Governance during the discovery phase will include a small team of senior people acting as sponsors and advisors. They’ll be closely involved with the delivery team exploring the service needs during discovery. Plan for your discovery Before discovery, you need to plan and gather sufficient information to make sure what you’re doing fits with the wider organisational strategy.  You need to: develop a service vision laying out the overall intention of the service, including who the users are and why the service is important  agree a handful of discovery goals and how to measure progress find out who the stakeholders are set up initial governance and communicate what people can expect from it Goals and measures are likely to be incomplete and there may be little evidence on which to base targets. Your service vision will probably evolve too. Go to show and tells ‘Seeing the thing’ at show and tells is an important governance activity – this is useful because: the team can demonstrate progress towards meeting discovery goals you can see how the well they are working together you can support and help unblock things that are slowing them down Even early in the life of a service, there’s a lot that the team can show to demonstrate the outcomes of their work. Identify benefits In discovery you’ll spend some time exploring and understanding what benefits the service could bring to users and government. You’ll do this through lots of user research and a close analysis of policies, laws and business needs.  If you don’t identify benefits from the start, the chances of them being realised are reduced. Different delivery options may offer different benefits – so identify the options in discovery and explore further during alpha. Identify costs for discovery and alpha Consider the costs for: people software infrastructure team environment and facilities You’ll probably need to bring in additional skills at different points. You’ll also need: design and development tools a user research lab (possibly)  a large stock of whiteboards, sticky notes and pens Rate cards and price lists available in procurement frameworks can help you estimate costs. Evolve the service vision The service vision should state: what the service is and who it is for key benefits it provides how success will be measured  how it fits in with organisational strategy It may also contain known risks and constraints. Get approval Get department investment approval and Cabinet Office spend approval for discovery and alpha. There’s more information about this in the guidance on funding your digital service. Consider procurement needs Involve procurement and HR during discovery to ensure there is no lag between discovery and alpha. They’ll help you secure the resources and people needed. Make a decision to proceed or not Discovery has 4 possible outcomes – you can decide to: stop extend discovery (if you haven’t found out enough yet) restart discovery (if you need to investigate something completely different) proceed to alpha You should use the outputs from discovery to help you decide. You can make the decision to stop at any point during discovery. Set up your discovery team You’ll need a small team consisting of your stakeholders and any core team members that have been identified, including the service manager and potentially policy, procurement, HR and legal as required. Equip the discovery team properly Make sure people have a good working environment and the tools they need.  Next: governing the alpha phase Get involved To give feedback, make a suggestion or share your experience, use the governance guidance hackpad.","description":"Discovery is more successful when you begin by:","link":"/service-manual/governance/governing-the-discovery-phase.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Governing the live phase","indexable_content":"Delivery and assurance Team      delivery and assurance         funding and approvals         the service delivery team         reviewing actual cost savings (eg less support calls, reduced administrative time spent on processing transactions) and deciding when to realise these by scaling back operations         checking your assumptions about assisted digital and finding out whether its users are the expected demographic         decommissioning obsolete systems    When you’re governing the live phase of a service, you’ll need to consider: delivery and assurance funding and approvals the service delivery team Revisit your vision and refine your roadmap The service vision and roadmap will be stable and certain as you go into live service as you’ll have used the previous phases to thoroughly understand user needs.  Any changes to the service should be based on emerging user needs and operating context (eg legislative or technological changes, espionage or criminal threats to the service). A service being continually improved is a condition of a service being on GOV.UK. Review your governance arrangements You should review what governance arrangements you’ll need for the live service, bearing in mind this covers both running the live service and improving the service. Plan for known risks By the time you’re running a live service, most of your delivery risks will have been dealt with in earlier phases. However, it’s important to understand what risks can affect the success of your service and the benefits it’s expected to deliver. Continue to work in an agile way You should continue to run show and tells to provide opportunities to demonstrate new and changed features. The team may ask for help in unblocking something or a decision on something that needs your steer. Retrospectives still provide valuable opportunities for reflection, learning and improvement in live operation. They might focus on the service operation in the broadest sense or specific activities like introducing new or improved features. If you have more than one service, you could look at ways of having larger retrospectives where the services share how they are working, what’s working well for them and what isn’t. Review benefits You should now be getting the benefits you expected. You’ll have a service dashboard on the performance platform which will show actual take-up of the digital service. You can compare its figures against those you predicted in the business case and find out if you are on track or not. You should also be: reviewing actual cost savings (eg less support calls, reduced administrative time spent on processing transactions) and deciding when to realise these by scaling back operations checking your assumptions about assisted digital and finding out whether its users are the expected demographic decommissioning obsolete systems Funding and approvals The business case for the service should cover development plus the first 2 years of live operation, including continuous iterative improvement.  Beyond this, you need to consider the budget demands for the live service as part of the department’s usual budgetary process. The funding should cover ongoing improvement and any minor changes needed to remain compliant with legislation. If the service needs a major overhaul, you should treat this as a new service development and start again with discovery. Revisit procurement needs Identify any external resource needs for live operation and ensure that investment and spend control approvals are secured to enable procurement activity to progress. The Government Digital Strategy aims to build in-house capability, so the need to procure external capability should reduce over time. Assurance and audit Part of assurance is reviewing whether or not expected benefits are being realised. You can use your department’s internal audit team to run reviews and the Major Projects Authority (MPA) will assure major projects. The National Audit Office audits most public-sector bodies in the UK and produces value for money reports into the implementation of government policies, so you may find that it does this for your service. Revisit the size and shape of the team During live service operation your team may need to change size and shape in response to user needs and operating context. Teams will expand and contract over the service lifetime. Find out more about live team requirements Ensure the team has skills necessary to run the live service It’s important the live service continues to improve. If you need to introduce new team members, you should manage this sensitively and gradually, in a way that allows them to gain confidence that they have the skills to continue development of the service. Review the team environment As your team changes it’s important to revisit their needs. Support the team You will need to remain involved so that you can help the team with decisions, encouragement, ideas and feedback. Previous: governing the beta phase Get involved To give feedback, make a suggestion or share your experience, use the governance guidance hackpad.","description":"When you’re governing the live phase of a service, you’ll need to consider:","link":"/service-manual/governance/governing-the-live-phase.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Government as a platform","indexable_content":"Government as a platform Tim O’Reilly’s “Government as a Platform” Mike Bracken’s blog on Why GOV.UK matters: A platform for a digital Government Further reading:","description":"Further reading:","link":"/service-manual/technology/government-as-a-platform.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Guerrilla testing","indexable_content":"How does guerrilla testing work Where and how you might use it Weaknesses and when not to use Number and types of participants GDS Example always ask permission first to speak with people outline briefly the purpose of the research reassure them about confidentiality keep it simple and quick consider the location and set up carefully eg a busy train station may have the footfall but people might be in too much of a hurry to spare the time providing incentives for audience participation is not required or necessary (however, depending on where you are running your sessions chocolates are often a welcome ‘thank you’ for peoples’ time) Guerrilla user testing is a low cost method of user testing. The term ‘guerrilla’ refers to its ‘out in the wild’ style, in the fact that it can be conducted anywhere eg cafe, library, train station etc, essentially anywhere where there is significant footfall. Guerrilla testing works well to quickly validate how effective a design is on its intended audience, whether certain functionality works in the way it is supposed to, or even establishing whether a brand or proposition is clear. This approach is quick and relatively easy to set up. Participants are not recruited but are ‘approached’ by those persons conducting the sessions. The sessions themselves are short, typically between 15-30 minutes and are loosely structured around specific key research objectives. The output is typically ‘qualitative’ so insight is often rich and detailed. Anyone on the service team can conduct ‘guerrilla testing’ on their site or service but often the best scenario is for a researcher to run the sessions with the designer or developer. The researcher can help with defining the tasks, moderating the sessions as well as provide a level of ‘objectivity’ by not being the person who designed or built what is being evaluated. Involving the designer / developer in the sessions enables them to see first hand ‘real’ people interacting with their product, where there are areas for improvement and how they might go about resolving any issues. This approach also does away with any lengthy reporting back. Insights can be observed, taken away and fed back into the design process almost immediately. However, a brief summary with key findings and recommendations can be written up as a more formal record. It is a method that suits the ‘agile framework’ well. Guerrilla testing can be used throughout the service lifecycle. As it is cheap to set up, run and report back on, it is a method that can be used frequently. There are a few logistics that should be taken into consideration before conducting any guerrilla testing; Remember, whenever recording sessions you must seek permission from the participant first. Provide them with a written consent form for them to sign. The key weakness of guerrilla testing as a research method it that is not statistically robust and participants may not always match your ‘target’ audience in terms of skills, expertise, knowledge. This can vary from between 6-12 participants in any given round of guerrilla testing. It is very much dependent on where and when those sessions are conducted. 16 teenagers evaluated the National Citizen Service website. Sessions were conducted in the canteen of a further education college in Nottingham. Usability and editorial findings were discovered and quickly fed back to the development teams. The whole process took 2 days.","description":"Guerrilla user testing is a low cost method of user testing. The term ‘guerrilla’ refers to its ‘out in the wild’ style, in the fact that it can be conducted anywhere eg cafe, library, train station etc, essentially anywhere where there is significant footfall.","link":"/service-manual/user-centred-design/user-research/guerrilla-testing.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: GOV.UK header and footer","indexable_content":"When to use the GOV.UK header and footer Header and footer code Choosing the right header Choosing the right footer always link the GOV.UK logotype back to the GOV.UK home page set the coloured bar width to be the same as the content area. The default is 960px at full width the coloured bar is always $govuk-blue (#005ea5) This guide explains when and how to use the GOV.UK header, footer, logo and typeface. If your service is available from www.gov.uk, service.gov.uk or blog.gov.uk then it must use the GOV.UK header and footer. This includes the crown device, the GOV.UK logotype and the New Transport typeface. If it’s not available from one of these domains (including other gov.uk domains like data.gov.uk) then it must not use the GOV.UK header and footer. If your service isn’t yet in beta then you don’t need to use the GOV.UK header and footer. The important thing is to make something and make it quickly, before testing it with real users. All the code and assets you need to implement the header and footer are available via the GOV.UK template app on GitHub. There are different versions of the GOV.UK header for different types of page. For simple services on GOV.UK use this: Remember: If your service is more than a few pages long, you can help users understand where they are by adding the service title, like this: If you need to include contact or account management links, do this: Here’s how to code the service title and navigation Services on GOV.UK should use the standard GOV.UK home page footer, but with the browse links removed. You may also add a link back to the department responsible and to other language options. Here’s an example from the ‘Renew a tax disc’ service:","description":"This guide explains when and how to use the GOV.UK header, footer, logo and typeface.","link":"/service-manual/user-centred-design/resources/header-footer.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Help text","indexable_content":"1. Writing help text 2. Inline help text 3. Hidden help text 4. Detailed help On this page: Do this: Don’t do this: Do explain: Don’t explain: Writing help text Inline help text Hidden help text Detailed help legal jargon unfamiliar concepts where to find obscure information what format the information should be given in what you do with personal information can access the help before and during the transaction can get back to the transaction without losing their place are shown help that’s relevant to their current situation The best way to help users is by making the simplest, clearest service you can. Sometimes though you’ll need to provide extra help. Keep it short and concise and speak to the user in their language.  Users are unlikely to read anything longer than 3 lines.  Focus on the action required and make it relevant to the user’s current situation. Don’t give background information like ‘This used to be called X but in 2008 it was changed to Y’. The interface. If you have to do that you haven’t made it simple enough. If you find yourself writing things like ‘Click on the green button at the bottom of the screen’ you know the interface needs work. Short, clear text positioned immediately next to the part of the page it relates to. Use this for help that’s relevant to the majority of users. Inline help for form fields should be positioned between the label and the field so it’s read by sighted and screen reader users before they get to the field itself. A short link that expands into more detailed help when clicked on. Use this to make your page easier to scan, but don’t hide text if a majority of users will need it. This pattern is still under discussion If your users have to make difficult or complex decisions that can’t be supported by the above approaches you might need to provide them with more detailed help. Make sure that users: Don’t use detailed help to explain how to use the interface - it should be simple enough to use without this. Discuss this page on Hackpad","description":"The best way to help users is by making the simplest, clearest service you can.\nSometimes though you’ll need to provide extra help.","link":"/service-manual/user-centred-design/resources/patterns/help-text.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Helpdesk","indexable_content":"Volume forecast: how many and what type of enquiry Handling time: time it takes to deal with an enquiry Service level: how fast you respond to user enquiries Staffing and scheduling Data collection and reporting Going even further handle user requests for information help direct users to the information they want email phone online chat Twitter Facebook surface mail volume handling time service level improve and refine the forecasts you made previously create long-term projections about the performance of your helpdesk the group itself individual members of staff You need to form a helpdesk as part of providing a high quality service. A helpdesk is a dedicated group of specialists – or current staff dedicating some of their time – that: Your planning will benefit from the use of traditional contact centre management and planning skill sets. If you don’t have access to staff with these skills, you may have to work closely with other groups who do in a consultancy capacity. This may include the team at GDS, or outside specialists. You need to have an idea of the volume and type of enquiries you’ll receive. In many cases, you can use historical data for similar services as a baseline. Also take into account the contact methods you intend to support, such as: To help with your planning, make sure each of these channels has a separate volume forecast. These forecasts will either inform, or help to make, decisions on what technology you’ll use to route, handle, and store the historical data of these enquiries. Work out the average handling time (AHT) and variance for each type of enquiry. If you’re building AHT with deep historical data, you may be able to model the AHT separately for each kind of enquiry. Use the best historical data available to calculate the average number of enquiries per day a single agent can handle when dealing with a representative mix of enquiries types. Work out the expected level of service that should be maintained by your helpdesk. Generally, service level (SL or SLA) is worked out as x per cent of enquiries resolved in y time units. For example, you might plan to answer 80% of your incoming email enquiries in 24 hours. You need to know the desired service level so you can work out how many agents you’ll need for your helpdesk. If your staffing is constrained by other factors, use this value as a constant and aim for the best service level that can be achieved at a certain volume level. You can only finalise your overall demand forecast and work out staff numbers when you have finished planning for: The same historical data you used to create your enquiry forecast will likely give you an idea of how the enquiries will be received throughout the planning period (likely a week). This will then help guide you on how to schedule your helpdesk staff, so they can achieve the service level you’ve planned for. Once your helpdesk is operational and actually supporting users, collect data on all the areas you planned to collect information on. This data can then be used to: Plan on how to measure the performance of your helpdesk from the very beginning. Develop at least minimal reporting so you can evaluate the performance of: These are good starting points to consider as and when you plan to develop new digital products and initiatives. There’s quite an extensive amount of online information that expands on the points made here, including some free planning tools. However, as with all areas of specialised knowledge, use the services of an experienced analyst rather than depending entirely on online sources for guidance.","description":"You need to form a helpdesk as part of providing a high quality service. A helpdesk is a dedicated group of specialists – or current staff dedicating some of their time – that:","link":"/service-manual/operations/helpdesk.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Hosting","indexable_content":"Decide how you’ll host your application Types of suppliers Make your decisions carefully Further information Ownership Co-location Shared or managed hosting Infrastructure as a Service Platform as a Service assess different options shortlist suppliers interview suppliers make a decision a hands-on technical understanding of the service and underlying software – these people are essential knowledge of the available procurement options knowledge of acceptable costs ownership co-location shared or managed hosting infrastructure as a service platform as a service infrastructure as a service platform as a service the technical requirements of your software applications your future capacity requirements user support reliance and redundancy mandated government information security requirements government network connectivity the cost of service flexibility and on-demand billing different supplier operating models Why IaaS? – a blog post about why the Government Digital Service chose an infrastructure as a service for GOV.UK G-Cloud – the procurement framework intended for purchasing infrastructure as a service, platform as a service and software as a service products You need servers to run your software. The information here will help you decide how you host your applications and the things to consider when selecting a vendor. To begin with, involve a small cross-functional group of people who will quickly: Make sure the people in your group have: It’s important that you keep good notes from any interviews or deliberation sessions and assess different suppliers equally. One way to do this is to use a scoring matrix. Know that it’s very common to use multiple suppliers. This can be because they offer different but compatible services, or additional redundancy. It can be technically challenging to do this, but can provide extra resilience for larger projects. Suppliers offer a number of different approaches to hosting services, which can make it difficult to compare them. The following information is intended as only a brief introduction of each type, covering: This advice is complicated because many service providers redefine the meanings of hosting terms for marketing reasons, especially: These terms are marketable at the moment and often used incorrectly, so always look into the details of the services being offered. If you have a large project with very specific requirements, you may decide that purchasing hardware and even running a dedicated data centre is suitable. The costs and timescales involved here are very high and in most cases it’s unlikely to be your best option. Many providers offer co-location services – you purchase your own hardware and rent space in a managed data centre.  This provides a great deal of flexibility, but can introduce lead times and other physical constraints. It also requires a wide range of technical specialist skills. Lots of service providers have a shared or managed hosting option. This tends to mean renting specific virtual or physical machines for fixed periods of time. Different suppliers offer different management services; some just manage the underlying machine while others will support the operating system and even specific applications running on the machines. In the last several years this has become a common approach to managing hosting and infrastructure requirements. It tends to involve a capability to rapidly add or remove capacity, often in minutes, and to be billed only for what is used. This provides a great deal of flexibility and the ability to keep costs down, but also requires a degree of technical skill to manage well. Similar to infrastructure as a service, platform as a service offerings tend to allow for quickly adding or removing capacity and precise ‘pay on demand’ pricing. The difference is that you are considered completely separate from the underlying infrastructure. The unit here is the running application, not a virtual or physical machine. Using a platform as a service places a number of restrictions on the software architecture, but can move the support burden for parts of the stack onto the supplier. Before making a decision about your hosting supplier, weigh up a wide range of different factors, including: Unfortunately this is a technical field with many options. Seemingly similar services can have wildly different structures or different cost models that can result in large differences in your total cost of ownership (cost of the project).  It’s important to involve technical colleagues or trusted third parties in any discussions and/or decisions.","description":"You need servers to run your software. The information here will help you decide how you host your applications and the things to consider when selecting a vendor.","link":"/service-manual/operations/hosting.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: How service.gov.uk domain names are managed","indexable_content":"How service.gov.uk domain names are managed How is this different to how direct.gov.uk domains have been managed? Why do we do this? There are service providers on GCloud who will provide DNS Hosting with a web-interface for you to manage your DNS records. Many infrastructure hosting providers also provide DNS services for the use of their customers. As a last resort (or for larger services) you may wish to ask the supplier to provide DNS services by any means they consider reasonable (including utilising options 1 and 2 above). Once a domain name for a service has been agreed, the GDS Infrastructure Team allocates service.gov.uk domain names by delegating control to the team running the service. The team should provide the details of nameservers and GDS will delegate {service-name}.service.gov.uk to those nameservers. At that point the service team will be able to manage the detailed configuration of which servers the domain name points to, and so on. The domain direct.gov.uk was managed by the DirectGov Service Desk directly (and inherited by GDS). All domains were administered by DirectGov and each subdomain did not have its own DNS servers. This meant that services would need to give details of the names and IP addresses of their webservers to DirectGov, who would directly create the necessary A and CNAME records. The DirectGov Service Desk were also a critical part of any DNS change. The new model for service.gov.uk domains requires that you provide your own DNS servers which can respond to requests for {something}.{servicename}.service.gov.uk. It is not necessary that you run and manage your own separate DNS servers: It is likely that as you build and iterate your service you will need to make a number of changes to its configuration. You might introduce load balancers or content delivery networks, you may move hosting providers, or your provider might need to change the IP addresses that your servers are assigned. In any of those cases it is important that your team is able to quickly respond to the situation and make any relevant changes with as few intermediaries as possible. By delegating control GDS ensures that control is in the hands of the service team and not blocked by a central authority.","description":"Once a domain name for a service has been agreed, the\nGDS Infrastructure Team allocates service.gov.uk domain names by delegating control to the team running\nthe service. The team should provide the details of nameservers and GDS will delegate\n{service-name}.service.gov.uk to those nameservers. At that point the service team will be able to\nmanage the detailed configuration of which servers the domain name points to, and so on.","link":"/service-manual/domain-names/how-they-work.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: How users read","indexable_content":"Reading Reading age Lower case Plain English Context Learning disabilities Why we do this Further reading The content design guidance and style guide are set in best practice and relate to how users read. This is an explanation of some of our guidance and the reasons behind the rules. Users only really read 20 to 28% of a web page. Where users just want to complete a service as quickly as possible, there’s added user impatience so you may find users skim words even more. The content design guidance, style guide and design patterns give guidance on how to write. This page details why we do it. All of this guidance is based on the learning skills of an average person in the UK, who has English as a first language. You don’t read one word at a time. You bounce around. You anticipate words and fill them in. By the time you’re 9 years old, your brain can drop up to 30% of the text and still understand. Your vocabulary will grow but this reading skill stays with you as an adult. You should also be confident in sounding out words and blending sounds. You may not know the word, but you have the skills to be able to learn it. This is why we talk about the reading age being around 9 years old. When you learn to read, you start with a mix of upper and lower case but you don’t start understanding upper case until you’re around 6 years old. At first, you may sound out letters, merge sounds, merge letters, learn the word. Then you stop reading it. At that point, you recognise the shape of the word. This speeds up comprehension and speed of reading. So we don’t want people to read. We want people to recognise the ‘shape’ of the word and understand. It’s a lot faster. Capital letters are reputed to be 13 to 18% harder for users to read. So we try to avoid them. Also, in modern usage it sounds like we’re shouting. We are government. We should not be shouting. By the time you are 9, you’re building up your ‘common words’. Your primary set is around 5,000 words in your vocabulary; your secondary set is around 10,000 words. These are words you use every day. They include a lot of plain English words, which is why we should be obsessed with them. These are words so easy to comprehend, you learn to read them quickly and then you stop reading and start recognising. We explain all unusual terms on GOV.UK. This is because you can understand 6-letter words as easily as 2-letter words – if they’re in context. Sometimes, you can read a short word faster than a single letter – if the context is correct. Not only are we giving users full information, we’re speeding up their reading time. By giving full context and using common words, we’re allowing them to understand in the fastest possible way. In tools and transactions you need to give people context. By giving them information they’re expecting, you help them get through it faster. We should remember that people with some learning disabilities read letter for letter. They don’t bounce around like other users. They also can’t fully understand a sentence if it’s too long. People with moderate learning disabilities can understand sentences of 5 to 8 words without difficulty. By concentrating on common words we can help all users understand sentences of around 25 words. Our audience is potentially anyone living in the UK. We need to be able to communicate in a way that most users understand. Government can’t afford to be elitist and use language only specialists can understand. We need to open up our information and services to everyone. That means using common words and working with natural reading behaviour. Nielsen: For more detail on why 20 to 28% of text is read.","description":"The content design guidance and style guide are set in best practice and relate to how users read. This is an explanation of some of our guidance and the reasons behind the rules.","link":"/service-manual/user-centred-design/how-users-read.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Recruitment Hub","indexable_content":"Senior Civil Servant (SCS) recruitment Non-SCS specialist recruitment feedback, advice and approval on tailored organisational design and SCS job descriptions as needed, to ensure that the lines of reporting and share of responsibilities between other digital and technology (IT) leaders within organisations are clearly set out. guidance in coordinating headhunting support for departments looking to identify suitable candidates for senior appointments GDS expertise to work with departments in sifting, shortlisting and interviewing of applicants feedback and advice on the design and organisation of digital delivery teams within departments guidance on routes to market (frameworks etc) guidance and advice on attraction and recruitment campaign strategies  GDS expertise to work with departments in sifting, shortlisting and interviewing of some key Band A / Grade 6 / Grade 7 specialist posts GDS is supporting departments on digital and technology recruitment through a Recruitment Hub. This service is designed to help ensure the best possible candidates are in post to deliver excellent digital public services supported by the right technology. The hub will provide recruitment guidance, support and advice to departments for senior and specialist roles. The Recruitment Hub is not intended to replace existing HR and recruitment processes, and exists purely to help organisations acquire these new and hard-to-reach skills. GDS must approve the job descriptions and appointments for all SCS-level technology positions. All senior grades (Deputy Director / Grade 5 level and above) are part of the Senior Civil Service, which is overseen by the Cabinet Office on behalf of the civil service as a whole. Senior civil servants may be called to account by Parliament, and are barred from holding any political office.  The template job descriptions, and organisation design advice should be a useful starting point for departments and agencies to tailor to their specific roles. For SCS level hires, the Recruitment Hub could also provide: Campaigns need to be planned effectively with sift and interview dates agreed before advertising so that GDS can provide appropriate resources with enough notice.  Departments should continue to liaise with advertisers and Civil Service Commissioners directly. GDS does not need to approve the appointment of non-SCS specialist posts, but will again provide guidance, support and advice for departments seeking to bring in specialist digital skills. The template job descriptions and organisation design advice should again be a useful starting point for departments and agencies, and is likely to require less tailoring than SCS roles. In addition, the Recruitment Hub could also provide: In specific cases where a position requires a particular specialist set of technical skills (eg technical architects, designers), departments should request sifting or shortlisting support from GDS via the Recruitment Hub. This service should be requested before the post is advertised.  We cannot promise to provide this in all cases, but will try to make expert advice available from the shortlisting stage for crucial posts. All queries for the hub should be made to digitaltalent@digital.cabinet-office.gov.uk.","description":"GDS is supporting departments on digital and technology recruitment through a Recruitment Hub. This service is designed to help ensure the best possible candidates are in post to deliver excellent digital public services supported by the right technology. The hub will provide recruitment guidance, support and advice to departments for senior and specialist roles.","link":"/service-manual/the-team/recruitment/hub.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Ideal alphas","indexable_content":"Preparing Executing Identifying goals Identifying risks Identifying the team Identifying process Inception Iterations Ending the alpha explore the major risks to the project discover whether the project is workable get some ideas about the cost of the project identifying risks prototyping solutions attempting to prove that you can replace an existing digital service prove that you can replace a paper service with a digital service create a new digital service design business process technical risks how easy is it to use will the user get through it correctly the first time what do errors look like how will I analyse the user research how do I build prototypes quickly how the department will integrate with the new process do they have sufficient fraud checks are they set up to handle support calls how to operate the service what sort of ongoing connections will be necessary are there special hosting arrangements, eg high security hosting design (service, UX, content as needed) user research (user testing) prototype development service integration development delivery management business analysis build a wireframe or prototype user journey in 1 week host it on the internet conduct some user research analyse the findings an inception a series of iterations of design, development and test either:            alpha termination       alpha to Beta transition         alpha termination alpha to Beta transition a shared understanding of the service User Personas (who will use the system) a clarified current business process (where applicable) a clear set of goals and tradeoffs some vision exercises a clear understanding of the existing technical estate a prioritised backlog of work for first iteration the entire team get to know one another beginning to form a team identity beginning to form a team understanding of the business itself Day 1            Retrospective (1 hour)       Iteration Planning (2 hours)       Start iteration         Retrospective (1 hour) Iteration Planning (2 hours) Start iteration Day 2            User research on previous iterations prototypes       Analysis of test results         User research on previous iterations prototypes Analysis of test results Day 5            Demo         Demo user research in iteration 1 stories completed in iteration 2 further research on updated prototypes in iteration 3 the first and last day of the iteration are the most critical for the entire team Mondays/Fridays have the most absence Next phase: beta Previous phase: discovery An alpha is a process that happens at the beginning of a project. It allows us to: It is primarily defined by two activities: This document covers how to go about executing an alpha project. Following this guidance will help you create a successful alpha project. You should bear in mind that these guidelines are flexible and are intended to direct you towards achieving your project goals simply and quickly. Before planning an alpha, the team must work out what the goals of the alpha are. For example, some of your goals might be: The department should communicate their goals in the request for proposals. The team will probably develop their own additional goals as well. The reason for running an alpha is to identify risks to the wider project early. This means it should identify a project’s biggest risks and explore those risks to understand them better. In most government projects, the risks fall into 3 main categories: The design risks cover the issues that come with running a user-centred design process within an iterative model. We find that it often takes several attempts to get the service proposition correct for the citizen. You’ll need to think about things like: The business process risks include: The technical risks tend to be about integrating into the existing systems. For example: In our experience, there are always technical risks to all projects. Most government departments do have a history of solving technical risks and have entire teams dedicated to them. Business process and design risks are far less well understood, and so we encourage alphas to focus on exploring those risks with prototyping over full service integrations. The well-rounded team will need to have various skills, including: The team will probably be bigger than this, and the service may supply its own team members who will normally be learning during this process, and thus probably won’t be contributing significantly. The team will work together in an agile manner, doing iterations of either 1 or 2 weeks. For a short project we’d normally recommend 1 week iterations where possible. The team must have the core skills to: The Business Analyst and Delivery Manager roles are there to ensure that the findings can be fed back into the team and the wider organisation. Each alpha will be different, because the risks and goals are different. But we find that there are common aspects in how good alphas are run. We want to maximise the amount of learning that is happening in the alpha. So we tend to expect alphas to be short, about 8 weeks in length. We expect alphas to normally consist of: We believe that alphas should start with an inception process. An inception is designed to bring the team together and share the knowledge about the risks and goals identified. We tend to recommend a short inception – between 4 to 7 days seems to work well. The inception should be run by the supplier team, probably by the Delivery Manager. It will look at a variety of business, technical and user aspects of the project. Each supplier has a different format for these, but we see fairly common outputs including: The facilitator will of course give up time to various specialists to run individual sessions. The aims include: The inception may well cover introductions to agile itself, if the project team need it, and should also have a demonstration and retrospective at the end of it. Alphas are often about identifying competing hypotheses about the user journey or business process. We expect to produce many prototypes to test the hypothesis, and run tests in a short amount of time so we can learn, change and retest. These feedback loops are crucial. At this early a stage of a project, it is rare to be able to choose a single flow or design. Because of this, we encourage the building and testing of many prototypes. This allows you to quickly get feedback on the best experience for the users and to test competing ideas. We expect that a first draft of a user flow will have issues. You should seek to revisit the design, research and test process on each flow during many iterations. The iterative process we would recommend should be a straight up agile process combined with UX and user research. We would encourage the following format: This structure might move around a bit, but you will want to do the user research early in the iteration to give plenty of time to feed into the next iteration. This enables fast turnaround on the stories. Working at this pace will result in: This assumes fairly high definition prototypes. These may have backend systems that mock out service integrations, but which contain actual working logic. If you need to iterate faster (as is likely at first), you can start with much lower fidelity prototypes (like paper). This can let you test various flows and allow much faster iteration of research and prototype development. We would expect that you will do a demonstration of the service journey so far. This happens at the end of each iteration, and highlights what the team has learnt and what the team is planning on doing next. We’ve often found that aligning the iterations to Monday to Friday is not a best practice. It can cause logistical issues: Starting the iteration on a Wednesday, with User testing on Thursday and demo the following Tuesday has worked well in the past. The value of the alpha process is that the team can use it to identify early the biggest risks to the Beta part of the program. It is important to remember that the most valuable goals from the alpha process are the increased domain understanding and the team forming behaviours. If the code produced during the alpha is not high-enough quality, then it’s fine to throw it away and rebuild. For example, the team may elect not to use test-driven development during this stage. Those activities represent investment in quality. Remember that a primary goal during the alpha is learning. We might not be ready to make the investment in creating production-ready systems. In particular, that investment might be premature if the team learns they are not solving the right problem. Another output of the alpha can be the early decision not to continue into Beta. This represents a successful alpha since it reduces wasted time and money. It may well be the case that at the end of the alpha you go back to the discovery outputs and start a new alpha with a different focus or go back and perform another discovery. However, assuming a successful alpha project, then early in the alpha process, the Delivery Manager and the Service Manager will start to work on the business proposal for the Beta program. The reason for starting so early on the Beta program proposal is that often the team would like to continue without a break into the beta program to maintain the momentum and so need to allow for the time to gain approval for further spending The last demo of the alpha should have the attendance of the project board or senior management team from the service. At that demo, you are able to show what the alpha has achieved and show the feasibility of the beta program. Assuming that the status reports to project board and the show and tell have got approval, then as the project draws to an end, the team will want to have a final retrospective, and produce a backlog of epic stories for the Beta program.","description":"An alpha is a process that happens at the beginning of a project. It allows us to:","link":"/service-manual/phases/ideal-alphas.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: index.html","indexable_content":" Government Service Design Manual Build services so good that people prefer to use them Think differently about digital delivery Making a service Discovery Alpha Beta Live Retirement Guides and resources for…      Discovery      A short phase, in which you start researching the needs of your service’s users, find out what you should be measuring, and explore technological or policy-related constraints.      Learn about the discovery phase              Alpha      A short phase in which you prototype solutions for your users' needs. You’ll be testing with a small group of users or stakeholders, and getting early feedback about the design of the service.      Learn about the alpha phase          Beta      You’re developing against the demands of a live environment, understanding how to build and scale while meeting user needs. You’ll also be releasing a version to test in public.      Learn about the beta phase          Live      The work doesn’t stop once your service is live. You’ll be iteratively improving your service, reacting to new needs and demands, and meeting targets set during its development.      Learn about the live phase          Retirement      Even the best services may eventually reach retirement. That should be treated with the same care as went into the building and maintaining of that service.      Learn about the retirement phase     Service managers Content designers Designers Developers User researchers Web operations Performance analysts Chief technology officers Technical architects      All new digital services from the government must meet the Digital by Default Service Standard.           Read the standard »    Discover what it means to be part of an agile, user-focused and multidisciplinary team, delivering digital services in government. Start building digital by default services Learn about the different phases of service design and get guidance for the phase you're in now. A short phase, in which you start researching the needs of your service’s users, find out what you should be measuring, and explore technological or policy-related constraints. Learn about the discovery phase A short phase in which you prototype solutions for your users' needs. You’ll be testing with a small group of users or stakeholders, and getting early feedback about the design of the service. Learn about the alpha phase You’re developing against the demands of a live environment, understanding how to build and scale while meeting user needs. You’ll also be releasing a version to test in public. Learn about the beta phase The work doesn’t stop once your service is live. You’ll be iteratively improving your service, reacting to new needs and demands, and meeting targets set during its development. Learn about the live phase Even the best services may eventually reach retirement. That should be treated with the same care as went into the building and maintaining of that service. Learn about the retirement phase Browse all guides…","description":"\n    All new digital services from the government must meet the Digital by Default Service Standard.\n    \n\n    Read the standard »\n  ","link":"/service-manual/index.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Service design phases","indexable_content":"Discovery Alpha Beta Live Retirement      Discovery      A short phase, in which you start researching the needs of your service’s users, find out what you should be measuring, and explore technological or policy-related constraints.      Learn about the discovery phase              Alpha      A short phase in which you prototype solutions for your users' needs. You’ll be testing with a small group of users or stakeholders, and getting early feedback about the design of the service.      Learn about the alpha phase          Beta      You’re developing against the demands of a live environment, understanding how to build and scale while meeting user needs. You’ll also be releasing a version to test in public.      Learn about the beta phase          Live      The work doesn’t stop once your service is live. You’ll be iteratively improving your service, reacting to new needs and demands, and meeting targets set during its development.      Learn about the live phase          Retirement      Even the best services may eventually reach retirement. That should be treated with the same care as went into the building and maintaining of that service.      Learn about the retirement phase     Building a digital service is a complex task, with many risks. By breaking development into phases you minimise risk, learning about what works and what doesn't, iterating as you go. As the service progresses through development you'll find out more about users' needs, development requirements and the conditions your service will be operating in. The level of detail, complexity and risk will increase along the way. This approach allows the team making and operating the service to start small, learn fast, and provide value to users as soon as possible. A short phase, in which you start researching the needs of your service’s users, find out what you should be measuring, and explore technological or policy-related constraints. Learn about the discovery phase A short phase in which you prototype solutions for your users' needs. You’ll be testing with a small group of users or stakeholders, and getting early feedback about the design of the service. Learn about the alpha phase You’re developing against the demands of a live environment, understanding how to build and scale while meeting user needs. You’ll also be releasing a version to test in public. Learn about the beta phase The work doesn’t stop once your service is live. You’ll be iteratively improving your service, reacting to new needs and demands, and meeting targets set during its development. Learn about the live phase Even the best services may eventually reach retirement. That should be treated with the same care as went into the building and maintaining of that service. Learn about the retirement phase","description":"Building a digital service is a complex task, with many risks. By breaking development into phases you minimise risk, learning about what works and what doesn't, iterating as you go.","link":"/service-manual/phases/index.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Start building digital by default services","indexable_content":"Putting users first A new way of doing things Essential reading Start building digital by default services the Digital by Default Service Standard, a list of criteria that services and teams must meet before they go live the Government Service Design Manual, a pool of guidance and advice about how to design and build digital services from teams across government The service manual is here to help service managers and digital delivery teams across government make services so good that people prefer to use them. It’s made up of two things; Building good services means meeting the needs of users. 82% of the UK population is online. They have high expectations for what makes a good digital service, whether it’s from a bank, a travel agent, a retailer or a broadcaster, and when services don’t meet those standards they let the people responsible know. Government needs to learn from teams who are building successful, world class services. It needs to place users at the heart of service design, incorporating their feedback at every step of the way. Typically, government services are built after long, costly procurement processes.  In this way of working, users are seldom – if ever – consulted about the service they’ll be using. The first time the public might see a service is when it goes live, by which time it’s too late to make any changes when it turns out to be unfit for purpose. This way of working tends to encourage the creation of overly prescriptive policy, which then forms the basis of the requirements document. Instead, teams need to constantly iterate against user feedback.  This means building and testing in small chunks, working quickly to deliver improvements to a service. Teams will work out how to best meet the needs of users, releasing code regularly and working in an agile way. This new approach allows closer working between policy and delivery teams and as a result, the development of more responsive policy, two aims of the Civil Service Reform plan. The GDS Design Principles present a clear vision of how delivery teams can think about designing digital by default services. These should be a touchstone for every member of a delivery team. The Government Digital Strategy sets out the context we are working in. The departmental responses to the strategy contain a detailed breakdown of which services in particular will be tackled first. The progress of the services is also published on GOV.UK. And the Digital by Default Service Standard lists the criteria against which all new and redesigned services will be judged from April 2014. Find out which services must meet the standard. Another good way of finding out what’s going on in the government around digital is through the SPRINT initiative, organised by the Cabinet Office. At SPRINT13, Minister for the Cabinet Office Francis Maude discussed the current landscape of digital within government. Watch that video below. SPRINT 13 also saw a talk by Stephen Kelly in which he paints a picture of what service redesign could mean for users. When you’re confident about the basics of service design and the requirement of the standard, you can start exploring the advice and guidance in the manual. You can explore the manual by phase of delivery, specific roles, or by topic.","description":"The service manual is here to help service managers and digital delivery teams across government make services so good that people prefer to use them. It’s made up of two things;","link":"/service-manual/start/index.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Domain Names, SSL, Email","indexable_content":"Where services live on the web Internal services, intranets and extranets Getting a domain name and start/end page How service.gov.uk domains work Managing email HTTPS Asset Hosts Rules for operating service.gov.uk subdomains Suggested names for service.gov.uk subdomains    The Cabinet Office runs a service for those involved in the Controls processes. That is not a public facing service so does not have a start page on GOV.UK, but as a new government service it lives on the relevant service.gov.uk domain.  Every digital service offered by the UK government must have a single, well-known place on the web where users can locate and use the service. For public-facing services that will be the relevant start page on GOV.UK – for instance, the DVLA’s vehicle tax service is at https://www.gov.uk/vehicle-tax. Service Managers must not advertise any URL other than that of the GOV.UK start page as the starting point for the relevant service. This is what gets advertised, whether digitally or in other media. The start page URL for a given service will be allocated by GDS based on discussions with the Service Manager and analysis of user behaviour, search referrals and other relevant data. The service itself will be provided on a URL of the form www.{service-name}.service.gov.uk. Services designed for an internal audience should also live on a domain of the form {service-name}.service.gov.uk but do not need a start and end page on GOV.UK. The Cabinet Office runs a service for those involved in the Controls processes. That is not a public facing service so does not have a start page on GOV.UK, but as a new government service it lives on the relevant service.gov.uk domain.","description":"Every digital service offered by the UK government must have a single, well-known place on the web where users can locate and use the service. For public-facing services that will be the relevant start page on GOV.UK – for instance, the DVLA’s vehicle tax service is at https://www.gov.uk/vehicle-tax.","link":"/service-manual/domain-names/index.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Give feedback","indexable_content":"Get in touch Guiding questions the site tells you enough about how to meet the digital by default service standard? you can find useful material to answer your questions on this site? the guidance easy to navigate? you would like to see the guidance structured in any other way? there any other guidance or tools you would want to see on this web site? We would love to get feedback on your experiences of using this site. Please send us your thoughts: dbdss@digital.cabinet-office.gov.uk We’ll use your feedback to help improve this tool. Please offer feedback as appropriate. In particular, we would like to know if;","description":"We would love to get feedback on your experiences of using this site.","link":"/service-manual/feedback/index.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: GOV.UK elements","indexable_content":"This guide shows how to make your service look consistent with the rest of GOV.UK. It includes example code and guidance for layout, typography, colour, images, icons, forms, buttons and data. It’s currently in alpha and not hosted on GOV.UK. Go to the GOV.UK elements guide","description":"This guide shows how to make your service look consistent with the rest of GOV.UK.","link":"/service-manual/user-centred-design/resources/elements/index.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Design patterns","indexable_content":"Transaction flow Visual style Form questions Account management Other patterns Get involved Names Dates Addresses National Insurance numbers Email addresses Gender and sex Help text Usernames Create a user account Create a username Create a password Email confirmation loops Knowledge-based authentication Progress indicators Start pages Check your answers pages Confirmation pages Feedback pages Form structure Alpha and beta banners These patterns are based on the research and experience of the whole GOV.UK design community. If you can’t find the pattern you’re looking for here, we’re discussing many more on designpatterns.hackpad.com. For a visual style guide, including front-end code snippets, see the  GOV.UK elements guide. These patterns are always being tested and improved. If you find a better approach to something or want to suggest a new pattern then let us know on the design patterns wiki. ","description":"These patterns are based on the research and experience of the whole GOV.UK design community.\nIf you can’t find the pattern you’re looking for here, we’re discussing many more on designpatterns.hackpad.com.","link":"/service-manual/user-centred-design/resources/patterns/index.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Guidance for CTOs","indexable_content":"The technology code of practice Processes and controls Building on openness Changing culture Services and devices concentrates on user needs puts outcomes, not inputs, first uses openness to our advantage – open data, open standards, open source, open markets ensures that technology becomes so good that our colleagues, citizens and businesses want to use it building on open standards building on open data building on your own API defining services based on user needs using government as a platform creating a culture that supports change thinking of security as an enabler for service transformation understand the architecture of services and technology across team, departments and government provide end user devices which meet user needs have a good understanding of what effective service integration looks like understand the role of GOV.UK Verify when providing services to citizens Government needs services that are genuinely agile and responsive to changing needs – where change reduces, not raises, cost and risk – making government more productive, and our public services better. To do that, Chief Technology Officers (CTOs) need to help government departments make major structural improvements to how technology is used and procured, alongside an approach which: Liam Maxwell, Chief Technology Officer of the UK government, wrote a post on the GDS blog about the landscape CTOs are operating in. It’s essential reading for those transforming technology provision across government, describing the challenges and opportunities of this new approach. The technology code of practice describes the criteria that requests for spending approval will be judged against. It describes a vision of technology provision which can meet and adapt to the changing needs of users throughout government. The spending controls process ensures that spending on technology and digital services meets the needs of users and provides good value for money. There must be a level playing field for open source and proprietary software solution, that allows organisations the flexibility to change technologies and innovate based on data. This means: Providing technology that meets the needs and expectations of users will require major changes to the culture and mindset of teams. That means: Understanding the interoperability of services and devices is crucial to being able to improve part or the whole of the technology provision for a team or service. You will need to:","description":"Government needs services that are genuinely agile and responsive to changing needs – where change reduces, not raises, cost and risk – making government more productive, and our public services better. To do that, Chief Technology Officers (CTOs) need to help government departments make major structural improvements to how technology is used and procured, alongside an approach which:","link":"/service-manual/technology/index.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Digital by Default Service Standard (original 26 points)","indexable_content":"The criteria needs to be met by all new or redesigned transactional government services going live after April 2014 has to be maintained after a government service has gone live aims to make digital services so good that people prefer to carry out the transaction online rather than by phone, post or in person      Understand user needs. Research to develop a deep knowledge of who the service users are and what that means for digital and assisted digital service design.            Related guides                Know your users         User needs         Assisted digital                Know your users User needs Assisted digital      Put in place a sustainable multidisciplinary team that can design, build and operate the service, led by a suitably skilled and senior service manager with decision-making responsibility.            Related guides                The team         Service manager                The team Service manager      Evaluate what user data and information the service will be providing or storing, and address the security level, legal responsibilities, and risks associated with the service (consulting with experts where appropriate).            Related guides                Cookies         Information security                Cookies Information security      Evaluate the privacy risks to make sure that personal data collection requirements are appropriate.            Related guides                Information security         User accounts and logins                Information security User accounts and logins      Evaluate what tools and systems will be used to build, host, operate and measure the service, and how to procure them.            Related guides                Making software         Operating a service         Technology guides                Making software Operating a service Technology guides      Build the service using the agile, iterative and user-centred methods set out in the manual.          Related guides                Agile         Alpha phase         Working with prototypes                Agile Alpha phase Working with prototypes      Establish performance benchmarks, in consultation with GDS, using the 4 key performance indicators (KPIs) defined in the manual, against which the service will be measured.            Related guides                Measurement                Measurement      Analyse the prototype service’s success, and translate user feedback into features and tasks for the next phase of development.            Related guides                Continuous delivery                Continuous delivery      Create a service that is simple and intuitive enough that users succeed first time, unaided.            Related guides              Completion rate                Completion rate      Put appropriate assisted digital support in place that’s aimed towards those who genuinely need it.            Related guides                Assisted digital                Assisted digital      Plan (with GDS) for the phasing out of any existing alternative channels, where appropriate.            Related guides                Retirement phase                Retirement phase      Integrate the service with any non-digital sections required for legal reasons.            Related guides                Printed forms                Printed forms      Build a service consistent with the user experience of the rest of GOV.UK by using the design patterns and the style guide.            Related guides                Design and content         Service user experience                Design and content Service user experience      Make sure that you have the capacity and technical flexibility to update and improve the service on a very frequent basis.            Related guides                Releasing software                Releasing software      Make all new source code open and reusable, and publish it under appropriate licences (or give a convincing explanation as to why this can’t be done for specific subsets of the source code).            Related guides                Choosing technology         Open standards and licensing         Culture that supports change                Choosing technology Open standards and licensing Culture that supports change      Use open standards and common government platforms (eg GOV.UK Verify) where available.            Related guides                Open standards and licensing                Open standards and licensing      Be able to test the end-to-end service in an environment identical to that of the live version on all common browsers and devices. Use dummy accounts and a representative sample of users.            Related guides                Testing in agile         Operating a service                Testing in agile Operating a service      Use analytics tools that collect performance data.            Related guides                Analytics Tools         Performance Platform                Analytics Tools Performance Platform      Build a service that can be iterated on a frequent basis and make sure resources are in place to do so.            Related guides                Release strategies                Release strategies      Put a plan in place for ongoing user research and usability testing to continuously seek feedback from users.            Related guides                Know your users         User research         Multivariate testing                Know your users User research Multivariate testing      Establish a benchmark for user satisfaction across the digital and assisted digital service. Report performance data on the Performance Platform.            Related guides                User satisfaction         Performance Platform                User satisfaction Performance Platform      Establish a benchmark for completion rates across the digital and assisted digital service. Report performance data on the Performance Platform.            Related guides                Completion rate         Performance Platform                Completion rate Performance Platform      Make a plan (with supporting evidence) to achieve a low cost per transaction across the digital and assisted digital service. Report performance data on the Performance Platform.            Related guides                Cost per transaction         Performance Platform                Cost per transaction Performance Platform      Make a plan (with supporting evidence) to achieve a high digital take-up and assisted digital support for users who really need it. Report performance data on the Performance Platform.            Related guides                Digital take-up         Performance Platform                Digital take-up Performance Platform      Make a plan for the event of the service being taken temporarily offline.            Related guides                Release strategies                 Release strategies       Test the service from beginning to end with the minister responsible for it.            Related guides                No related guides yet                No related guides yet      The Digital by Default Service Standard is a set of criteria for digital teams     building government services to meet. Meeting the standard will mean digital     services are of a consistently high quality. This includes creating services     that are easily improved, safe, secure and fulfill user needs.    The standard: The service manual will help digital teams meet the standard and select people with the skills they need. If a Government Digital Service assessment panel doesn't pass a service, it won't be awarded the standard and won't appear on GOV.UK. The Digital by Default Service Standard was a commitment the government made in its Digital Strategy. Teams must meet the criteria below, and maintain this quality for the full life of their service. Related guides Related guides Related guides Related guides Related guides Related guides Related guides Related guides Related guides Related guides Related guides Related guides Related guides Related guides Related guides Related guides Related guides Related guides Related guides Related guides Related guides Related guides Related guides Related guides Related guides Related guides Find out more at www.gov.uk/service-manual/digital-by-default","description":"\n    The Digital by Default Service Standard is a set of criteria for digital teams\n    building government services to meet. Meeting the standard will mean digital\n    services are of a consistently high quality. This includes creating services\n    that are easily improved, safe, secure and fulfill user needs.\n  ","link":"/service-manual/digital-by-default-26-points/index.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Communications","indexable_content":"using blogs to report on a service’s progress how digital take-up will be measured for every service, how to increase digital take-up and digital take up case studies.  engaging with users during development talking to stakeholders to anticipate the needs of assisted digital users Service teams moving through the development process will want to make sure that users and stakeholders know about any changes to how they interact with government transactions. As many such redesigns are still in the early stages, guidance about talking about them is still evolving. In the meantime, you can read about:","description":"Service teams moving through the development process will want to make sure that users and stakeholders know about any changes to how they interact with government transactions.","link":"/service-manual/communications/index.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Making software","indexable_content":"The basics Security Licensing Configuring services Testing services Choosing technology APIs Cookies Release strategies Version control Development environment Standalone apps Deploying software Information security Logins Open standards and licensing Configuration management Analytics tools Sandbox and staging servers Code testing Accessibility testing Testing in agile","description":null,"link":"/service-manual/making-software/index.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Operating a service","indexable_content":"Operating your service Testing User support Sample stories Cloud security Monitoring Hosting DevOps Availability Operating service.gov.uk subdomains Penetration testing Load and performance testing Helpdesk Managing user support","description":null,"link":"/service-manual/operations/index.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Digital by Default Service Standard","indexable_content":"The criteria Service assessment: prompts and evidence Download the poster Alpha Beta Live             Understand user needs. Research to develop a deep knowledge of who the service users are and what that means for the design of the service.                     Put a plan in place for ongoing user research and usability testing to continuously seek feedback from users to improve the service.                     Put in place a sustainable multidisciplinary team that can design, build and operate the service, led by a suitably skilled and senior service manager with decision-making responsibility.                     Build the service using the agile, iterative and user-centred methods set out in the manual.                     Build a service that can be iterated and improved on a frequent basis and make sure that you have the capacity, resources and technical flexibility to do so.                     Evaluate what tools and systems will be used to build, host, operate and measure the service, and how to procure them.                     Evaluate what user data and information the digital service will be providing or storing, and address the security level, legal responsibilities, privacy issues and risks associated with the service (consulting with experts where appropriate).                     Make all new source code open and reusable, and publish it under appropriate licences (or provide a convincing explanation as to why this cannot be done for specific subsets of the source code).                     Use open standards and common government platforms where available.                     Be able to test the end-to-end service in an environment identical to that of the live version, including on all common browsers and devices, and using dummy accounts and a representative sample of users.                     Make a plan for the event of the digital service being taken temporarily offline.                     Create a service that is simple and intuitive enough that users succeed first time.                     Build a service consistent with the user experience of the rest of GOV.UK including using the design patterns and style guide.                     Encourage all users to use the digital service (with assisted digital support if required), alongside an appropriate plan to phase out non-digital channels/services.                     Use tools for analysis that collect performance data. Use this data to analyse the success of the service and to translate this into features and tasks for the next phase of development.                     Identify performance indicators for the service, including the 4 mandatory key performance indicators (KPIs) defined in the manual. Establish a benchmark for each metric and make a plan to enable improvements.                     Report performance data on the Performance Platform.                     Test the service from beginning to end with the minister responsible for it.         Alpha review (PDF) Alpha review (ODS) Beta assessment (PDF) Beta assessment (ODS) Live assessment (PDF) Live assessment (ODS)      The Digital Service Standard has changed from 26 points to a more concise 18. From 1 June 2015 all transactional services will be assessed on the new 18 points.         The Service Standard ensures digital teams build high quality government services.         A transactional service must meet each criteria to pass the Government Digital Service assessment. If a service doesn’t pass it won’t appear on GOV.UK.         Assisted digital support is an integral part of any service, helping users who can't complete the service on their own.         Digital Service Standard poster (PDF)    Find out more at www.gov.uk/service-manual/digital-by-default","description":"\n    The Digital Service Standard has changed from 26 points to a more concise 18. From 1 June 2015 all transactional services will be assessed on the new 18 points.\n  ","link":"/service-manual/digital-by-default/index.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Connecting to GOV.UK Verify","indexable_content":"Start using GOV.UK Verify GOV.UK Verify allows users to securely access digital government services, and makes sure that people using your service are who they say they are. Eventually, if you’re running a service that needs to know a user’s identity, you’ll have to use GOV.UK Verify, unless the user is a business or an agent, in which case you must use Government Gateway. To start using GOV.UK Verify, you need to go through a 6-stage onboarding process. Find out how the onboarding process works. This guidance is for people building government services, find out more about GOV.UK Verify.","description":"GOV.UK Verify allows users to securely access digital government services, and makes sure that people using your service are who they say they are.\nEventually, if you’re running a service that needs to know a user’s identity, you’ll have to use GOV.UK Verify, unless the user is a business or an agent, in which case you must use Government Gateway.","link":"/service-manual/identity-assurance/index.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Governance","indexable_content":"Governance principles Governing a digital service Funding a digital service Governing a scaled service Seeing progress Assuring digital services Governance principles Introduction to governance for service delivery Governance across the life of a service Governing the discovery phase Governing the alpha phase Governing the beta phase Governing the live phase Funding your digital service Scaling a service team When to scale up Governing a service after scaling up Seeing progress How delivery teams manage their work What to expect from the show and tell Setting up the right reporting Assurance for digital services Self assurance by Agile teams Assurance from those outside the service team","description":null,"link":"/service-manual/governance/index.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Printable cards for each page","indexable_content":"","description":null,"link":"/service-manual/cards/index.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: User-centred design","indexable_content":"User-centred design in alpha and beta Combining design and research to create user-focused services GOV.UK Design Principles  A guide to designing services that people prefer to use An introduction to user research techniques  Ways to understand your users and their needs GOV.UK elements Layout, grids, typography, colours, images, icons, form fields, buttons and data Design patterns Common patterns for forms and transactions Working with prototypes Learning by doing Accessibility How to make services that everyone can use Browsers and devices Which ones to test with, and how best to support them Service user experience Guidance for creating a consistent user experience Header and footer Guidance on using the GOV.UK header Print forms Designing or redesigning paper components for transactions How users read Reading age, reading online, plain English, learning disabilities Data visualisation Creating valuable and meaningful graphics to help analyse data Shared asset libraries Sharing your frontend assets so that they can be easily reused Sass repositories Sharing blocks of code and techniques People come to GOV.UK with specific needs. Anything that gets between our users and meeting those needs should be stripped away. The design of GOV.UK reflects this, existing primarily as a way of providing the right content and services to our users. Find out here how we approach this challenge. See all guides for designers","description":"People come to GOV.UK with specific needs. Anything that gets between our users and meeting those needs should be stripped away. The design of GOV.UK reflects this, existing primarily as a way of providing the right content and services to our users. Find out here how we approach this challenge.","link":"/service-manual/user-centred-design/index.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Guides by theme","indexable_content":"Getting started Understanding your audience Security and the law Designing and building your service Maintaining and improving your service         Assessments at GDS            Awarding the standard            Failing to meet the standard            Service design phases            Digital by Default Service Standard (original 26 points)            Communications            Digital by Default Service Standard            Maintaining the standard            Scope of the standard            Self-assessments and certification   ","description":null,"link":"/service-manual/themes/index.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Browse guides by topic","indexable_content":"Contents Agile Assisted digital Governance Making software Measurement Operations Phases Identity assurance Technology The team User-centred design Agile Assisted digital Governance Making software Measurement Operations Phases Identity assurance Technology The team User-centred design         Agile       What it is, why it works and how to do it            Continuous delivery       Making releases boring            Features of agile       Sprints, stand-ups and other regular meetings            Quality       How to define it, measure it and maintain it            Running retrospectives       Reviewing a team's work and the way it was done            Testing in an agile environment       How to get testing right            Training and learning about agile       How you can learn more about agile            What agile looks like       Common features of agile projects            Writing user stories       How to write a useful user story            Assisted digital support       Policy information for service teams            Developing assisted digital support       How to develop assisted digital support that meets the Digital Service Standard            Researching assisted digital users       A guide for service managers            Governance       Information and guidance for governing a digital service            Scaling a service team       Learn about adding people or adding teams            Seeing progress       How to judge progress without creating extra work for delivery teams            Assurance for digital services       Who’s involved in assurance, why it matters and how it works            Assurance from those outside the service team       How teams benefit from an independent view of their work            Funding your digital service       How to get approval and funding for your service            Governance across the life of a service       What to do in each phase of a service            Governance principles       Principles for governing service development            Governing a service after scaling up       How to keep governance simple and effective when you've scaled your team            Governing the alpha phase       What to do in alpha            Governing the beta phase       What to do in beta            Governing the discovery phase       What to do in discovery            Governing the live phase       What to do in live            How delivery teams manage their work       Understanding delivery team walls            Introduction to governance for service delivery       Governance for digital services - why it’s important and how to get it right            Self assurance by agile teams       How Agile approaches help teams and people who govern to assure their work            Setting up the right reporting       How to set up the reporting you need for service delivery            What to expect from the show and tell       How show and tells work and why they’re important if you’re involved in governance            When to scale up       Signs that you might need to add more people to your team            Making software       Information and guidance for building services            Accessibility testing       Testing to see if your service is inclusive            Analytics tools       Choosing the right tools for your service            APIs       Using and creating Application Programming Interfaces            Choosing technology       How to go about choosing what software tools to use            Configuration management       Manage a team's approach to software configuration            Cookies       When to use them, and when to tell users about them            Deploying software       Principles to ensure frequent, low-risk deployments            Development environments       Early infrastructure needs for agile projects            Exploratory testing       What is exploratory testing and how to use it            Information security       Making sure user data stays secure            Managing software dependencies       Making use of third party libraries and frameworks            Open standards and licensing       Use open standards to build your service            Progressive enhancement       How to create pages that work regardless of browser capability            Releasing software       How regular releases can reduce risk            Sandbox and staging servers       Working in a development environment            Standalone mobile apps       Government's position on mobile apps            Testing code       How to make sure your code does what it's supposed to            User accounts and logins       How to do them and how to avoid them            Using open source software       How open source fits into Government IT strategy            Version control       Make sure the team can work together on code            Measurement       Data you need to measure and monitor            Completion rate       How many people manage to use the service successfully?            Digital take-up       How to draw up a channel shift strategy for your service            Measuring cost per transaction       Work out the total cost of each transaction            Other metrics       Other metrics your service might measure or track            Performance Platform       Displaying performance metrics            User satisfaction       How satisfied are the people who use your service?            Using data       How to make use of the performance information your service collects            Domain Names, SSL, Email       Where things live on the web            Operating a service                   Cloud security       Secure use of third party cloud services            DevOps       Bringing development and operations together            Helpdesk       How to incorporate user enquiries into design            Hosting       Deciding where your service will live            Load and performance testing       How to avoid your service collapsing under pressure            Managing user support       Use helpdesk enquiries to improve services            Monitoring       Tracking the status of a service            Operating a service.gov.uk subdomain       Consistency across central government digital services            Service management       Managing a running service            Uptime and availability       Keeping the government online            User stories for web operations       A useful starting point when understanding the scope of infrastructure work            Vulnerability and penetration testing       Identify insecurities in your service            Service design phases       Learn about the discovery, alpha, beta, and live phases of service design            Connecting to GOV.UK Verify       For government service providers            Guidance for CTOs                   Creating a culture that supports change       The freedom to meet changing needs and expectations            End user devices       Delivering IT that meets the needs of users            Government as a platform       Services built on a shared core            Open data       Accessible, machine-readable data about services            Security as enabler       Using technological change to build secure services            Service integration and management       Managing providers in an efficient way            Spending controls       Navigating the controls process            Technology architecture                   Technology code of practice       Guidelines for the approval of technology spending            Recruitment       A hub of resources for bringing new skills into government            The team       What you'll need to build a successful service            Accessibility skills       What your team needs to build for inclusion            Content designer       Writing high quality content for sites and services            Delivery manager       Enabling teams to deliver high-quality services            Design skills       What designers do and what to look for            Developer skills       What developers do and what to look for            Digital leaders       Champions for digital throughout government            Job descriptions       Setting out the skills government expects from applicants            Recruitment Hub       Helping departments bring digital and technology skills into government            Service managers       Learn what a service manager does, and how to be a good one            Technology leadership       Organisational design for delivering successful change through technology            User research skills       What user researchers do and what to look for            Web operations skills       Helping develop secure, maintainable and available systems            Working with specialist suppliers       How people outside government can help build your service            Your working environment       The physical space and tools that bring out the best in a team            User-centred design       Designing content and services for GOV.UK            Accessibility       How to make services that everyone can use            An introduction to user research techniques       Ways to understand your users and their needs            Browsers and devices       Which ones to test with, and how best to support them            Card sorting       Ways of framing or interpreting information            Choosing appropriate formats       Help your users by providing content in a format they can use            Data visualisation       Creating valuable and meaningful graphics to help analyse data            GOV.UK header and footer       How to make your service look like GOV.UK            How users read       Reading age, reading online, plain English, learning disabilities            Print forms       Designing or redesigning paper components for transactions            Privacy note template for services       Writing questions, wording for labels, addressing the user and more            User needs       How to focus a service on users            User-centred design in alpha and beta       Combining design and research to create user focused services            What your service should look like       Guidance for creating a consistent user experience            Working with prototypes       Learning by doing   ","description":null,"link":"/service-manual/browse/index.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Information security","indexable_content":"What’s information security? Information security in government Assurance Service security Government assurance Risk management Concepts Roles IT Health Check Ongoing Senior Information Risk Owner (SIRO) Accreditor Information Asset Owner CESG (originally Communications Electronics Security Group) provide you with the briefest of introductions to information security explain the communities and processes that exist to help you build world-class secure services unauthorised or unintended access destruction disruption tampering confidentiality - the assurance that information is not disclosed to individuals or systems that are not authorised to receive it integrity - the assurance that information can’t be modified by those who are not authorised to modify it, or that any such modifications will not pass undetected availability - the assurance that information is available when it’s needed, and that mishap or malice cannot affect the ability of systems to provide information when requested physical controls, eg walls, locked doors, guards procedural controls, eg managerial oversight, staff training, defined emergency response processes regulatory controls, eg legislation, policy, rules of conduct technical controls, eg cryptographic software, authentication and authorization systems, secure protocols OFFICIAL SECRET TOP SECRET you’ll have a clear and accurate understanding of what risks you’re accepting those delivering the service will know what controls they’re going to use to reduce risks Senior Information Risk Owner (SIRO) Accreditor Information Asset Owner Communications Electronics Security Group (CESG) the risk profile of the service identifying all of the risks making sure that appropriate mitigations are in place so that the risks can be accepted understanding the process identify risks suggest mitigations technology choice staffing processes data aggregation When building your service, you’ll need to make sure that appropriate steps are taken to guarantee its security. Information security is a topic both broad and deep, drawing from fields ranging from economics and psychology through to mathematics and probability. This document doesn’t provide a thorough review of the field. Instead, it aims to: The term information security refers to the theory and practice of defending data or information systems against: There are 3 main concepts that security professionals frequently refer to: In government, much is made of these 3 main “concepts of information security,” as will be explained below. Security systems typically attempt to address one or more of these concerns through: Not every system requires a full range of security controls. ’Completely secure systems’ don’t exist, and overly secure systems are often too expensive or thoroughly inconvenient for their users. Aim to build a service that is appropriately secure, which you can achieve by managing risks. In practice, you’ll be guided by an assessment of the risks related to a lapse in the confidentiality, integrity, or availability of your service. The Government Security Classifications Scheme divides information assets into three tiers: The vast majority of HMG information assets will be classified as OFFICIAL, including personal and sensitive personal citizen data as defined by the Data Protection Act. Security at OFFICIAL is achieved through following good commercial practices, using well configured commodity technologies and by people taking personal responsibility and using their judgement more actively. The Securing Technology at OFFICIAL collection is a valuable resource for understanding how OFFICIAL information should be protected. Assurance is the broad set of activities involved in assessing and managing the risks associated with a system under development. If your assurance processes work correctly: The important thing to note about building trustworthy and secure systems is that it’s a team game. Involve some level of assurance in your project, no matter how small. This may be as simple as documenting the limited risks. Don’t make assurance a completely separate strand of work, or see it as a hurdle to be jumped over (or sidestepped). You’ll only end up with the best product by dealing with risk and making decisions based on a range of expert opinion. It’s also important not to see assurance as a one-off activity, or as a milestone to be achieved and then forgotten about. As with every other element of your service, you should constantly review and iterate your information security practices. When designing or operating a service, you’re responsible for making sure that the service, as a whole, provides appropriate security for the information you receive, process and store. Whether you’re buying a commodity service such as hosting as a component of your service or designing a service from scratch, the Cloud Security Principles are a useful way of thinking about different elements of your service’s security. These principles are designed as a common way for suppliers to explain how their services manage information security. They also provide an excellent framework for thinking about how your service should manage information security. Not all of these principles will be applicable to every service but they’re a good starting point. The Cloud Security Guidance collection contains more information on how these principles work and how security can be implemented against each principle. The following content will introduce you to assurance in government and will use quite a lot of acronyms. Unfortunately these are in common usage and it’s very hard to work with the existing documentation and processes without speaking the lingo. It’s included here in the hope that they’ll provide a helpful reference that can be used when reading existing documentation. It’s important to understand the different roles involved with the assurance process. Each role requires formal training and specialist skills, so decide who plays each of these roles from the very beginning: A SIRO will generally be a senior member of the lead organisation that’s providing the service and is responsible for: More hands on than the SIRO, the accreditor or accreditors will work with the project team to help with: The IAO will be a senior individual involved in running the relevant business. Their role is to understand what information is held, what is added and what is removed, how information is moved, and who has access and why. As a result they are able to understand and address risks to the information. CESG are the government agency responsible for Information Security. They can provide technical assistance or consultation on project issues. The IT Health Check (ITHC) forms part of the assurance process. In essence it’s a penetration test carried out by a CESG approved supplier (specifically a CHECK certified individual). Read guidance on penetration and vulnerability testing for more details. Assurance work is not just about getting a project to launch, but also covers the running of the resulting service. Over time, new threats may emerge, systems and processes may change, and assumptions may become invalid. Keep your documentation up to date and carry out additional penetration tests on a regular or as-needed basis. It’s important to understand that the assurance processes and tools are all about managing the risk associated with the running service. Security is part of this, but just one part. Nearly everything will contain risks: Understand those risks and put in place sensible and suitable mitigations. It’s unrealistic in most cases to aim for a system with no risks, and ignoring them is a recipe for future failure. Aim for a system where the risks are known, and have your team work with risk professionals to make careful decisions about how to deal with them.","description":"When building your service, you’ll need to make sure that appropriate steps are\ntaken to guarantee its security.","link":"/service-manual/making-software/information-security.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Job descriptions","indexable_content":"Templates Senior Civil Servant (SCS) roles Non-SCS specialist roles information about the department and its objectives information about the role and current priorities / activities in digital and technology location recruitment process details and contacts information on reporting lines GIS declaration and Immigration Status Forms Chief Digital Officer (CDO) Download as OpenDocument Format / Download as MS Word doc Chief Technology Officer (CTO) Download as OpenDocument Format / Download as MS Word doc Head of Business IT Download as OpenDocument Format / Download as MS Word doc Head of Policy and Performance Download as OpenDocument Format / Download as MS Word doc Service Manager Download as OpenDocument Format / Download as MS Word doc Technology Lead Download as OpenDocument Format / Download as MS Word doc Agile Coach (this is not a permanent role) Download as OpenDocument Format / Download as MS Word doc Business analyst Download as OpenDocument Format / Download as MS Word doc Content designer Download as OpenDocument Format / Download as MS Word doc Delivery manager Download as OpenDocument Format / Download as MS Word doc Designer Download as OpenDocument Format / Download as MS Word doc Developer Download as OpenDocument Format / Download as MS Word doc Digital Communications Lead Download as OpenDocument Format / Download as MS Word doc Junior visual designer Download as OpenDocument Format / Download as MS Word doc Performance analyst Download as OpenDocument Format / Download as MS Word doc Portfolio manager Download as OpenDocument Format / Download as MS Word doc Product manager Download as OpenDocument Format / Download as MS Word doc Programme Delivery Manager Download as OpenDocument Format / Download as MS Word doc Technical Architect Download as OpenDocument Format / Download as MS Word doc User researcher Download as OpenDocument Format / Download as MS Word doc Web ops Download as OpenDocument Format / Download as MS Word doc Many of the skills now needed in government – both from the leaders of digital and technology teams and the teams themselves – are ones that have rarely been looked for in the past. To help support recruitment teams across government in bringing in those skills GDS has written a set of template job descriptions for SCS and specialist posts. These job descriptions assume that organisational design guidance set out in the manual has broadly been followed. For advice on tailoring these designs to your organisation, contact GDS via the Recruitment Hub. The templates have left gaps for departments to complete. Recruiters will need to add: Before advertising any SCS technology position, the Recruitment Hub must have reviewed and agreed the job description after it has been tailored to a specific organisation and post. GDS does not need to approve non-SCS job descriptions prior to advertisement, though will provide advice if asked to by departments.","description":"Many of the skills now needed in government – both from the leaders of digital and technology teams and the teams themselves – are ones that have rarely been looked for in the past. To help support recruitment teams across government in bringing in those skills GDS has written a set of template job descriptions for SCS and specialist posts.","link":"/service-manual/the-team/recruitment/job-descriptions.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Knowledge-based authentication","indexable_content":"1. Call it an ‘Identity test’ 2. Make the identity test very easy to use 3. Don't tell users which questions they got right or wrong On this page: Call it an ‘Identity test’ Make the identity test very easy to use Don’t tell the user which questions they got right or wrong Confirm a user’s identity by asking them questions that only you and they know the answers to. Be honest about the purpose of asking these questions and reassure users by explaining where the data has come from. If you’re using data from someone’s credit history reassure them that their credit rating won’t be affected by the process. Write in plain language and stick to one question on each page.  If you’re naming financial or other institutions, use their commonly known name.  If you’re asking users to choose from multiple options, expose them as a set of radio buttons rather than hiding them in a drop-down list.  Revealing which answer is wrong allows fraudsters to learn the correct answers through trial and error. For the same reason, don’t stop asking questions as soon as the user passes or fails. Discuss this page on Hackpad","description":"Confirm a user’s identity by asking them questions that only you and they know the answers to.","link":"/service-manual/user-centred-design/resources/patterns/knowledge-based-authentication.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Lab-based user testing","indexable_content":"How lab-based user testing works Where and how you might use it Weaknesses and when not to use Number and types of participants efficiency accuracy recall emotional response User testing is a ‘qualitative’ research method, used to gauge how easy and intuitive a (product, service, website) is to use and whether it supports the needs of its intended audience. From a traditional perspective, user testing measures how well participants respond in the key areas of: This approach is combined with more ‘qualitative’ techniques to help understand the users’ motivations and attitudes as well. A product or service is usually assessed by asking small samples of the target audience to complete specific but realistic tasks in one to one sessions. The facilitator observes how well the participants are able to complete those tasks, noting down the areas or features that cause the participant problems. Often the facilitator will ask participants to think aloud whilst completing those tasks in order to understand their decision processes better. User testing can be conducted on low-fi assets (eg paper prototypes, wireframes) and hi-fi assets, including working prototypes and live digital services. It’s most effective when conducted early within the lifecycle of a product but can be conducted towards the end of the service lifecycle to validate any usability improvements. A lab environment can provide a more formal setting for conducting sessions, with most spaces offering viewing facilities enabling stakeholders and key team members to watch the sessions in real time. Usability testing is best suited to finding the big issues, essentially the problems that affect users trying to perform tasks. However, it is qualitative in nature and is not statistically robust due to the small participant numbers, per round. The lab environment may also be considered a weakness. It means testing is conducted in a very different space to that a user will commonly be in. This method is not an appropriate for understanding user behaviour or user needs eg what they might want from a product or service. Between 5-8 participants, per round of user testing is sufficient to highlight the majority of usability issues. The key is to test often.","description":"User testing is a ‘qualitative’ research method, used to gauge how easy and intuitive a (product, service, website) is to use and whether it supports the needs of its intended audience.","link":"/service-manual/user-centred-design/user-research/lab-based-user-testing.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Live phase","indexable_content":"Going live Post-launch stages Team requirements monitor system performance optimise the code ensure the service remains secure Final phase: retirement Previous phase: beta You’ve been building a service to meet users needs, and after your public beta you have a tested solution that is ready to release. To provide a fully resilient service to all end users the service should now meet all security and performance standards. You have configured your analytics to accurately monitor the key performance indicators identified in the building of your service, and you have planned the transition or integration of any existing services. You have liaised with the team governing the Digital by Default Service Standard to make sure that you have met the requirements of new and redesigned services. And, most importantly, you have met the user needs identified in the discovery, alpha and beta phases. This is not the end of the process. The service should now be improved continuously, based on user feedback, analytics and further research. Operational support – both technical and customer-focused – is in place, and you have implemented pro-active monitoring methods. These will help you to: You’ll repeat the whole process (discovery, alpha, beta and live) for smaller pieces of work as the service continues running. Find something that needs improvement, research solutions, iterate, release. That should be a constant rhythm for the operating team, and done rapidly. You’ll have identified the roles required to run your service, including the service manager and user support teams, while building the service. As different areas of your service are iterated and improved the team size will expand and contract, accommodating specialists as appropriate.","description":"You’ve been building a service to meet users needs, and after your public beta you have a tested solution that is ready to release.","link":"/service-manual/phases/live.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: User accounts and logins","indexable_content":"Find alternatives When there isn’t an alternative Credentials Lower usage rates Also see significantly increasing your user support overhead (people forget how to sign in, lose their passwords etc) gathering personal data that you’ll need to constantly review and protect adding a relatively complex interaction for users to complete provide a tool to help send the URL to an email address have instructions on creating a bookmark in their browser take their email address and let them know if the search results change who you’re providing a login service for – a small number of agents (administrative users, accredited partners etc) or for a broad range of citizens or businesses? if you have enough user data – enough to build trust with users or will you need to match them against other services (online or offline)? specifically monitor use of the system for attempts to gain access, like            unusually high numbers of failed login attempts over a short period of time       a sequence of failed logins on a given account over a long period of time         unusually high numbers of failed login attempts over a short period of time a sequence of failed logins on a given account over a long period of time separate user data from other data you hold to avoid collecting a large amount of identifiable information swap to a new identity system, like the GOV.UK Verify scheme, without invasive changes to the rest of your codebase build a system for a broad range of citizens and businesses do sophisticated matching with other systems so you can build confidence in the identity of your users Wikipedia on password policies CESG Good Practice Guide 44: authentication credentials in support of HMG online services xkcd cartoon explaining password vs. passphrase    For some admin systems on GOV.UK we use the   zxcvbn   library that measures how hard it would be for a computer to crack a   passphrase using brute-force methods. That library is used to validate   new passphrases and insist on strong passphrases. It’s only one measure,   but it increases our confidence that our users are picking good   passphrases.  GDS advise that teams do not build login systems. Building a login system is no easy task. While there are numerous open source libraries that make it trivial to add login functionality to your service, the moment you add that feature you’re: Many features that often come with using a login system can be completed in other (and potentially more useful) ways. Saving search results, for example, doesn’t require a login but just a way of helping users remember a specific URL. Instead of having users log in, you could: The exact details will vary according to what users need from your service, but if there’s an alternative to a login system – use it. If, after careful review and design work, there’s no option but to build a login system, you’ll need to consider: It’s probably safe to carry on if you’re building a service for a small number of clearly identified agents. Make sure that any authentication and authorisation code written for your system is carefully separated from the application so that you can: Read the advice published by the GOV.UK Verify if you need to: You should help your users to pick strong, secure passwords or phrases and consider whether it’s appropriate to require 2-factor authentication for extra security. As a minimum passphrases should be eight characters long and include a mix of letters, numbers and symbols, but ideally they’ll be longer than that. We refer to passphrases as a phrase is usually easier to remember but harder to guess than a short collection of symbols or a single word. For some admin systems on GOV.UK we use the   zxcvbn   library that measures how hard it would be for a computer to crack a   passphrase using brute-force methods. That library is used to validate   new passphrases and insist on strong passphrases. It’s only one measure,   but it increases our confidence that our users are picking good   passphrases. All new government services should be served over HTTPS to ensure the communication between the user and the service is encrypted. This is especially important when logging in. The Electronic License Management System (ELMS) license application system on Business Link required a login to complete an application. In building a new version of the system for GOV.UK we removed that requirement and usage rates have increased considerably. There’s still a login system for approved users in local authorities who need to process those applications.","description":"GDS advise that teams do not build login systems.","link":"/service-manual/making-software/logins.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Maintaining the standard","indexable_content":"continuously update and improve the service on the basis of user feedback, performance data, changes to best practice and service demand contribute to best practice developed across government to be shared through the service manual show high levels of user satisfaction are maintained in the digital and assisted digital service show high levels of transaction completion are maintained in the digital and assisted digital service show cost per transaction is decreasing in line with plans submitted ahead of the service’s launch show digital take-up is increasing in line with plans submitted ahead of the service’s launch, and assisted digital support is targeted at the people who really need it make all new source code open, reusable and publish it under appropriate licenses (or else give a convincing explanation of why this can’t be done for specific subsets of the source code) The launch of a service on GOV.UK is only the beginning. Once it has been released, users become the arbiters of its quality. Having designed the service to be iteratively developed using performance data and user research, service teams will have all the tools and techniques they need to continuously improve what they offer. Performance against the 4 key indicators will be tracked and publicly displayed. To keep to the standard, your team must:","description":"The launch of a service on GOV.UK is only the beginning. Once it has been released, users become the arbiters of its quality. Having designed the service to be iteratively developed using performance data and user research, service teams will have all the tools and techniques they need to continuously improve what they offer. Performance against the 4 key indicators will be tracked and publicly displayed.","link":"/service-manual/digital-by-default/maintaining-the-standard.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Managing user support","indexable_content":"Arrange enquiry data into subgroups Potential subgroup categories Send data to appropriate groups Treating enquiries as defects Improvement projects Enquiry type Requester details Reply type Enquiry status Enquiry category or categories Service category or categories Root cause category or categories how complex your enquiries are the sophistication of the systems you use to handle them channel (phone, email, chat, social media, surface mail etc) the target group that can act on the feedback question problem complaint Freedom of Information (FOI) request non-actionable rant support level priority internal resolving group resolving agent(s) day/date/time received day/date/time resolved page failing to load database down forgotten password user error software bug Now you’ve created a helpdesk, it’s time to figure out how to make the best use of user feedback so you can improve your service and your user experience. Your ability to act quickly and constructively on user feedback depends on how well you can arrange enquiries for analysis. How you choose to put enquiries into relevant subgroups will depend on: These factors will decide if the process of arranging enquiries will be carried out manually (staff create the metadata for the enquiries they handle) or be automated (the software you use for handling enquiries will add most of the metadata automatically). At the very least, arrange enquiries by: In addition to the very basic channel and target arrangements mentioned above, the following categories may be suitable. Is the enquiry a: Given user privacy concerns, spend time thinking about how you’ll collect data on your users. At the very least, gather minimal information on who is requesting assistance. Is a reply necessary/expected or not? Is an enquiry open, pending some other action, solved etc Some internal sectional or functional categories along which you’ll want to be able to stratify–think of separate URLs or sections on a large website as a concrete example. Capture information on the different aspects of the handling processes. You’ll want to have information on: The ultimate reason for contacting helpdesk, eg: As with most of these, tailor these categories based on the specifics of your product and support model. Now that you’ve gathered your data, you need to decide how to share it. Sharing data is easier if all the data can be handled and used directly within your organisation. However, it’s likely that different teams will act on different types of problems, and some may be outside of your department. If possible through your system, use automation for sharing data. Treating each enquiry from a user as being due to a defect in the service itself is a very ambitious improvement model. Here enquiries are either seen as a fault in your support model for your service, or in the communications processes you use to handle enquiries. While this can exaggerate the reason for some kinds of contact (is a user’s email of thanks and appreciation genuinely a defect?) it’s an excellent way to direct critical thinking on how you provide service. Once the enquiry-level improvement data has been sent to the right group, begin doing analysis for improving processes. Join your enquiry level data with cost data to help prioritise which areas to target first. But you can also simply use the number (or percentage) of affected users as a tool for prioritising. In any case, make sure you have access to staff with experience in process analysis and improvement, ideally with relevant contact centre or technology experience. Directly involving the individuals who handle enquiries and those who built the service into improvement projects will produce better and faster results.","description":"Now you’ve created a helpdesk, it’s time to figure out how to make the best use of user feedback so you can improve your service and your user experience.","link":"/service-manual/operations/managing-user-support.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: A/B and multivariate experiments","indexable_content":"Further reading This article in Wired shows how A/B experiments were used to good effect in Obama’s election campaign. This article in eConsultancy shows how multivariate experiments were used to improve conversion rates at LoveFilm. We interviewed Craig Sullivan, an industry expert on conversion optimisation. He explains when he uses A/B and multivariate experiments in the design process.","description":"We interviewed Craig Sullivan, an industry expert on conversion optimisation. He explains when he uses A/B and multivariate experiments in the design process.","link":"/service-manual/user-centred-design/user-research/multivariate-testing.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: People's names","indexable_content":"Single name field Multiple name fields Further reading The good The bad Labels Titles Internationalisation can accomodate any kind of name requires the least effort to use avoids the issue of how to label multiple fields difficult if you need to parse out things like last name can’t use formal contractions in correspondence (like ‘Mr. Smith’) may be difficult to integrate this approach with legacy back-end systems a person’s name won’t fit the format you’ve chosen they enter their names in the wrong order they try to enter their full name in the first field Given name(s) Family name Any other names (eg maiden name) http://baymard.com/blog/mobile-form-usability-single-input-fields http://www.w3.org/International/questions/qa-personal-names http://www.kalzumeus.com/2010/06/17/falsehoods-programmers-believe-about-names/ Be sensitive to different cultural conventions when asking for people’s names. If you can, use a single name field. A single text field where the user can enter their full name, including title if they wish. The good: The bad: If you’re constrained by business or technical requirements you can use multiple fields. Depending on your audience you’ll be able to extract the parts of the name and do stuff with them. As soon as you adopt multiple fields you introduce the possibility that: If you need to use multiple name fields then the preferred labelling is ‘First name’, ‘Last name’.  Don’t include ‘Middle names’ unless you absolutely have to and make sure it’s optional (you don’t need to mark it as optional as users will understand this). We recommend against asking for people’s title.  It’s extra work for users and you’re forcing them to potentially reveal their gender and marital status,  which they may not want to do. There are appropriate ways of addressing people in correspondence without using titles. If you have to implement a title field, make it an optional free-text field, not a drop-down list. Predicting the range of titles your users will have is impossible, and you’ll always end up upsetting someone.  Remember to deal with the name data sensibly in any resulting correspondence. Users of GOV.UK services come from many different cultural backgrounds, each with their own conventions regarding personal names. If you can’t use a single name field and your service is used by many users outside the UK you can use this format: For a more detailed overview of issues relating to personal names, read these articles: Discuss this page on Hackpad","description":"Be sensitive to different cultural conventions when asking for people’s names.\nIf you can, use a single name field.","link":"/service-manual/user-centred-design/resources/patterns/names.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: National Insurance numbers","indexable_content":"Use a single text field labelled ‘National Insurance number’. National Insurance numbers are 9 characters long, but the characters are often spaced in pairs  so you’ll need to allow for at least 13 characters in any text fields. Don’t use ‘NINO’ or ‘NI number’ as the label. Do not use “AB 12 34 56 C” as an example (it belongs to a real person).  Instead, use: “QQ 12 34 56 C” Discuss this page on hackpad","description":"Use a single text field labelled ‘National Insurance number’.","link":"/service-manual/user-centred-design/resources/patterns/national-insurance-number.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Online Omnibus surveying","indexable_content":"Where and how you might use it Weaknesses and when not to use Timescales Online Omnibus surveys are an effective way of interviewing a representative number of people, in a short period of time, and for a relatively low cost. Omnibus surveys of this type use an online panel to gather the sample and, as with regular offline Omnibus surveys, costs are kept down by collating questions on a variety of subjects from a number of clients. Omnibus panels are ideal when you have key questions that you want answering, and need to reach a representative number of people quickly and cheaply. An omnibus survey is not appropriate when there are too many questions required. This would result in the survey being too long (combined with questions from other clients). An online Omnibus panel can be conducted relatively quickly, with most companies running surveys twice a week with data delivered 4 days after survey goes in to field. Some companies offer an ‘on demand’ service where surveys can start any time, although a minimum number of questions is normally required.","description":"Online Omnibus surveys are an effective way of interviewing a representative number of people, in a short period of time, and for a relatively low cost. Omnibus surveys of this type use an online panel to gather the sample and, as with regular offline Omnibus surveys, costs are kept down by collating questions on a variety of subjects from a number of clients.","link":"/service-manual/user-centred-design/user-research/online-omnibus-survey.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Online research panels","indexable_content":"Where and how you might use them When not to use Timescales An online panel is a collection of pre-recruited research participants who have agreed to take part in online research over a period of time. Members of the panels are incentivised to take part, and normally rewarded through vouchers, or points that have a monetary value. As members of the panel are pre-recruited it means that they can be easily targeted by demographic, ownership and lifestyle information. As well as regular consumer panels, some companies also run business panels. Online panels are used to target representative samples of people easily, cheaply and quickly. They can also be useful to target hard to reach groups, who would otherwise be impossible to reach within a realistic timeframe. Respondents completing research in the shortest amount of time is a common problem, as this indicates that they have not given full consideration to the research that they are taking part in. Most panel companies reduce this prospect by monitoring how respondents complete the research (by time completion, and by the way they answer certain questions) and remove any suspicious respondents from their database. In addition to this, most panel companies will ensure that their respondents only complete four to six surveys a year. As the sample is ‘on tap’ online panels can be used to turn round projects very quickly. When used for remote user testing, fieldwork is normally complete within three to four days.","description":"An online panel is a collection of pre-recruited research participants who have agreed to take part in online research over a period of time. Members of the panels are incentivised to take part, and normally rewarded through vouchers, or points that have a monetary value.\nAs members of the panel are pre-recruited it means that they can be easily targeted by demographic, ownership and lifestyle information. As well as regular consumer panels, some companies also run business panels.","link":"/service-manual/user-centred-design/user-research/online-research-panels.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Open data","indexable_content":"The data that you should make open Making your data open Building on open data Building trust Licensing your open data for reuse Why we do this Examples of open data in action Where to get help Further reading accessible (ideally via the internet) at no more than the cost of reproduction, without limitations based on user identity or intent in a digital, machine-readable format for interoperation with other data free of restriction on use or redistribution in its licensing conditions on which public services are run and assessed on which policy decisions are based is collected or generated in the course of your service delivery Public data policy and practice will be clearly driven by the public and businesses that want and use the data, including what data is released when and in what form. Public data will be published in reusable, machine-readable form. Public data will be released under the same open licence which enables free reuse, including commercial reuse. Public data will be available and easy to find through a single, easy-to use, online access point (data.gov.uk). Public data will be published using open standards, and following relevant recommendations of the World Wide Web Consortium (W3C). Public data from different departments about the same subject will be published in the same, standard formats and with the same definitions. Public data underlying the Government’s own websites will be published in reusable form. Public data will be timely and fine-grained. Release data quickly, and then work to make sure that it is available in open standard formats, including linked data forms. Public data will be freely available to use in any lawful way. Public data will be available without application or registration, and without requiring details of the user. Public bodies should actively encourage the reuse of their public data. Public bodies should maintain and publish inventories of their data holdings. Public bodies should publish relevant metadata about their datasets and this should be available through a single online access point; and they should publish supporting descriptions of the format provenance and meaning of the data. your users information and informed choice about the services they use your service managers the information they can rely on to provide what your users need businesses and the community or voluntary sector the opportunity to take the data released and produce goods and services from it Analytics tools Open formats Open standards Open Data White Paper: Unleashing the Potential Open Government Partnership: UK National Action Plan 2013 National Information Infrastructure Data that is truly open is: Overall, government produces a lot of data that describes the services that we offer and how well those services are performing, for example data from analytics tools or key performance indicators. There’s also data on how people use these services and who those people are. Data about service performance allows service managers to see how well a service is running. It also means that users can hold us to account. Data about service performance should therefore be public data. You should publish all public data, unless it is private data collected from people or restricted for national security reasons. Public data is anonymised data: If for some reason you have made a procurement choice that means your performance data is monitored or stored by a third party, you should make sure that you have the right to access, export, share and reuse that data openly and in an open format. Your department has its own Open Data Strategy (for 2012-2014) and will already be publishing data sets on data.gov.uk. Your commitment to open, public data doesn’t stop there. Your open data should be user-friendly and findable. You should support people who want to reuse it and provide guarantees about how it will be made available. Publishing on data.gov.uk makes your data findable and will tell you how it ranks on a 5-star rating scheme to indicate whether the data and the format that it is published in is open. You should aim to reach at least the 3-star standard. The Open Data Certificates from the Open Data Institute measure how usable the data is for people who want to reuse it, and help identify ways in which you can improve how you publish the data. You should get a certificate for your data. The Open Data Principles that you must follow are: Users are already using your services, giving you lots of data about their behaviour. This means you can learn from real world behaviour when you’re designing a new digital service. You can watch and learn from your users, shaping the system to fit what people naturally choose to do. Recognise that you can’t do it all. In making your data open, you are encouraging greater use of the data and helping users to innovate. Developers need to be able to use the data, to share it, and combine it with other data to use in their own applications, for example through application programming interfaces (APIs). As with all government digital services you’ll need to understand the user need for the data you publish. You’ll need to keep developers aware of what datasets you are releasing and to maintain relationships with those primary data users at the cutting edge of technology who can help you to do things differently and in more agile ways. You can use the engagement areas of data.gov.uk to reach out and keep in touch with data users. You need to be scrupulous in protecting individual privacy by taking appropriate steps to ensure personal data is secure. You should consider privacy issues at the beginning of all discussions concerning the release of a new dataset or the building of or change to a digital service dealing with personal data. If you are using new technology to handle personal data or reusing the data in a different way, you may need to carry out a Privacy Impact Assessment. This is an important part of the process for identifying and managing risk. The Cabinet Office is preparing a new Code of Practice (Datasets) (the consultation on this draft version is now closed). This code includes the licensing framework you must use when making your datasets available for reuse. There are a number of licenses in the framework. For UK government open data, this will be the Open Government Licence. Your open data can give: Building your services on open formats and open standards means that you can more easily share, reuse or exchange data. It will also mean that you will have a choice in which technology to implement, rather than being limited to a particular product or supplier. You can browse the apps section of data.gov.uk to see hundreds of applications that have been built using open government data. You can also find case studies of how people are sharing, using and building open data on the data.gov.uk site. The team at data.gov.uk is a good first point of contact for help in understanding how to make your data open. You can get in touch with them through data.gov.uk. The Open Data Institute and the Open Knowledge Foundation can give you advice and training on how to open up your data. The Government Statistical Service has developed computer-based training to help you to make the right decisions about how to make data available in open formats.","description":"Data that is truly open is:","link":"/service-manual/technology/open-data.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Open programme","indexable_content":"Who should take part Supporting people to use your digital service Design and improve your digital service Practice, procurement and platforms for your digital service Request a place Design principles in action Always be testing Data-driven services Procurement, spend controls and governance Being agile Identity assured      learn about why it’s important to support people to use your service         develop your understanding of why people might not use your digital service and how to overcome barriers         walk personas through a user journey, to see where they might need support and what that could look like         discuss the requirements of the Digital by Default Service Standard for these areas, covering expectations at each development phase         share your experience with other service managers and work together to improve your understanding of effective support for digital services.         discover the 10 design principles and how they were created         hear from a product manager who has used them on a project         consider how they will be applied to your own services         keep your team concentrating on real user needs         help teams design products which are prioritised by user needs          help teams iterate products in response to user feedback         discuss your service’s performance indicators as they currently stand         consider what data you currently use to inform your decision-making, and what data you would like to use         design your own performance data dashboard to understand your services in a simple, visual way         find out about the Performance Platform and how you can use it         consider how to procure tools and systems fit for the purpose of agile, user-centred development         discuss the range of options available, and how to determine make or buy decisions         hear the latest on the government procurement processes and frameworks available on digital transformations         discuss the prevailing investment approvals and spend controls         agile artefacts, themes, epics and user stories         sprint planning and the use of walls to make shared priorities visible         retrospectives to continually inspect and improve the work teams do together         creating a market so that users can choose from a number of identity providers         setting standards for consistently meeting the needs of users, services and departments         building and running the hub that connects services to identity providers         working with departments and agencies to identify their services’ GOV.UK Verify requirements and plan their transition to using the GOV.UK Verify service    These 3 days contain specialist modules to help you succeed and improve in particular areas of the Digital by Default Service Standard. Unless already very familiar with digital in government, participants should complete either the specialist digital foundation day or the service manager induction before taking part in the open programme. All sessions will run at a central London location. The programme is funded centrally, but you or your department will need to pay for any accommodation, meals and subsistence expenses, in line with your departmental policies. This one day module will cover digital take-up, digital inclusion and assisted digital, and focuses on supporting your users to use your digital service. These are all areas required by the Digital by Default Service Standard. It’s essential to ‘shift’ people away from non-digital channels and increase the take-up of government digital services so more users can benefit from improved government services. Departments and agencies will also come under increasing pressure to deliver the return on the investment in digital services – estimated at between £1.7 and £1.8 billion per year. Digital inclusion helps people become capable of using and benefiting from the internet. This means that most users will be able to use new and redesigned government digital services unaided, but there are some who will need help through assisted digital support. You will: learn about why it’s important to support people to use your service develop your understanding of why people might not use your digital service and how to overcome barriers walk personas through a user journey, to see where they might need support and what that could look like discuss the requirements of the Digital by Default Service Standard for these areas, covering expectations at each development phase share your experience with other service managers and work together to improve your understanding of effective support for digital services. This one day module will cover the GOV.UK Design Principles, user research and data-driven services. These are all areas required by the Digital by Default Service Standard. A set of simple but powerful design principles underpin all the work done by GDS to develop the award-winning GOV.UK website. In this session your group will: discover the 10 design principles and how they were created hear from a product manager who has used them on a project consider how they will be applied to your own services Carry out user research in every stage of your project. Do it continuously through each stage – don’t leave it as something that happens at the beginning and end of phases. You will find out how doing user research continuously will: keep your team concentrating on real user needs help teams design products which are prioritised by user needs  help teams iterate products in response to user feedback Using data to inform decision-making is vital to the development of your services. Your group will: discuss your service’s performance indicators as they currently stand consider what data you currently use to inform your decision-making, and what data you would like to use design your own performance data dashboard to understand your services in a simple, visual way find out about the Performance Platform and how you can use it This one day module will cover governance, procurement and spend control in an agile context, along with an overview of the GOV.UK Verify platform. These are all areas required by the Digital by Default Service Standard. Delivering high quality digital services that can be improved continuously requires investment in the right capabilities. As a group you will: consider how to procure tools and systems fit for the purpose of agile, user-centred development discuss the range of options available, and how to determine make or buy decisions hear the latest on the government procurement processes and frameworks available on digital transformations discuss the prevailing investment approvals and spend controls … all in the context of governance that helps to direct, steer, monitor but above all else, enable service delivery. GDS delivery managers will lead discussions and exercises around key features of agile, such as: agile artefacts, themes, epics and user stories sprint planning and the use of walls to make shared priorities visible retrospectives to continually inspect and improve the work teams do together We need to know that users of digital services are who they say they are. In this session, you will learn how the GOV.UK Verify Programme is: creating a market so that users can choose from a number of identity providers setting standards for consistently meeting the needs of users, services and departments building and running the hub that connects services to identity providers working with departments and agencies to identify their services’ GOV.UK Verify requirements and plan their transition to using the GOV.UK Verify service All foundation day participants will be invited to book their places on the open programme. If you have not completed this, but are very familiar with digital in government and wish to book straight onto the open programme, please contact the Skills Team for an invitation: digital.academy@dwp.gsi.gov.uk. Please note that the 3 days outlined above have recently been reviewed and the content is now covered across 2 days. To find out about dates and for advice about which days will best suit your needs, please contact digital.academy@dwp.gsi.gov.uk.","description":"These 3 days contain specialist modules to help you succeed and improve in particular areas of the Digital by Default Service Standard.","link":"/service-manual/the-team/learning-and-development/open-programme.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Using open source software","indexable_content":"Government IT Strategy A level playing field Understanding risks Further reading characteristics of the software support models level of maturity future direction Choosing a license    Where appropriate, government will procure open source solutions. When used in conjunction with compulsory open standards, open source presents significant opportunities for the design and delivery of interoperable solutions.     Ensure a level-playing field for open source software. Demonstrate an active and fair consideration of using open source software – taking account of the total lifetime cost of ownership of the solution, including exit and transition costs.  Formally the Government IT Strategy says the following about open source. Where appropriate, government will procure open source solutions. When used in conjunction with compulsory open standards, open source presents significant opportunities for the design and delivery of interoperable solutions. As part of that strategy Government committed to producing an Open Source Procurement Toolkit. Government has a level playing field between proprietary and open source software. This is formalised in the Technology code of practice. Ensure a level-playing field for open source software. Demonstrate an active and fair consideration of using open source software – taking account of the total lifetime cost of ownership of the solution, including exit and transition costs. CESG, the National Technical Authority maintain a series of good practice guides for information assurance, including Good Practice Guide (GPG) 38 - Open Source Software - Exploring the Risk. Broadly this concludes the quality of the software is the primary driver for security, rather than the license model. Closed or open source code is not more or less secure in the majority of cases due to licensing alone. The licensing model doesn’t really have an impact on how you use software. You need to consider the same things as part of selecting technology: Open Source has a vibrant commercial ecosystem, and lots of companies provide support or work on proprietary and open source software. What varies is generally how you buy something (licenses vs pure support), what that means for internal skills and ultimately your rights to modify the software for your needs. In reality it’s very likely you’ve used open source software at some level of the stack already. Whether that’s the Linux operating system or the Java programming language, or it might be ASP.NET MVC or NHibernate on a Microsoft platform.","description":"Formally the Government IT Strategy\nsays the following about open source.","link":"/service-manual/making-software/open-source.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Open standards and licensing","indexable_content":"Open standards explained Building on open standards Choosing which open standards to use Why GDS do this Benefits of using open standards Further reading Exemptions Collaboration Transparency Due process Fair access Market support Licensing rights collaboration transparency due process fair access market support licensing rights place the needs of your users at the heart of your standards choices your selected open standards will make it possible for suppliers to compete on a level playing field your choices of standards support flexibility and change you take on open standards that support sustainable cost your decisions on standards selection are well-informed you select open standards using fair and transparent processes you are fair and transparent in the specification and implementation of open standards user and functional needs security and legal requirements economic efficiency of government as a whole interoperability preventing lock-in you’re considering use of a standard that doesn’t meet GDS’s definitions of an open standard you want to use an alternative standard to one that performs the same function (if a compulsory standard has already been selected for that specific purpose) improving adaptability and the ability for government to provide services based on users’ needs – avoiding digital exclusion based on the technology choices made putting in place a level playing field for open source and proprietary software, giving government the ability to move between different technologies without the risk of lock-in making it easier to share appropriate data across and beyond government boundaries, providing efficient services for users and delivery partners making the cost of government’s digital services more sustainable by making things simpler and encouraging reuse provide you with an adaptable design give you greater choice make it possible for your digital services to change over time software interoperability data formats document formats Standards Hub - go here to get involved in the debate about which open standards we should choose and to find out about which ones we’re adopting. Completed challenges – go here on the Standards Hub to find out which standards profiles are compulsory for use in government. Open Standards Principles Open Standards: Open Opportunities consultation outcome This information is to be used as guidance on using open standards and why they matter, and should not be taken as legal advice. GDS define open standards for software interoperability, data, and document formats as those that meet all of these criteria: The standard is maintained by working together in a decision-making process that is consensus-based and independent of any individual supplier. Involvement in the development and maintenance of the standard is accessible to all interested parties. The decision-making process is transparent and a review is carried out by subject matter experts, which is open to the public. To guarantee quality, the standard is taken on by a specification or standardisation organisation, or a forum or consortium with a feedback and approval process. The standard is published, thoroughly documented, and publicly available at zero or low cost. Other than creating innovative solutions, the standard is supported by the market and displays platform, application and vendor independence. Rights are essential to the implementation of the standard, and for communicating with other implementations using that same standard. They’re licensed on a royalty-free basis that’s compatible with both open source and proprietary licensed solutions. These rights should be irrevocable unless there is a breach of licence conditions. There are 7 open standards principles to follow when thinking about which open standards to use: GDS describe the reasons for these principles and their implications in ‘Open Standards Principles’, published in November 2012. The Open Standards Board will be selecting some compulsory open standards using the Standards Hub process. Use standards that have been identified. There might be instances where you’re looking for an open standard and GDS have not set a compulsory one in that space. If so, do a thorough assessment of existing standards and choose one that meets your needs and is consistent with GDS’s definitions. You should also consider how the standard fits with: You’ll need to apply for exemptions to open standards if: By implementing open standards for software interoperability, data and document formats, government is: Basing your build of component-based digital services on open standards will: Expressing your user needs in terms of required capabilities, which are in turn based on open standards, helps you to make better choices for service delivery. It also means that there is no unintentional lock-in built into government digital services. Whether designing and building in-house or outsourcing, your solutions need to comply with open standards (where they exist and meet functional needs) for: On a case-by-case basis, an exemption may be agreed in advance if there’s an exceptional reason why using open standards is inappropriate. The government’s Chief Technology Officer will agree this, or it could also be through Departmental Accounting Officer procedures for cases below the Cabinet Office’s spend controls threshold for IT.","description":"This information is to be used as guidance on using open standards and why they matter, and should not be taken as legal advice.","link":"/service-manual/making-software/open-standards-and-licensing.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Operating a service.gov.uk subdomain","indexable_content":"One entry point Creating a domain Subdomains Transport Layer Security Cookies robots.txt and root level redirections Origin servers for CDN-based provider of DDOS protection Emails sent to service users Lifecycle of service subdomains www.servicename.service.gov.uk is for the public facing, dynamic web pages that make up your service. assets.servicename.service.gov.uk is for assets such as static images and shared JavaScript files needed to run your live service (note: written content about the service, such as guides to eligibility or detailed guidance for applicants, SHOULD be on GOV.UK) admin.servicename.service.gov.uk is for features that enable non-technical staff to run the service (eg contact centre staff might use this subdomain to access and process work items where human judgement is needed)      have a robots.txt file on the www, admin and assets subdomains asking search engines not to index any part of the site. Example content for robots.txt is given below, and more details can be found on The Web Robots Pages:      User-agent: * Disallow: /          have an HTTP 301 redirection from the top-level index page of the www and assets subdomains to the relevant start page on GOV.UK. (Note: this means that the service start page on GOV.UK SHOULD NOT link to the root of the www domain.)    www-production.servicename.service.gov.uk admin-production.servicename.service.gov.uk assets-production.servicename.service.gov.uk the DDOS protection provider’s servers the locations where the service itself is being developed and/or managed continuing to use SSL serving a redirect from your service to the GOV.UK start page    Note: This document is written as a ‘standard’, and as such uses the words MUST, SHOULD, MAY and MUST NOT as defined in RFC 2119.     Note: This does not apply to the set of interactive tools on GOV.UK known as ‘smart answers’ which are developed and maintained by GDS in partnership with other government departments.  Government offers a number of different digital services to citizens. While the start and end of a user’s journey will be on GOV.UK, the service itself will typically be hosted elsewhere, and will need a different domain name as a result. This page describes the use of service.gov.uk subdomains for hosting digital services. Note: This document is written as a ‘standard’, and as such uses the words MUST, SHOULD, MAY and MUST NOT as defined in RFC 2119. Every digital service offered by the UK government MUST have a single, well-known place on the internet where users can go to when they want to use the service. That well-known place will be the relevant start page on GOV.UK – for instance, the DVLA’s vehicle tax service is at https://www.gov.uk/vehicle-tax. Service managers MUST NOT advertise any URL other than that of the GOV.UK start page as the starting point for the relevant service. This is what gets printed in literature and used in email signatures, TV adverts etc. The start page URL for a given service will be allocated by GDS based on discussions with the service manager and analysis of user behaviour, search referrals and other relevant data. The transactional part of a service – the dynamically generated pages where users interact with the service – will typically not be hosted on the www.gov.uk domain. That means that each service needs its own domain name for the transactional part of the service. Note: This does not apply to the set of interactive tools on GOV.UK known as ‘smart answers’ which are developed and maintained by GDS in partnership with other government departments. For all new digital government services GDS will create a domain name of the form servicename.service.gov.uk (where “servicename” is a plain English description of the service agreed between the relevant dept/agency and the Government Digital Service). This will introduce consistency across central government domains for digital services and remove the dependency on departmental subdomains (which are of course vulnerable to machinery of government changes) and the now-retired DirectGov and BusinessLink online brands. The process of obtaining a service.gov.uk subdomain begins when the service manager emails gdsapprovals@digital.cabinet-office.gov.uk.  Subdomains of service.gov.uk SHOULD describe the service (eg lastingpowerofattorney.service.gov.uk) and should not contain the name of the service owning department or agency (eg ministryofmagicwandregistration.service.gov.uk) The service-owning dept/agency will be given delegated authority to manage the domain and its subdomains, although in some cases this work will be carried out by third party suppliers. This section gives some guidance about which subdomains a service manager should create once they have been given control of servicename.service.gov.uk. Maximum number of visible subdomains The user-facing live service SHOULD be operated using at most three user-visible subdomains of servicename.service.gov.uk: You SHOULD NOT create separate domains for application programming interfaces (APIs) unless there’s a really good reason to have a completely separate domain. (Really good reasons are few and far between.) Service managers should notify the Government Digital Service technical architects (via your transformation team contact) if you intend to create user-visible subdomains other than the three listed above. We’re developing some patterns for more unusual system designs as well as for mainstream transactional services, and we’re always up for a discussion about exceptions and edge cases. Usernames and passwords If the service is a private alpha or private beta release then it should be protected by a username and password known only to the development team and the users who are testing the service. If a service, or part of a service, is a public alpha or beta releases then it should be clearly marked as such with a text label on every page (ie don’t use an image containing the word alpha or beta) and in every API response. Multiple environments It is good practice to have multiple ‘environments’ for the development, testing and live (aka production) versions of any service. The development and testing environments allow the team to assess the correctness and quality of the service before it goes live. Typically, the subdomains used to access a development or testing instance of the service are structured in the same way as the subdomains used in the live version of the service. Therefore, you MAY create other subdomains of servicename.service.gov.uk for use in testing and development, such as www-preview. and www-dev, or www.preview. and www.dev.. If there’s a compelling reason to use a non .gov.uk domain for testing and/or development subdomains, that’s also acceptable. Regardless of the domain name used, web-based services on testing and development domains (including APIs) should be protected by a username and password along the same lines as private alpha and beta releases. Many services will collect personal information from users. It’s very important that this information can’t be intercepted by malicious third parties as it travels over the internet. Therefore, all services accessed through service.gov.uk domains (including APIs) MUST only be accessible through secure connections. For web-based services this means HTTPS only (often referred to by the acronyms TLS or SSL, which both refer to the protocol underpinning these secure connections). Services must not accept HTTP connections under any circumstances. Once a service manager has verified that their HTTPS setup is working fine they SHOULD enable HSTS on the production domains (www., admin. and assets.), by setting an HTTP response header such as representing a commitment to HTTPS-only traffic for 14 days. Once the service manager is confident that HSTS is configured correctly, they SHOULD increase the commitment to months or years: Cookies used on www.servicename.service.gov.uk and admin.servicename.service.gov.uk MUST be scoped to the originating domain only. Cookies MUST NOT be scoped to the domain servicename.service.gov.uk. Cookies SHOULD NOT be used on assets.servicename.service.gov.uk (they introduce a browser overhead that slows down the response time for users without providing any benefit for the service manager). Cookies MUST be sent with the Secure attribute and SHOULD, where appropriate, be sent with the HttpOnly attribute. These flags provide additional assurances about how cookies will be handled by browsers. GOV.UK is the place for users to find all government services, so it’s important to ensure that users always start on the relevant GOV.UK page, rather than a different or duplicate start page on www.servicename.service.gov.uk. As a result, services need to ask search engines not to index pages on their domains, so that the relevant GOV.UK page and the service domain don’t compete with each other in search engine results. This can be achieved by redirecting users to the relevant GOV.UK start page if they go directly to the service’s domain name, and by asking search engines not to index pages on the service’s domain name. Therefore, every service hosted on a service.gov.uk domain MUST: have a robots.txt file on the www, admin and assets subdomains asking search engines not to index any part of the site. Example content for robots.txt is given below, and more details can be found on The Web Robots Pages: have an HTTP 301 redirection from the top-level index page of the www and assets subdomains to the relevant start page on GOV.UK. (Note: this means that the service start page on GOV.UK SHOULD NOT link to the root of the www domain.) If you have contracted with CDN-based DDOS-protection suppliers then you should register these additional subdomains for use by your suppliers: Your suppliers will use these subdomains to address your www, admin and assets services. Detailed configuration advice for origin servers is outside of the scope of this document, but it’s important to ensure that these ‘origin domains’ only listen for traffic from trusted sources like: At present we advise against allowing DDOS protection suppliers to terminate SSL connections for transactional services carrying personal information, but this behaviour isn’t prohibited at present. Although SSL termination on the third party network would allow the supplier(s) to carry out additional analysis and potentially extra mitigations against certain types of attack, it would also give the supplier access to all the personal information being submitted to your service.  There are obvious downsides to allowing this level of access, especially if the supplier’s network and processes have not been accredited to the same level as the rest of the service. It’s a risk-based decision, but if in doubt we suggest a presumption against SSL termination on third party networks. Many suppliers offer IP forwarding DDOS protection, which does not have the same security issues as SSL termination, and is recommended in preference to SSL termination.  If your service requires transaction monitoring (which is not at all the same thing as DDOS protection) you should contact your CESG account manager for advice. Emails to users of your service SHOULD be sent from a human-monitored email address that originates from the domain servicename.service.gov.uk (and not the dept/agency or any other domain name). You SHOULD apply the Common Technology Services email blueprint on the sending domain, including implementing Domain-based Message Authentication, Reporting and Conformance (DMARC). If your service should need to wind down for any reason, you MUST ensure continued useful service and information for users by: For services that have been live for less than 6 months, you MUST continue to do the above for the remainder of a year total. For services that have been live longer than that you MUST continue to do the above for a further 12 months or until the expiry of the current SSL certificate, whichever comes first. The GOV.UK start page will be amended to explain that the service is no longer running, and cease to provide a start button pointing at the defunct service.","description":"Government offers a number of different digital services to citizens. While the start and end of a user’s journey will be on GOV.UK, the service itself will typically be hosted elsewhere, and will need a different domain name as a result. This page describes the use of service.gov.uk subdomains for hosting digital services.","link":"/service-manual/operations/operating-servicegovuk-subdomains.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Print forms","indexable_content":"Don’t make paper forms unless you have to Do as much as possible online Start from scratch The context of a form may be other forms (not the web) Digital to paper and back to digital again Test your forms with real users If you have to choose between hiring a writer and a designer, choose a writer Forms are not an alternative channel Templates and examples Download template form getting rid of any duplication pre-populating forms with information entered online making sure a user only prints the forms they require eliminating unnecessary steps and fields (if a field is marked as optional, consider whether you need to collect that information at all) numbering the forms so it was clear in which order they needed to be completed clearly indicating when questions are optional (‘skip this question if…’) improving the forms in response to user testing giving an overview of the application process at the start labelling pages and continuation sheets clearly explaining jargon inline add familiarity by showing some of the information the user has just entered in large font – their name, for example create a transcript of the information entered using the digital tool, with a signature box at the bottom. if your digital output has to match an existing paper form, you can still add a cover sheet with user’s input (for familiarity) clearly explain the next step: for example, give the address to send the form to, or give a web address to continue using the digital tool; if you give a URL, include a short ID string which logs the user in and takes them to the relevant step the digital and paper counterparts should have a similar look and feel InDesign CS6 (.indd) file InDesign CS4 (.idml) file PDF file (not editable) Occasionally, digital services will need paper forms at some stage in the transaction. Sometimes an application may require one or more ‘wet signatures’ to validate it, or print alternatives may be part of your assisted digital support. For the most part you can approach these as design challenges like any other – the Design Principles are a great way to start this work – but there are some specific things you’ll want to bear in mind. The most important thing to remember is that print forms should only be made when absolutely necessary. For example, the legislation governing Lasting Powers of Attorney (LPA) requires ‘wet signatures’ for the application to be valid. Until this legislation changes, a print form has to be part of the final service. Aim for as much of the transaction as possible to be completed online. Users should have to print as little as possible in order to complete an application, which means: In lots of cases, paper forms will exist for services being redesigned. The launch of the digital service gives you an opportunity to improve them. When doing so, don’t just base your designs on the existing forms. Look at what information you need to carry out the transaction and try to eliminate unnecessary steps.   Performance data should be available for the existing forms. This will tell you about the most common options people select and the kinds of mistakes they make. This will be useful information while you design the new forms. Links, smart answers, search boxes and drop downs are not available in print publications. If a transaction requires multiple forms you will need to make sure the design and layout helps users navigate them as easily as their online counterparts.  In the Lasting Power of Attorney application forms, this meant:   Your paper forms are a small but important part of a digital process. Typically you’ll be switching to paper forms to collect a signature. There are some ways you can make this transition easier: You should test your paper forms with real users. For complex forms, plan for 3 rounds of lab testing, making improvements to the form between each round. You’ll gain valuable insight by attending the user research in person. Get an introduction to user research and find out about all the different kinds of testing. Templates (based on the Lasting Power of Attorney application) created by teams across government will help you make easy-to-complete forms, but lots of the ‘heavy-lifting’ will be in making specialist terms and language accessible to the majority of users. Where specialist terms are unavoidable, or necessary, you’ll need people who understand how users read and how best to write for government services when working on the design. Skilled content designers and copywriters will make forms – whether online or in print – simpler and clearer, improving the completion rate and user satisfaction in the process. They complement the digital transaction and should be easy to process by staff supporting the digital service. Their performance also has an impact on the KPIs you gather for digital services. Contact us if you’re starting work on print forms. We can send you the LPA form pack to use as a template. (The redesigned LPA forms will be made public soon by the Ministry of Justice as part of a formal consultation.) If you get the error ‘There is no application set to open the document…’, you don’t have InDesign installed. You’ll need to install a trial or buy a subscription.","description":"Occasionally, digital services will need paper forms at some stage in the transaction. Sometimes an application may require one or more ‘wet signatures’ to validate it, or print alternatives may be part of your assisted digital support.","link":"/service-manual/user-centred-design/print-forms.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Privacy note template for services","indexable_content":"Your personal information What information [department name] holds about you How [department name] protects your personal information Sharing your information [Department name] staff handling your information Asking to see your personal information How to make a complaint What information DWP holds about you [example] Asking to see your information [example] [information 1] [information 2] your personal details, eg name, address, telephone number your financial details, eg bank account number your partner’s personal details your employment details [any other information that your department holds, give examples if the list is too long]  tell you why the information is needed, eg [examples] only ask for what’s needed  make sure nobody has access to it who shouldn’t tell you if the information is shared with other organisations and if you can say no to this – see the [full privacy notice] only keep the information for as long as it’s needed not make it available for commercial use, eg marketing, without your permission give accurate information tell [department name] about any changes, eg to your address Departments should base their customer-facing, plain English privacy notice on this template and add links to it from existing related statements, like their personal information charter and full privacy notice. This page explains what kind of personal information [department name] holds about you, how it’s protected, and how you can find out about it.  You can also read the full privacy notice. [Service action] + [department name] will, for example, hold the following information about you: When you apply for Carer’s Allowance, the Department for Work and Pensions (DWP) will, for example, hold the following information about you: [Department name] complies with the Data Protection Act 1998.  [Department name] will:  You must: Your information can be shared with other teams in [department name] or other organisations where this is needed to process your application. There are some cases when your information can be shared for other reasons, eg to prevent crime.  [Department name’s] staff are trained in handling information and understand how important it is to protect personal and other sensitive information.  You can ask to see the personal information [department name] holds about you. Fill in the [personal information request form] or write a letter to: [address] [Information about costs when making a request.] Sometimes [department name] can withhold information, eg to protect national security.  You can ask to see the personal information DWP holds about you. Fill in the personal request form or write a letter to: DWP Personal Information Requests Sample Street 1 London   There’s no charge.  Sometimes DWP can withhold information, eg to protect national security.  If you’re unhappy with the way [department name] handled your personal information, you can write to: [contact information] You’ll get a confirmation that [department name] has received your complaint within 5 days and a full answer within 20 days. [Department name] will tell you if there’s going to be a delay.  If you’re unhappy with the answer or need any advice, contact the Information Commissioner’s Office (ICO). The ICO can investigate your complaint and take action against anyone who has misused personal data.","description":"Departments should base their customer-facing, plain English privacy notice on this template and add links to it from existing related statements, like their personal information charter and full privacy notice.","link":"/service-manual/content-designers/privacy-note-template-for-services.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Progress indicators","indexable_content":"Progress indicators as navigation 1. Start without a progress indicator 2. If you do use one, keep it simple 3. Avoid complex progress indicators Further reading On this page: Start without a progress indicator If you do use one, keep it simple Avoid complex progress indicators they’re rarely used they take up valuable space they don’t scale well on small screens they can distract and confuse some people it’s hard to write good labels for the steps it’s hard to handle conditional questions Do Less: Problems as shared spaces. Ben Holliday talks about removing the progress indicator from the Carer’s Allowance service. Help people understand where they are in a transaction and give them the confidence to continue. Test your service first without any progress indicators at all. It may be simple enough that you don’t need them. If it isn’t, then at least you’ll discover the point at which people start to struggle. It’s often the order, type or number of questions that causes issues, so try improving these first. If people still have problems then try adding a simple progress indicator. If you do need a progress indicator, just add the step or question number at the top of the page, like this: Only include the total number if you can do so reliably. This approach is compact, accessible and usually enough to give people the confidence to continue. We recommend against these because: Complex progress indicators have been safely removed from a number of services on GOV.UK. For example, the Carer’s Allowance team removed a 12-step horizontal progress indicator from their live service, with no effect on completion rates or times. We occasionally see progress indicators (usually vertical ones) used as navigation. These suffer from many of the issues listed above. If people really need to be able to jump around the transaction, try meeting this need on a separate page. Discuss this page on Hackpad","description":"Help people understand where they are in a transaction and give them the confidence to continue.","link":"/service-manual/user-centred-design/resources/patterns/progress-indicators.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Question pages","indexable_content":"Guidance Accessibility 1. Back link 2. Page title 3. Form elements 4. Continue button back link page title form elements continue button use ‘Continue’, not ‘Next’ align the button to the left so it’s not missed by users  Split complex forms into individual question pages. Question pages consist of:  There’s a coded example of this page in the GOV.UK prototyping kit. For general advice on how to structure complex, multi-page forms read our form structure guide. Some users don’t trust browser back buttons when they’re entering data (often for good reason).  Back links reassure users that it’s possible to go back and change previous answers. It’s positioned at the top of the page so users don’t fill out the form, click back and lose data. Can be in question or statement format as long as you’re consistent throughout the transaction. The question format has the advantage that it’s more explicit about what the user should do. For more complex forms you may want to include a simple progress indicator above the title. Follow the GOV.UK form element styles.  For public facing services we recommend starting with one thing per page. All HTML forms should contain at least one fieldset and therefore also one legend. For simple question pages, the page title is often enough. You can hide the legend using the ‘visuallyhidden’ class. It will still be read out by screen reader users.","description":"Split complex forms into individual question pages.","link":"/service-manual/user-centred-design/resources/patterns/question-pages.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Releasing software","indexable_content":"Releasing software on demand Why government does this Regular releases reduce risk Further reading releasing regularly having the ability to release all the time your approach to testing the quality of low-level code – approaches like test-driven design and continuous integration (where code is tested constantly) can be helpful using the same tools and release processes for both the development and production environments - this way the software and tools will be well understood and will have been run thousands of times before the first public launch the people using your service don’t get new features and improvements quickly bundling up lots of new features makes the release more complicated releasing smaller chunks regularly makes it much easier to see what’s going to change, and if something goes wrong it’s much simpler to undo doing something regularly makes the case for investing in automation easier, removing much of the potential for human error and making each release the same if you’re doing something several times a day you tend to get better at it mean time between failures mean time to recovery Regular Releases Reduce Risk blog post about the approach to releasing software onto GOV.UK Constantly improving online services means releasing updates to the underlying software. How often you want to do this will affect how you design and build applications, and will present a number of challenges that this guide hopes to address. As early as possible in your product’s development, think about how you release updates to a running application. This is because it affects how software is developed and tested, and how your product may be supported. Being able to release software on demand is important. Release cycles of 6 months or longer are dangerous. Not only do new features rarely see the light of day, but known problems will have to be fixed within a rigid release schedule. It’s important to make the distinction between: Your application should always be in a state where it could be released, meaning quick changes can be made when needed. As an example, changes to the software running GOV.UK are made on average 5 times per day. To be able to do this, you have to consider: Although you need tools (potentially including commercial tools) to help with rapid releases, don’t start your discussions around what tools should be used or procured! Start discussions based on the needs of the service and the product team. In some organisations, people fear releasing new applications or new versions of software. Lots of websites, especially large applications within large traditional organisations, don’t change very often. Many will have fixed release schedules which might mean 1 release every 6 months or so. This means bundling up lots of changes into a single release, which is bad in at least 2 ways: It could be weeks or months before an improvement is actually released for people to use, even though it only took a few days to finish. This complexity means there are lots of different ways the release can go wrong. The combination of complexity, risk, and the infrequent nature of releases make for a stressful event for all involved. No wonder most people don’t like release day! Releasing software comes with risks, so trying to minimise those risks is a good idea. GDS does this in a number of ways, and the benefit of this is that: As well as reducing risk, being able to release early and often also helps products improve quickly, as it removes barriers to quick experiments and rapid iteration. Finally, consider the following 2 measures of a system: A very traditional approach involves working to reduce the time between any failures happening, hopefully improving the quality of the overall system. But problems will always happen at some point! Using your efforts to reduce the time taken to fix problems can often be much more cost effective, as well as improving the overall system uptime.","description":"Constantly improving online services means releasing updates to the underlying software. How often you want to do this will affect how you design and build applications, and will present a number of challenges that this guide hopes to address.","link":"/service-manual/making-software/release-strategies.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Remote usability/summative testing (quantitative)","indexable_content":"How remote usability testing works Where and how you might use it Weaknesses and when not to use Number and types of participants Examples produces results quickly is relatively cheap compared with conducting face to face user testing at scale produces statistically significant results removes moderator bias Remote usability testing takes place outside the lab with users participating in their own home, using their own computers, and with no third party moderator present. Each session usually includes tasks being given to see how users interact with the website/online service and are followed up with a series of questions about their perceptions and how easy it was to complete the tasks. As there is no moderator present, special software is used in order to record the user’s interactions with the website/online service. While traditional lab testing focuses of gathering rich and detailed information, remote usability testing aims to test with large numbers of users and produce statistically significant results. It is recommended that remote testing is not conducted in isolation, and face to face testing is also completed. Remote usability testing can be used to test both website content, and online services. Testing content normally involves people completing tasks based on the online content, while online services are normally tested by asking users to complete a task using the online tool or transaction (like claiming Jobseeker’s Allowance or applying for a driving licence). Success is measured on whether the user can complete the tasks or transaction. Benchmarks on new and existing products are gathered so that completion rates (and other success measures) be collected and performance monitored. The key advantage of remote usability testing are that it: Although remote usability provides testing with large numbers of people, findings can lack depth as they focus on what people do, and not why they have done it. When testing new products the tasks can seem artificial, and this is increased when testing is conducted with a panel of users, instead of real users on a live site. Remote usability testing should not be used in isolation and it is recommended that it is used in conjunction with face to face testing with real users. In order to test new products that are still in development it is necessary to engage an online panel in order to recruit participants. Existing products and services can be testing via the live website. It is recommended that 400 responses are collected so that results are robust. An example from the second round of Inside Government usability testing (PDF, 2MB)","description":"Remote usability testing takes place outside the lab with users participating in their own home, using their own computers, and with no third party moderator present.","link":"/service-manual/user-centred-design/user-research/remote-usability.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Retirement phase","indexable_content":"User needs Next stages Data Let your users know Plan to redirect traffic Make sure your subdomain continues to work Previous phase: live    When we retired the DirectGov, BusinessLink and departmental websites, we invested a lot of effort in mapping content on those sites to GOV.UK content. We redirected as many of the old sites’ URLs as we could to corresponding GOV.UK URLs, and provided ‘Gone’ pages with links to the National Archive where the content was not being transferred. We wrote about that in “No Link Left Behind”.  Even the best services may eventually reach retirement. Changes in policy may mean that the service is no longer offered or new understanding may mean that those user needs are better provided through a different service. Whatever the reason, the retirement of digital services should be handled with the same degree of care as their creation, concentrating on user needs. Your service will have been built to serve user needs. It’s vital to understand how those needs are to be addressed once your service has been retired, whether they are deemed to no longer exist, are no longer served by government, or will in future be served as part of another service. If the needs will no longer be met by government but will now be met by the private or voluntary sectors then it’s important to for these organisations to be adequately prepared. This means that the appropriate online user journey can be developed. Similarly, if the needs are to be served as part of another government service or services then you should identify which services they are and work with those services’ teams. This means the new teams can learn from your experience and you can understand how you’ll support your users to make the transition. The vast majority of users of your service will begin and end their journeys via GOV.UK. As soon as you know that your service is likely to be retired you should contact the GOV.UK team to make sure that those journeys are amended and appropriate information is supplied. The GOV.UK team will need to know why the service is being retired, and how those user needs are to be served in future so that they can provide the appropriate information, advice and links to users. For that majority of users who begin and end their journeys on GOV.UK the most important thing is to ensure that GOV.UK is updated. There will be some users, however, who access your service directly whether via links in emails, bookmarks on their computer or remembering your URLs. It’s important to prepare them for the change and lay out clearly what it will mean to them. Your planning should aim to produce the minimum possible disruption for users, but it will still be a significant change for them. Details of what the change is, why it’s being made, what they will need to do, and what will happen to their data should be made easily available. Users who access your service via an application programming interface (API) will need time to update their software to use the replacement service’s APIs or to make other relevant adjustments. You should reach out to your API users as early as possible and remember that they may have significant lead times for making and distributing changes. Changes to the service online will also need to be seamlessly tied in with messaging to offline users who are receiving the service through assisted digital channels. Once GOV.UK is updated the vast majority of users will begin to be directed to the new service. Some users will still try to access the service at its current (now retired) home. You should have a plan for redirecting those requests to the appropriate new service, or to provide clear information about the service that has been retired in perpetuity. When we retired the DirectGov, BusinessLink and departmental websites, we invested a lot of effort in mapping content on those sites to GOV.UK content. We redirected as many of the old sites’ URLs as we could to corresponding GOV.UK URLs, and provided ‘Gone’ pages with links to the National Archive where the content was not being transferred. We wrote about that in “No Link Left Behind”. If you operate a service.gov.uk subdomain, please read your technical responsibilities for ending your service. In running your service you will have accrued a large amount of data about the service and its users. You should already have policies in place to manage that data responsibly, including details of how long it will be retained. Those policies will continue to apply, and you will need to ensure that there is support in place to maintain them. Where data is being transferred to a new service owner, that should be done in accordance with your existing data protection policies, and communicated clearly to your users.","description":"Even the best services may eventually reach retirement. Changes in policy may mean that the service is no longer offered or new understanding may mean that those user needs are better provided through a different service.","link":"/service-manual/phases/retirement.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Same day user testing (online qualitative)","indexable_content":"How same day user testing works Where and how you might use it Weaknesses and when not to use Number and types of participants fits with an agile way of working produces quick results can be relatively cheap compared with conducting face to face user testing removes moderator bias Like standard remote usability testing, same day user testing also takes place outside the lab with users participating in their own home, using their own computers, and with no third party moderator present. Where standard remote usability testing focuses on large numbers, and providing statistically significant findings, this type of testing enables you to get rich, qualitative data from respondents that would normally only be possible by observing users in person. As there is no moderator present, each session is recorded via screen capture software, that records the whole session, including the user talking about what they are doing, why they are doing it, and how they feel about it etc. All interviews are conducted in one hour, with the video sessions, and the answers to the follow up questions made available immediately. Analysis is then conducted in house. Rapid 1:1 testing can be used to test both website content, and online services. Each session involves participants completing a series of online tasks to see how they use the website or tool to find information and/or complete transactions. Each participant is then asked a few follow up questions focusing on overall satisfaction with the site and the experience it provided. The key advantages of remote usability testing are that it: Although this type of remote usability testing can provide depth of content, the lack of moderation can result in respondents veering off topic and the analysis be quite lengthy. Also, respondents can sometimes appear to be ‘professional testers’, and therefore can perform tasks with greater ease than other users. Testing is normally conducted using the online panel that is available through the panel company. As with regular lab based testing, it is recommended that you test with 8-12 users.","description":"Like standard remote usability testing, same day user testing also takes place outside the lab with users participating in their own home, using their own computers, and with no third party moderator present.","link":"/service-manual/user-centred-design/user-research/same-day-user-testing.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Survey sampling methodologies","indexable_content":"What to consider when sampling Further reading When conducting quantitative research it is essential that all findings are statistically valid, so that there is confidence in the findings and inferences can be made from the sample to the population. This process of collecting information from a sample is referred to as ‘sampling’ and enables researchers to understand the views, and needs of a user base, without interviewing the whole user population. Consideration needs to be given to how the sample is collected (who, how, where, when) and the size of the sample collected for each study. A large sample size can be more accurate and provide greater confidence in the data collected, however a large sample is not required for all research projects. The sample method chosen should consider the size and scale of the project, the sub-group analysis required, and balance the robustness of the approach that with the time, money, and resource available to ensure its fit for purpose. Sampling can be a complex process to understand, but on most occasions a sample of 400-500 will be sufficient for most in-house studies – again this is dependent on the level of subgroup analysis needed and also the penetration of the target group in the population. The greater the number of population groups that need to be analysed may increase the sample size needed, as typically for findings to be statistically valid a minimum of 100 in a subgroup is required. A larger overall sample of 1000+ is normally required if findings need to be nationally representative. This guide to sampling (PDF, 885kb) was written by the National Audit Office, and although it was first published in 2001, still provides a helpful introduction to sampling methodologies.","description":"When conducting quantitative research it is essential that all findings are statistically valid, so that there is confidence in the findings and inferences can be made from the sample to the population.","link":"/service-manual/user-centred-design/user-research/sampling-methodologies.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Sass repositories","indexable_content":"Categories of Sass mixins Responsive design Cross browser Further reading GOV.UK typography, colours and image assets Mixins for responsive designs Mixins for targeting old versions of Internet Explorer Mixins for cross browser CSS Sass is a language for creating stylesheets that lets us share blocks of code and techniques. GDS has created a repository called the GOV.UK Frontend Toolkit to simplify the creation of services with a consistent look and feel. The toolkit is available as a gem for easy inclusion in Ruby projects. The files in the gem can be categorised into four main parts: The first part is the bit that gives all GOV.UK projects a familiar look. There are a collection of pre-defined font sizes that we use on GOV.UK. There is a mixin for each one, for example heading-26. These also include a standard amount of whitespace around the text to help with vertical rhythm on the page, spacing things out nicely. The second is a way to develop sites that are able to respond to different sized displays. The third is an easy way of writing Internet Explorer specific CSS in our stylesheets without using hacks. The fourth is a way to keep browser specific styles out of our projects. We encapsulate new or non-standardised CSS into mixins. In this way we can easily update all the instances of a new CSS property without having to do a search and replace across all our projects. It is generally advised to write your markup with a mobile first attitude. That is, add desktop styles to an otherwise narrow screen stylesheet. In this way you only add styles for desktop and don’t reset desktop styles for a mobile device. There are two main types of cross browser CSS that we are concerned with. Firstly using different techniques to achieve a consistent effect and secondly using vendor prefixes to apply consistent behaviour for newer features. For example: @extend %contain-floats uses a cross-browser technique to ensure that the element wraps all the floated elements within it. It is not a property that normally exists in CSS but is something we often need to do and don’t want to use different techniques everywhere. It gives us consistency across our code. @include border-radius is designed to use the different border radius implementations (-moz-border-radius, -webkit-border-radius etc) to create consistent presentation across different browsers. The README.md file in the GOV.UK Frontend Toolkit has more information.","description":"Sass is a language for creating stylesheets that lets us share blocks of code and techniques. GDS has created a repository called the GOV.UK Frontend Toolkit to simplify the creation of services with a consistent look and feel. The toolkit is available as a gem for easy inclusion in Ruby projects.","link":"/service-manual/user-centred-design/resources/sass-repositories.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Technology leadership","indexable_content":"Leadership People Future organisation design: maintaining agility and relevance digital public services mission IT systems infrastructure back office services Government has traditionally treated IT as a single entity with the same approach being used for very different types of service. Government IT is more appropriately mapped into distinct areas with different user needs — some are public facing, and some are for internal use. These areas include: These 4 areas are in very different stages of development. Digital public services, for example, are being built by developers using very new technologies. In contrast, technology infrastructure such as data storage has evolved over the course of decades. Products which 5 or 10 years ago would have required detailed and expensive customisation are now commoditised (packaged as ‘pay-as-you-go’) – to the extent where they have virtually become utility services like gas or electricity (you can move between suppliers easily). By using technology based on commodity products and services government can benefit from economies of scale and much greater efficiency. Improvements to digital and technology leadership will create a new way of working in government, supporting teams across the civil service in producing digital public services and better department-specific IT (or mission IT), and ensuring these teams have the right technology to support them. For this to happen, government digital and technology leadership should be strongly represented at board level and be specifically responsible for enabling business change. In many cases digital services and department-specific IT should – for now – be managed as separate but strongly linked workstreams, with a common aim of ensuring transformation across the department. In all cases where departments run digital services, digital should be represented at the same or higher seniority level as technology. The new roles of the Chief Digital Officer (CDO) and Chief Technology Officer (CTO) are for strategic, transformative leadership. CDOs will concentrate on producing digital public services and the wider digital by default strategy. CTOs will concentrate on providing the department-specific IT that supports digital public services and internal users. CTOs will also be in charge of moving their departments to common technology services. Both CDOs and CTOs will need to work out which areas can be commoditised and which need customised services. Both roles need to be getting the most out of new technologies, and will need to structure their teams accordingly. Models, structures and strategies are irrelevant without the right people in the right roles. The best way to restructure senior technology roles will vary across departments according to how much involvement they have across the 4 areas of government technology. In some cases both the CDO and CTO will sit on the board; in other cases they might report into a board level Chief Operating Officer (COO), or only the CDO might be on the board.  Where digital public services rely on department-specific IT systems to support them, a service manager role should be in place, reporting into the CDO with accountability and decision-making authority for the end-to-end service. In all organisations, roles that manage existing contracts, retiring legacy systems, and providing infrastructure services should be subordinate to leadership roles. GDS will provide support by establishing common infrastructure services so senior leaders in departments can concentrate on providing department specific technology for their users. GDS has also established a Recruitment Hub – an advisory service provided to departments to help them recruit talented technology leaders and digital specialists. As part of the Recruitment Hub service, GDS has already begun to work with departments and agencies to help with organisation design. Contact digitaltalent@digital.cabinet-office.gov.uk for more information. The CTO/CDO organisational model is not designed to provide a fixed structure for decades to come. It’s a model that reflects the current state of government technology with both large legacy estates and new digital services in development, and is designed to support the changes in technology use over the next 3 to 5 years. Over time the distinction between digital public services and department-specific IT will weaken. As legacy systems are retired and more technology is delivered through the cloud, it will make less sense to draw a separation between the two areas. It’s quite possible that in 5 years’ time both citizens and internal users will use the same digital services and technology. Organisation and governance structures – both within and across departments – will need to develop too. It will be part of the responsibility of senior digital and technology leaders to keep the effectiveness of these structures under review to ensure that they remain fit for purpose.","description":"Government has traditionally treated IT as a single entity with the same approach being used for very different types of service. Government IT is more appropriately mapped into distinct areas with different user needs — some are public facing, and some are for internal use. These areas include:","link":"/service-manual/the-team/recruitment/scs-orgdesign.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Security as enabler","indexable_content":"Security must be proportionate and justified User experience and security aren’t exclusive Commercial threats, commercial solutions Trust responsible users, audit and verify New classification policy Procurement Top 8 security myths 1. “Security says No!” 2. Accreditation of government systems is costly, time consuming, and doesn’t help secure them. 3. Open source software is more / less secure than proprietary code. 4. “Product X can’t be used because it isn’t accredited by CESG” 5. Restricted systems need to have bespoke security controls. 6. “I’m not a target for cyber attack!” 7. Impact Levels define security requirements. 8. “I’ve got lots of IL3 records, which aggregate to IL4, so I need to build a confidential back end system” Protecting information from valid threats to its confidentiality, integrity and availability is an enabler of digital services. Without such protection, digital services would be impossible or unsafe. Please note: this guide sets out the governing principles for developing efficiently secured digital services. It also busts some common myths around security. There is another guide about the practical process of security accreditation. Security must be applied intelligently. This means analysing the probable interest in official information from threat sources, establishing their capabilities and methods, and matching proportionate mitigations against these in a traceable manner. Other attempts at security run the risk of over-engineering security controls, or providing an illusion of security by not mitigating the actual risks. Technology alone is never capable of addressing security and privacy risks: there needs to be a risk model that spans technology, people (their behaviours and culture), and processes. Bad user experience arising from over-prescriptive use of technology can lead users to circumvent security controls by employing less secure unofficial IT solutions. Users who then retain the official IT systems suffer degraded productivity. The outcome is that security is not maintained and user experience is unpleasant. However, security and a good user experience do not need to be mutually exclusive. Modern, intelligently designed security can often be made largely transparent to the user, while also providing the enterprise with the confidence it needs that its information is suitably managed. Where some degradation of user experience is unavoidable, a risk management analysis must consider the negative impact of users avoiding unpleasant official IT and degraded productivity. Furthermore, a transparent understanding of the mapping between official information, threat sources and capabilities, through to the required mitigations, enables informed risk management when there is a change in environment or appetite for risk. The vast majority of government’s information will fall into the lowest ‘official tier’. This ‘official tier’ has been created on the basis of a ‘commercial threat model’. This means protection from the type of threats faced by a large company or bank, eg cyber criminals or hacktivists intent on stealing personal or financial information, or disrupting services. To defend against threats such as these, government will use only the very best security technology sourced from the commercial market, and there will be no need for any bespoke or government-only controls at this level. A small set of software will perform security enforcing functions, such as firewalling or encrypting data. Such products require assurance that they function as advertised, achieved through the CESG Commercial Product Assurance (CPA) scheme. This scheme is lightweight compared to previous schemes, reflecting the commercial grade risks for OFFICIAL information. It’s the intention of the Civil Service Reform Plan and the new Security Classification Policy that there’s greater emphasis on user responsibility, reducing expensive and overbearing technical controls. This requires proper training to assist users in handling sensitive information, and auditing to verify users are acting responsibly. Users should be trusted to carry out their roles and given the responsibility to do so securely. Audit and verification of user behaviour should be used to ensure policy compliance instead of preventative measures which add cost and degrade productivity. Such audit and verification should be implemented by services or network infrastructure, away from the end user device. Government should not invest in security controls to protect users from risks they can protect themselves from. Departments should, however, invest in security controls that help defend individual users against threats that they themselves cannot reasonably defend. For example, government should not invest in special technology to prevent civil servants working on sensitive information in an open busy public place; users should be able to judge, assess and use appropriate risk mitigation approaches by themselves. Likewise, civil servants should be able to exercise reasonable judgement about what information is sent to external recipients by email over the public internet. This will lead to reduced technical controls and their associated costs, while also optimising the usability and flexibility of the IT tools for the majority of responsible users. The new security policy provides for a simpler and more meaningful approach to denoting the value of information assets and threat model. Together with a set of common controls for OFFICIAL information, it enables a more consistent, standardised, reusable and interoperable approach to securing information assets in government. There should no longer be situations where information cannot be shared between bodies solely because their interpretation of the same security guidance has led to incompatible controls. The Cabinet Office Security Classification Policy provides guidance as to which information falls into the ‘official’ category. It’s important to start thinking about security from the very start of an IT procurement project, as bolting it on later invariably introduces additional cost and delays. In fact, getting appropriate security into your system needs to start with specifying security requirements correctly in the contract. When we don’t adequately articulate security requirements for design and ongoing maintenance of a system, our suppliers price for the level of contractual risk the ambiguity introduces. Specifying security requirements of the form “must be accreditable for IL3” must become a thing of the past. The risk appetite of departments varies and suppliers not used to working with government will struggle to find out what that means for the design and maintenance of their system – adding cost and risk to both parties. Instead, when writing IT contracts, departments should think about what they care about from an information assurance perspective and specify requirements that actually manage their concerns. As well as getting a solution that address the real business needs, this approach also allows industry to innovate to solve the security problem. It is often said that security is the reason something can’t be done. This is very rarely the case in practice – rather, security is being (mis)used as a handy excuse to not do something. Good information risk management practices allow organisations to understand the risks they are taking with their information assets, and work out the most effective ways to manage and control those risks – while not hindering business. If someone tells you that “Security says No”, or that “CESG say No”, you should ask for more information to learn what the risks actually are, and what techniques and tools are available to help manage those risks. Accreditation effort should always be proportional to the complexity, threat and impact of a system. It is vital that the effort spent should scale to match the challenge at hand. Can the business accept the risks of undertaking a given activity? Accreditation documentation only needs to contain the information needed to enable this decision. Clearly, any accreditation activity which just generates documentation that’s never read is not adding value, nor helping to secure information. When performed well, accreditation helps risk owners have confidence that all aspects of a system’s security have been appropriately considered, and that proper through-life security processes are in place to maintain information security. Experience has shown that the licensing model for software is not an accurate gauge for the security of the finished item. There are very securely developed open source projects and proprietary products. The opposite is also true. The experience and competence of those developing code is of primary importance. The same considerations should be given to the security of open source products as are applied to proprietary code. The Open Source toolkit has more information. There are 3 problems with this statement. Firstly, CESG don’t accredit products – they provide assurance in the security properties of products and certify them when they meet various standards. Secondly, only those products which provide a security enforcing function need to be evaluated and certified by CESG; products like switches or email servers don’t need to be CESG-certified. Finally, CESG certification of a product is a component of risk management – it doesn’t absolve an organisation of their responsibilities to information security. While using certified products should certainly speed up the risk management process, it isn’t mandatory. Restricted systems can be built today using assured commodity security products – many of which are found natively in the operating systems of modern platforms. In the past, some of the security controls which were recommended for protecting Restricted IT systems were bespoke variants of commodity products, or were “government specials”. The requirements which drove these choices are no longer relevant given advances in commodity IT, and better approaches are available to provide an equivalent level of security, but with enhanced usability and overall cost of ownership. Foundation grade assured products are appropriate for the protection of these systems, and commodity security products (including open source) can achieve this grade of certification. Sometimes it’s obvious that attackers are going to be interested in getting hold of your sensitive information – it has clear value to someone. However, immediate financial reward isn’t the only reason government IT systems get attacked. Attackers may be disgruntled insiders, seeking to disrupt or embarrass an organisation. Attacks may come from protest groups, seeking a platform for their views. Or attacks may seek to use one system as a point to launch attacks on another – so you might not be the primary target, but might unwittingly be a conduit to them. Are you specifically being targeted? Maybe not – but your systems and internet presence probably are being attacked on a daily basis for a wide range of reasons. Asking a someone to build an “IL3 system” or to “protect at IL5” is a misuse of the Business Impact Level framework, and will generally cause confusion and an ineffective approach to securing information. Business Impact Levels are intended to help organisations to think about the consequences of certain events, and thus steer where effort should be used when managing information risks. There is no standard set of technical, procedural or personnel controls for any particular Impact Level, and so specifying requirements in terms of Impact Levels is unclear and inaccurate. Instead, security requirements should be defined in terms of the level of protection that each information asset should be given, and expectations around its through-life protection. It’s true that if you are building a system which handles many records, the impact of loss of many of them is likely to be higher than loss of one or two. Although Business Impact Levels are often misused as shorthand for protective markings, there is no direct mapping from Impact Level to protective marking. In the IL3 aggregated to IL4 example, rather than building a system capable of storing confidential records, it’s likely to be acceptable to simply add some simple additional controls around the data store, to reduce the risk of a bulk loss of data.","description":"Protecting information from valid threats to its confidentiality, integrity and availability is an enabler of digital services. Without such protection, digital services would be impossible or unsafe.","link":"/service-manual/technology/security-as-enabler.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Self assurance by agile teams","indexable_content":"How Agile approaches reduce risk How a phase based approach reduces risk Agile processes for assurance Assurance at scale      you can test what you’ve produced to see how well it’s meeting user needs — this is the best form of assurance         there are regular opportunities to adjust or stop the work as you progress and as the results of testing become known     a culture of trust develops through transparency problems are less likely to be hidden blockers and dependencies are easy to see      early phases are about experimenting, learning and prototyping to decrease uncertainty about what to build and how to build it         there’s an early and continuous focus on meeting user needs — this makes sure the right thing gets built         an early focus on investigating major risks and dependencies reduces the chances of them later affecting delivery         early phases have bounded time and cost but variable scope — this forces everyone to focus on the most important areas to be investigated          there are opportunities to try out different assurance, reporting and governance approaches and team composition to see what works and what doesn’t before things scale up — this helps set the project up for success    daily (for the coming day) every few weeks within a phase (for the next few weeks) at the end of each phase (for the next phase) there’s consensus around user needs, goals and priorities you’re focusing on the most valuable things to meet user needs and satisfy goals you have the people and resources you need to do the work risks, blockers and dependencies are being managed teams are continually improving how they do their work form an opinion of whether the work is likely to be successful see progress towards meeting user needs  understand the things blocking the team retrospectives — to understand what the team thinks is and isn’t going well (this helps continuous improvement of how the team works) daily stand ups — for making sure that people are doing the right things, working well together and communicating Assurance is built into Agile ways of working with regular checkpoints and opportunities for feedback during service development. This means that assurance is proactive, ongoing and helps keep the service on track. The governance principles for digital services outline how to keep governance work on track in a similar way. A phase based approach to service development helps to increase the chances of success with in built assurance throughout each stage. Assurance should support delivery (only do something if it adds value) and be proportionate to each phase. Agile, user centred delivery is incremental, with delivery occurring every few weeks. In the early phases, you’ll be creating prototypes, and you’ll be delivering a live service for testing in the later phases. Regular delivery means that: you can test what you’ve produced to see how well it’s meeting user needs — this is the best form of assurance there are regular opportunities to adjust or stop the work as you progress and as the results of testing become known  Visual management is built into Agile approaches. This supports ongoing assurance because progress is visible to anyone. It also means that: People who govern need to go and see things for themselves to judge progress. Doing this regularly is part of assurance — it ensures there won’t be any surprises. This is the best way to understand how user needs are being met and to be reassured that delivery will be successful. At the beginning of a service delivery, you have most to learn. Using a phase based delivery approach significantly reduces risk because: early phases are about experimenting, learning and prototyping to decrease uncertainty about what to build and how to build it there’s an early and continuous focus on meeting user needs — this makes sure the right thing gets built an early focus on investigating major risks and dependencies reduces the chances of them later affecting delivery early phases have bounded time and cost but variable scope — this forces everyone to focus on the most important areas to be investigated  there are opportunities to try out different assurance, reporting and governance approaches and team composition to see what works and what doesn’t before things scale up — this helps set the project up for success It’s also important to assure governance as well as development, particularly early in the life of the service — this is where the pattern of governance is formed and it’s vital to get it right. Agile approaches have a set of processes and ‘assets’ (eg data produced by teams, daily stand up meetings, walls displaying information). You can use these for assurance — there’s no need to ask for anything in addition to this — this just creates extra work and doesn’t add value. When using Agile methods, planning happens: This regular planning gives assurance that: Regular show and tells allow delivery teams to demonstrate their progress. They also allow people who govern to go and see what’s being delivered for themselves. This is another useful form of assurance because they can: Assurers can also see governance in action at show and tells and so assess its effectiveness. Other Agile processes useful for assurance are: When there are multiple teams working on a service — or a set of related services — Agile ways of working support assurance. Activities like cross-team show and tells and the ‘stand up of stand ups’ help multiple teams communicate to make sure dependencies, risks and opportunities are being managed. More on assurance for digital services Assurance for digital services Assurance from those outside the service team Get involved To give feedback, make a suggestion or share your experience, use the governance guidance hackpad.","description":"Assurance is built into Agile ways of working with regular checkpoints and opportunities for feedback during service development. This means that assurance is proactive, ongoing and helps keep the service on track. The governance principles for digital services outline how to keep governance work on track in a similar way.","link":"/service-manual/governance/self-assurance-by-agile-teams.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Sentiment analysis","indexable_content":"How sentiment analysis works Weaknesses and when not to be used Sentiment analysis is a method used to analyse high volumes of verbatim comments from users in order to help easily understand the attitude and tone of users’ comment. The method uses tailored software to analyse user comments and structure them in a manner that can be used to understand what user ‘feel’ towards a product or service. This enables positive and negative comments to be grouped so that actions can be assigned to resolve problems and issues raised by users. Some of the free tools available can group comments very broadly, and not enable the level of granularity that enables the analysis to be useful and actionable. Furthermore, comments can sometimes be analysed incorrectly. This is especially the case when slang is used, or phrases are not meant literally (when negative words and phrases are meant positively, or positive comment are meant ironically). This is a notoriously difficult technique for anything beyond broad statements, and requires a very large sample size both of seed (already analysed) material and of commentary from each user. It’s not much use on twitter comments, for example, because they’re so short.","description":"Sentiment analysis is a method used to analyse high volumes of verbatim comments from users in order to help easily understand the attitude and tone of users’ comment.","link":"/service-manual/user-centred-design/user-research/sentiment-analysis.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Service integration and management","indexable_content":"Service integration today What service integration looks like Involve everyone in a clear process being able to define different service requirements for critical and non-critical services (for example, some commodity public cloud services may require online service support or service desk only, whereas mission critical IT systems will require a more integrated service model) a performance regime that ensures organisations don’t pay for services they can’t or don’t use explicit service integration arrangements that focus on service performance, usability and availability from a user perspective, not just from a supplier’s commercial perspective skills and capabilities that support transitioning to, and managing services in a new commodity-based environment a focus on open standards and interoperability to support workflow, performance management and service management, billing and payment Service integration and management lets an organisation manage the service providers in a consistent and efficient way, making sure that performance across a portfolio of multi-sourced goods and services meets user needs. Service integration models have been around for some time, but are now evolving from the challenges of managing a small number of large suppliers (typically systems integrators) to a model of managing a greater number of smaller suppliers, often providing commodity services. For the model to be effective the component services need to be well defined and understood. It’s important to avoid any ambiguity about the boundaries of both responsibilities and accountabilities. Key features of an effective model are: The level of service integration will differ depending on the complexity of the business services and/or customers that are being supported, and the complexity of the services that are being delivered to those businesses. As the services and businesses become more critical or complex, the level of service integration becomes deeper.  The design of the service integration function will differ by department. It may be completely operated in-house. Or it might consist of a thin in-house capability ultimately responsible for the integrated end to end operation and management of quality IT services, underpinned by outsourced integration services for specific elements – for example performance monitoring, service desk, or service level reporting. The G-Cloud framework offers a number of services to support service integration. Particularly for smaller departments and simple services, care needs to be taken not to over-engineer the service integration approach – effective use of commodity standards-based IT should mean that integration and support requirements are much less onerous than managing a locked-down bespoke system. As part of service integration, you should maintain an accurate service catalogue including a service dependency map, so that you can effectively manage changes that are high risk, high impact, or that can affect multiple suppliers. When buying in services you should retain the contractual authority to ensure suppliers follow your service integration processes – so that you can ensure the integrity and availability of a department’s user services. This could include an end-to-end service performance incentive model so all suppliers collectively share in the benefits and penalties of a joint performance management regime.","description":"Service integration and management lets an organisation manage the service providers in a consistent and efficient way, making sure that performance across a portfolio of multi-sourced goods and services meets user needs.","link":"/service-manual/technology/service-integration.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Service management","indexable_content":"Agile and service management ITIL The importance of implementation Other frameworks exist Problematic concepts Further reading Service Strategy Service Design Service Transition Service Operation Continual Service Improvement Functions An example – service transition Projects Business as usual Individuals and interactions over processes and tools Working software over comprehensive documentation Customer collaboration over contract negotiation Responding to change over following a plan IT service management Service portfolio management Financial management for IT services Demand management Business relationship management Design coordination Service Catalogue management Service level management Availability management Capacity Management IT service continuity management Information security management system Supplier management Transition planning and support Change management Service asset and configuration management Release and deployment management Service validation and testing Change evaluation Knowledge management Event management Incident management Request fulfillment Problem management Identity management Service operations Technical management Application management Operations management ISO/IEC 20000 Microsoft Operations Framework Functions in service management ITIL on Wikipedia Introduction to ITIL v3 Discussion of DevOps and ITIL Discussion of ITIL and Continuous Delivery    Put in place a sustainable multidisciplinary team that can design, build and operate the service, led by a suitably skilled and senior service manager with decision-making responsibility.     That is, while there is value in the items on the right, we value the items on the left more.     A service is a means of delivering value to customers by facilitating outcomes customers want to achieve.     Make sure that you have the capacity and technical flexibility to update and improve the service on a very frequent basis.     continuously update and improve the service on the basis of user feedback, performance data, changes to best practice and service demand  The Digital by Default standard says that organisations should (emphasis on operate added): Put in place a sustainable multidisciplinary team that can design, build and operate the service, led by a suitably skilled and senior service manager with decision-making responsibility. This implies a change to how many organisations have traditionally run services, often with a team or organisation building a service separate from the one running it. This change however does not mean ignoring existing good practice when it comes to service management. The principles of IT service management (ITSM) and those of agile do not necessarily conflict – issues can arise however when organisations implement rigid processes without considering wider service delivery matters, or design and build services without thinking about how they will be operated. The agile manifesto makes the case for: It is too easy to position service management as opposed to agile as traditional service management practices can be viewed as focusing on processes, tools, documentation, planning and contract negotiation – the items on the right hand side of the points above. However, the agile manifesto goes on to say: That is, while there is value in the items on the right, we value the items on the left more. To build and run a successful service you will need to work on suitable processes and manage third party relationships. Using existing service management frameworks (especially as a starting point) is one approach to this problem. ITIL (the Information Technology Infrastructure Library) is one such framework. ITIL does a particularly good job of facilitating shared language. For instance it’s definition of a service is: A service is a means of delivering value to customers by facilitating outcomes customers want to achieve. The current version of ITIL currently provides 5 volumes and 26 processes describing in detail various aspects of service management: ITIL also describes four functions that should cooperate together to form an effective service management capability. The above processes and functions make for an excellent high level list of topics to discuss when establishing an operating model for your service, whether or not you adopt the formal methods. In many cases if you have well understood, well established and well documented processes in place for all of the above you should be in a good position to run your service. When looking to combine much of the rest of guidance on the service manual with ITIL or other service management frameworks it is important to challenge existing implementations. This is less about the actual implementation and more often about the problems that implementation was designed to originally solve. As an example ITIL talks a great deal about Service Transition – getting working functionality into the hands of the users of the service. This is a key topic for The Digital Service Standard too which says that teams should: Make sure that you have the capacity and technical flexibility to update and improve the service on a very frequent basis. GOV.UK for instance made more than 100 production releases during its first two weeks after launch. This high rate of change tends to challenge existing processes designed for a slower rate of change. If you are releasing your service every month or every 6 months then a manual process (like a weekly or monthly in-person change approval board or CAB) may be the most suitable approach. If you’re releasing many times a day then the approach to how change is evaluated, tested and managed tends towards being more automated. This moves effort from occasional but manual activities to upfront design and automation work. More work is put in to assure the processes rather than putting all the effort into assuring a specific transition. Service management frameworks tend to acknowledge this, for instance ITIL has a concept of a standard change (something commonly done, with known risks and hence pre-approved), but a specific implementation in a given organisation might not. It is important to note that other service management frameworks and standards exist, including some that are of a similar size and scope to ITIL: Many organisations also use smaller processes and integrate them together. The needs of your service and organisation will determine what works best for you. Some traditional language tends to cause some confusion when discussing service management alongside agile. It’s generally best to avoid the following terms when possible, although given their widespread usage this isn’t always possible. It is however worth being aware of the problems these concepts raise. Projects tend to imply a start and an end. The main goal of project work is to complete it, to reach the end. Especially for software development the project can too often be viewed as done when the software is released. What happens after that is another problem entirely – and often someone else’s problem. However when building services the main goal is to meet user needs. These needs may change over time, and are only met by software that is running in production and available to those users. This doesn’t mean not breaking work down into easily understandable parts, but stories, sprints and epics are much more suited to agile service delivery. The concept of business as usual also clashes with a model of continuous service improvement. It immediately brings to mind before and after states, often with the assumption that change is both much slower and more constrained during business as usual. In reality, until you put your service in the hands of real users as part of an alpha or beta you won’t have all the information needed to build the correct service. And even once you pass the live standard you will be expected to: continuously update and improve the service on the basis of user feedback, performance data, changes to best practice and service demand","description":"The Digital by Default standard\nsays that organisations should (emphasis on operate added):","link":"/service-manual/operations/service-management.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Service managers","indexable_content":"Responsibilities Sample job description Learning and development Guidance Further reading What’s the difference between a service manager and a product manager? be experienced leaders, with an in-depth understanding of their service (built on continuity of involvement over a period of years) and equipped to represent their service and its users’ needs at all levels within the organisation. For high-profile services these will be at Senior Civil Service level be accountable for the quality and usage of their service, and able to iterate the service based on user feedback at least every month be able to lead effectively on the change management and process re-engineering required to implement successful services have the digital literacy to engage with technical staff and suppliers to define the best system and platform configurations to achieve business/user objectives encourage the maximum possible take-up of their digital service by effective marketing, and specify/manage the requirements for assisted digital activity to supplement this oversee service redesign and subsequent operational delivery; supporting and ensuring the necessary project and approval processes are followed, monitoring and reporting on progress in line with the Digital by Default Service Standard, identifying and mitigating risks, and be authorised to deliver on all aspects actively participate in networking with other service managers inside and outside government, and share good practice and learning Building a team Being a service manager – presentation by a Service Manager from the Office of the Public Guardian Service managers are business people who work full time to develop and deliver an effective user focused digital service, including all related processes, for which they are responsible and accountable. Outside government, organisations in the public and private sector are learning that experienced and highly skilled managers (often called product managers in the commercial world) are necessary to deliver high-quality digital services. We are adopting that model, requiring each transactional digital service handling over 100,000 transactions each year to be developed, operated and continually improved by a service manager. These are not technical IT posts, nor are they confined to running a website. Instead, they are individuals who work full-time to develop and deliver all the changes necessary to provide effective digital services. With a handful of exceptions, this is a new role within government. These service managers will: This will depend on the scale of the service you are working on.  In some cases the service manager will also be able to fulfil the role of product manager – working closely with the delivery team (the ‘makers’), prioritising stories for each sprint, attending daily stand ups, being on hand to comment on solutions as they emerge, and accepting stories once they are delivered. However, in many cases it is likely that the service manager won’t have the capacity to be this hands-on, so they are likely to need a dedicated product manager. Click either of the options below to download a template service manager job description. Download as OpenDocument Format / Download as MS Word doc Cabinet Office will help departments to recruit suitably skilled individuals through the Recruitment Hub. Newly appointed service managers are supported by GDS through specialist learning and development. Read guidance within the manual of particular interest to service managers. Read more about how service managers should interact with other technology leaders in our organisation design guidance.","description":"Service managers are business people who work full time to develop and deliver an effective user focused digital service, including all related processes, for which they are responsible and accountable.","link":"/service-manual/the-team/service-manager.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Service subdomain names","indexable_content":"Subdomain names and certificates *.preview.servicename.service.gov.uk *.staging.servicename.service.gov.uk *.servicename.service.gov.uk being able to do HTTP 1.1 virtual hosting for HTTPS without relying on Server Name Indication (SNI). This is important because of legacy software which does not support SNI. It may not be possible to enumerate all of the names that will be served from a domain, so by allowing wildcards, we can meet that need. (You may have the public-facing www, admin and assets, but also non-public logging, monitoring and other services that still require TLS) having a different certificate for preview versus staging versus production means that we can potentially restrict who has access to the certificate. This is better from an operational security perspective. We should not use the same certificate on production as in development/test environments. When you are operating a service at servicename.service.gov.uk, you will need testing and development environments. Given that these must use HTTPS, we suggest purchasing wildcard certificates with this naming convention: The reasons why we prefer wildcard certificates include:","description":"When you are operating a service at servicename.service.gov.uk, you will need\ntesting and development environments. Given that these\nmust use HTTPS, we suggest\npurchasing wildcard certificates with this naming convention:","link":"/service-manual/domain-names/service-subdomain-names.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: What your service should look like","indexable_content":"Designing your service Using the GOV.UK logo Typography Pictures and icons Header and footer Alpha Beta Live The service standard states that you should build your service to be “consistent with the user experience of the rest of GOV.UK by using the design patterns.” The first place to start is by reading our design principles. These will help you to understand how we approach design. You should keep these in mind when designing and building your service, always thinking about how to make the service simpler, clearer and faster. At alpha stage, the important thing is to make something and make it quickly, before testing it with real users. You should follow the design principles and should spend your time working on the user journey, interface and experience, but you don’t need to worry about making everything pixel perfect in your prototype. At beta the main priority is creating a simpler, clearer, faster service. You should concentrate on making the user journey and experience as simple and clear as possible. However, you should  be moving towards a finalised look and feel, how fast you achieve this depends on the size of your design team and how much time you can dedicate to it. Before your service goes live it must look and feel like GOV.UK.  You should be using the correct typography, header and footer and if necessary should have followed the guidance on pictures and icons. Throughout the process of creating your service you should refer to the design principles, think of the user and design for simplicity. The user experience across all services on GOV.UK should be consistent. Your service is on GOV.UK if it’s either at www.gov.uk/myservice, myservice.service.gov.uk or myblog.blog.gov.uk. At the point of launch it should look like GOV.UK and should have the GOV.UK logotype, including the crown. You can still follow the guidance if your service isn’t on GOV.UK, but your site shouldn’t identify as being part of GOV.UK and shouldn’t use the crown or the GOV.UK logotype in the header. This blog post talks about a good example of a site that follows our design patterns without using the logo. Services should use clear, easy to read type, with consistent styles and a clear hierarchy of information. You’ll be given access to the New Transport typeface if your service is on GOV.UK. The GOV.UK elements page has more information about typography. In most cases your service will not need to use pictures. You should only use icons if they are clear, simple and help to convey information more clearly than text alone could. You should use the standard template for your header and footer if your service is on GOV.UK. Read our guidance on headers and footers","description":"The service standard states that you should build your service to be “consistent with the user experience of the rest of GOV.UK by using the design patterns.”","link":"/service-manual/user-centred-design/service-user-experience.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Getting a domain name and start/end page","indexable_content":"Getting a domain name and start/end page Service name, eg {service-name}.service.gov.uk DNS servers to delegate to (ask your technical team to see the guidance on DNS delegation) Date you need it by (at least 5 working days’ notice) To make sure that the right user journey (appropriate start/end pages, clear domain names) are set up for a new service it’s important that you engage with the GOV.UK team within GDS early. You can start the service domain name process by emailing gdsapprovals@digital.cabinet-office.gov.uk who will discuss with you the best name for your Service Domain and the start pages on GOV.UK. While you are waiting for this process, you should start looking at where you will host the DNS, as GDS will delegate control of your Service Domain to you. You should read more about delegating DNS for Service Domains here which will prepare you for the next step. Once your service has an agreed name, you will need to supply the following information to your GDS contact who will put you in touch with the GDS Infrastructure Team to arrange delegation. If you are intending to use the “Managed DNS” product offered by Dyn, then you will need to give as much notice as possible, as Dyn is currently used to manage the main service.gov.uk DNS domain. Dyn requires a signed letter of authorisation from GDS and work on their systems in order to manage sub-zones of service.gov.uk via other customer accounts – this may take additional time to arrange.","description":"To make sure that the right user journey (appropriate start/end pages, clear domain names) are\nset up for a new service it’s important that you engage with the GOV.UK team within GDS early.","link":"/service-manual/domain-names/setting-up.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Shared asset libraries","indexable_content":"Why we do this Where to find our code govuk_template govuk_frontend_toolkit Yahoo recommends minimising HTTP requests to improve performance All used libraries are kept at known versions, which guarantees compatibility and reduces the risk of security vulnerabilities through external server compromise govuk_template govuk_frontend_toolkit Shared asset libraries are helpful for using the same frontend and branding on multiple services. When building services around patterns and consistency, it’s important to share your frontend assets so that they can be easily reused as required. There are two more benefits to this approach: The templates on GOV.UK are constantly changing as we react to user feedback and evolving best practice, so the best place to find them is by looking at the code we publish: The GOV.UK template is a project that provides the GOV.UK header and footer, as well as associated assets. It generates a variety of output formats and you can extend it with more depending on the language your service is written in. GOV.UK’s static consumes the Ruby version of the template application, which in turn provides shared resources like footer links across the various frontend apps that run GOV.UK. The Performance Platform’s Node.js application uses the Mustache version of the template. You should be able to become a consumer of the template in exactly the same way these two projects are, by adding it to your application’s dependencies. The service manual article on Sass repositories has more information on this repository. GDS is continuously improving GOV.UK, which means that template and asset code changes regularly. All services on GOV.UK are expected to keep their templates and assets updated. How you do this will depend on how your frontend is implemented, but if you include the template and the toolkit as dependencies in your application it should be relatively easy to update as GDS publishes new versions. Please contact the GDS team for help or advice.","description":"Shared asset libraries are helpful for using the same frontend and\nbranding on multiple services.","link":"/service-manual/user-centred-design/resources/shared-asset-libraries.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Standalone mobile apps","indexable_content":"The government’s position on apps Types of apps Reasons for government’s position Exceptions Further reading Native apps Responsive web design Setting out your proposal The process device-specific ‘download and install’ apps (aka native apps) websites that respond to various screen sizes (aka responsive websites, web apps or HTML5) various combinations of the two generate revenue (download and buy) have a constant presence on device can access all functions on a device better performance in general can be used offline, in some cases expensive to develop and maintain needs several different versions (Android, iPhone, iPad, Blackberry etc) service iteration is more complex (at least 3 types of app to deploy) can only be downloaded via gatekeeper app stores (Apple, Google) most apps are rarely downloaded, and even then hardly used it’s your website, so costs are minimised and service iteration simplified uses open standards (HTML5) no gatekeepers to restrict access performance is maintained mobile web exceeding the reach of mobile apps clear winning strategy for ‘utility’ services which do not require complex device features or persistence not continuously on device some device features unavailable (camera, address book) requires internet connection not snappy enough for some complex services (eg Spotify, Facebook, Skype) no ‘download and buy’ revenue stream condition 1 – your web service is already designed to be responsive condition 2 – the service or the content you’re looking to build an app for is already open to third-parties via APIs or as open data what’s the user need? which third-party native/hybrid apps have already been developed to meet this user need?            if there are none and condition 2 has been met, please provide your thoughts on why this might be the case       if there are third-party alternatives, please state why you believe a government-developed app is required         if there are none and condition 2 has been met, please provide your thoughts on why this might be the case if there are third-party alternatives, please state why you believe a government-developed app is required is this user need of sufficient importance to (your users to) justify the lifetime cost of your proposed app?            if you believe it is, how have you determined this? You might find it useful to review articles within the service manual such as, know your users and writing user stories         if you believe it is, how have you determined this? You might find it useful to review articles within the service manual such as, know your users and writing user stories is there evidence of demand for this type of app amongst your target users? (provide supporting evidence, eg similar apps that have proven popular with your target audience and evidence of their popularity) is there evidence to justify building an app for the platform you’re proposing to do this for? (please provide supporting evidence, eg analytics data that shows proportion of visitors to your content/service that currently access it using relevant devices) Tom Loosemoore’s blog post about standalone apps    “Standalone mobile apps will only be considered once the core web service works well on mobile devices, and if specifically agreed with the Cabinet Office”  The government’s position is that native apps are currently rarely justified. At the October 2012 Digital Leaders meeting, the position was clarified: native apps could not be developed without Cabinet Office approval. The November 2012 Digital Strategy says: “Standalone mobile apps will only be considered once the core web service works well on mobile devices, and if specifically agreed with the Cabinet Office” Making sure your service meets the Digital by Default Service Standard means it will work well on mobile devices. Making your data and application programming interface (API) available for reuse will stimulate the market if there is real demand for native apps. So-called ‘apps’ come in several different forms, which makes confusion understandable. When people are talking about ‘apps’, they can mean: Native apps are downloadable software applications that run using the device’s operating system code and APIs. Native apps remain on the device and can access all the hardware features (camera, storage, phone capabilities etc). Because they run using native code, different versions must be created for each operating system. Examples of native apps include Spotify, Angry Birds, Instagram, and Skype. Pros of native apps: Cons of native apps: Responsive web design is a design approach that optimises users’ viewing experiences across a wide range of devices. When a responsive website is accessed via a mobile phone, it’s sometimes referred to as a ‘mobile web app’. Responsive websites are built using open web standards (HTML, CSS, JavaScript etc) and they run inside a device’s web browser. Examples include GOV.UK, PM’s dashboard, Financial Times app, and BBC Sport. Pros of responsive web design: Cons of responsive web design: If there’s a market for native apps, why should government monopolise it? There’s a vibrant market of third party native app developers using government data and APIs. The government’s position is that native and hybrid apps are currently rarely justified. GDS back open standards – this removes the risk of having to create different, yet similar, versions of a service so an app is compatible with new devices that enter the market. While people spend as much time using apps as using mobile web, the vast majority of app use is for gaming and social networking. For ‘utility’ needs, such as those met by government services, the mobile web is preferred to native apps. GDS recognise that there’ll be a few exceptions. To help you assess whether your case is likely to be considered an exception, consider if you’ve met the following conditions: If these conditions are not in place, it’s unlikely that your app proposal will be approved. If you believe there are compelling reasons why these conditions have not been met, please set them out in your proposal. In your proposal, please provide answers and evidence on the following: In addition to the evidence requested above, all digital spend for the development of standalone mobile apps is subject to the GDS spend approval. Guidance (including details about response times) on the process can be found on GOV.UK. If you have any queries, please contact GDS.","description":"The government’s position is that native apps are currently rarely justified. At the October 2012 Digital Leaders meeting, the position was clarified: native apps could not be developed without Cabinet Office approval. The November 2012 Digital Strategy says:","link":"/service-manual/making-software/standalone-apps.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Start pages","indexable_content":"Code Guidance Examples on GOV.UK Getting a start page on GOV.UK Research 1. Service name 2. Short introduction 3. Call to action button 4. Additional information the name of the service a short introduction a call to action button additional information Register to vote start page what the service is and who it’s for what user needs it addresses eligibility criteria and restrictions what documents or information the user will need to use the service a preview of the service (if available) other methods of applying eg phone and post a demo URL (with a username and password if necessary) when the service is expected to pass its beta assessment the service passes its beta assessment or self-certification the service goes live how much do people read on these pages? positioning of content above or below the button? does the button style encourage people to not read? what are the most important eligibility requirements to state up front? how should start pages be positioned with regards to related guidance? All services must have a starting point on GOV.UK, unless they’re ‘invitation only’ services. Usually (but not always), this will mean asking the GOV.UK content team to create a GOV.UK start page. Status: Research required Start pages consist of: There’s a start page template in the GOV.UK Prototype Kit. The name of the service should tell people what it lets them do and who it’s for. Avoid acronyms and noun phrases. The best service names start with a verb, like “Pay your car tax”. This should be no more than a few lines. Focus on things that most users will definitely need to know. For example, costs, or required documents. There’s no need to mention something that people will already know or assume. Don’t try to get across complex eligibility criteria. Instead, deal with these inside the service itself. This button text should contain a clear call to action to start the service. If the service is not hosted on GOV.UK then this should be made clear. Use this to tell people about other ways that people can access the service. Well before your beta launch, contact the GOV.UK content team. Tell them: They will either create a start page for you - or if that’s not the best way of meeting the user need, recommend another way of joining up your service with the rest of GOV.UK. Factor in time it’ll take for the content to be written and fact-checked - a start page can only be published after both: You can help improve this pattern by researching the following: Update the Hackpad with your findings","description":"All services must have a starting point on GOV.UK, unless they’re ‘invitation only’ services.","link":"/service-manual/user-centred-design/resources/patterns/start-pages.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Survey design","indexable_content":"What to consider when creating a survey Further reading Questions Common survey question types introduction – purpose of the survey, why it’s important that people take part. screener questions KPI questions – questions that benchmark performance follow up demographic questions – additional respondent information eg working/not working, salary, location etc Male Female Prefer not to answer Was visiting the site for more than one reason Information I was looking for was spread over more than one area of the site Couldn’t find what I was looking for in one area, so went into other areas Stumbled into other interesting areas of the site Just browsing other areas of the site Other (please specify) Strongly agree Agree Neither agree nor disagree Disagree Strongly disagree Don’t know The abundance of free survey tools makes it cheap and easy for user research teams to produce their own surveys. However, thought still needs to be given to the survey design, understanding the goal(s) of the survey , and how the results will be used. In order to increase the effectiveness of the survey, it is important to include an introduction that explains the purpose of the survey to potential respondents. Make it clear what the survey will be used for, and how it will help improve the service. As it’s unlikely that it will be possible to include monetary incentives, it’s important the introduction is worded in a manner that increases the likelihood that people will want to take part. It is also recommended that people are informed of how long the survey will take to complete as this will reduce drop out rate (people who start the survey but don’t finish). Screener questions should be placed at the beginning of surveys to ensure that the correct people take the survey eg demographic information, reason for visiting etc. Quotas may also be set so that you can control the number of people taking the survey that fall into a specific demographic group or audience type. A typical user survey will be structured in the following manner: Surveys are normally composed of the following types of questions: Single response (these type of questions allow the respondent to select just one answer) eg: Q. What is your gender? Multiple choice questions (respondents can select more than one answer) eg: Q. Why did you visit more than one area of the site today (please tick all that apply)? Open ended questions (respondents can write a text response) Q. You rated ‘ease of using the site’ as fair or poor. Please tell us why you gave this rating? Scale questions (respondents are asked if they agree/disagree with a statement) Q. The site was easy to use Once you have decided on the questions that you want to be included, it is necessary to set up the question logic so that respondents are routed through the questionnaire correctly. It is recommended that all surveys are tested thoroughly to ensure the correct data data is collected. University of Leeds – Guide to design of questionnaires","description":"The abundance of free survey tools makes it cheap and easy for user research teams to produce their own surveys. However, thought still needs to be given to the survey design, understanding the goal(s) of the survey , and how the results will be used.","link":"/service-manual/user-centred-design/user-research/survey-design.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Training and learning about agile","indexable_content":"Intended audience Options for learning Agile delivery practitioner Delivery team members People who govern and stakeholders Interested others iterative development responding to change collaboration and completing, releasing and iterating work as set out in the agile manifesto delivery managers / scrummasters / agile project managers agile programme managers product managers service managers developer user researcher designer tester etc SRO, director HR, business operations marketing commercials policy teams legal teams shadowing / learning from others formal training self-initiated learning (networking, reading blogs) sharing ideas and support (community, blogging, public speaking) small experiments and running short projects questioning and retrospecting (short feedback loops and retrospectives) specific frameworks and methodologies: Start with lightweight frameworks and methodologies such as Scrum, Kanban and Extreme Programming (others are available) more specific practices such as: agile user stories, agile estimating, agile planning, managing teams (more are available) softer skills such as workshop facilitation, coaching, motivating teams pairing / learning from others formal training self-initiated learning (reading books and blogs) sharing ideas and support (community, blogging, public speaking) specific frameworks and methodologies: Start with lightweight frameworks and methodologies such as Scrum, Kanban and Extreme Programming (others are available) more specific practices such as: agile user stories, agile estimating, agile planning (more are available here) understanding and applying the GDS governance principles formal training learning from others self-initiated learning (reading books and blogs) questioning and retrospecting don’t slow down delivery decisions when they’re needed, at the right level do it with the right people go see for yourself only do it if it adds value trust and verify go and see / learning from others self-initiated learning (networking, reading blogs) formal training When we refer to agile, we are referring to a culture of: When taking part in any training, it’s important to remember that there are many different approaches to applying this manifesto and its principles. These are only interpretations and there is no one way of doing it. At GDS, we are framework- and methodology-agnostic. We take the tools that we need to from a range of techniques and use the most appropriate for the project and team to help support them to deliver. This means that not all teams will be working in exactly the same way. These are the four categories of people that might want to learn more about agile, the content in the more detailed sections below is repeated where it is relevant to multiple categories of people: Agile delivery practitioner (and those wanting to be) These people need to use agile every day and help teach others. Typical roles are: Delivery team members: if you are becoming part of a delivery team This covers any other people who’ll be part of an agile team, eg: People who govern and stakeholders: if you are going to be a member of a governance team or a stakeholder for an agile project These people are senior responsible officers (SROs), project leads, directors and those responsible for supporting an agile project. Typical roles are: Interested others: if you are just interested in finding out more This covers anyone that has an interest in understanding what it means to work in an agile way. Formal training is only one part of learning, as with most new skills. With agile delivery management in particular, training will only give you the foundations of some of the techniques, but the real learning comes from doing. It’s important that the organisational culture is set up to support learning and experimenting. Think about ways to start getting hands on experience as quickly as possible. This can come through: Shadowing / learning from others You can learn a lot from seeing others who are practicing agile techniques. Because teams are open and transparent, they should be happy to invite you into planning, review and retrospective sessions. You should also try to spend some time shadowing or working with delivery practitioners on problems to give you a better understanding of how they apply agile. Try to invite external speakers in to share their own experiences with you and your department and take the chance to discuss your ideas with them. Formal training Training courses are useful to help understand theory, but it’s important to remember that there is no one answer to how to run every project and that training does not equal learning. It’s worthwhile to learn a number of tools and techniques and strengthen your understanding with self-initiated learning and doing. Some options to think about when training are: Refer to the Civil Service Learning portal to book formal classroom training as a civil servant Self-initiated learning (networking, reading blogs) You can teach yourself through self-initiated learning, networking and reading books and blogs.  The first thing you should read is the agile section of the Service Design Manual. Networking is a useful way of learning. You can also find local meetups and events close to you at www.meetup.com or you could start your own. There are hundreds of blogs about agile and lean techniques with lots to learn. Subscribe to ones that you think are interesting. GDS also has an agile delivery community links list that’s updated regularly, which you can subscribe to. It’s also worth having a look to see if there are any relevant collections on the links collection service paper.li as well as other web collections of links. Sharing ideas and support (community, blogging, public speaking) Once you’re confident, teaching others is a good way to learn. Try engaging in communities like those on LinkedIn, blogging on your own and others’ blogs about agile or speaking about your experiences internally, at meetups or events. Small experiments and running short projects The best learning can come from doing. Agile techniques can be applied to many types of projects and are not restricted to software. This means that you can quickly start to learn as a team through running small projects. Take a project that you’re working on and start to apply agile techniques to it, then improve on them as you go. You can learn a lot from being unsuccessful: the phrase “fail fast” is often used in an agile context, but what you can benefit from is learning quickly. Questioning and retrospecting You’ll learn a lot through asking questions and there is an agile tool that helps you to do this – the retrospective. This is an important part of any agile team and a good way of learning – everyone has an input. Regular retrospectives are great way to understand what works and what doesn’t, so you can think about how your learning can be directed. Pairing / Learning from others Pairing comes from the term pair programming in Extreme programming, but is just as relevant to different types of work. During this practice two people will sit together to work on the same thing at the same time. One of you is writing the code or doing the task, and the other is observing and giving input. The benefits of working in this way lead to better quality of code, better communication amongst the team and learning how something is built and why decisions were made. Pairing will rapidly increase learning as an individual and as a team. Formal training Training courses are useful to help understand theory, but it’s important to remember that there is no one answer to how to run every project and that training does not equal learning. It’s worthwhile to learn a number of tools and techniques and strengthen your understanding with self-initiated learning and doing. Some options to think about when training are: Refer to the Civil Service Learning portal to book formal classroom training as a civil servant Self-initiated learning (networking, reading blogs) You can teach yourself through self-initiated learning, networking and reading books and blogs.  The first thing you should read is the agile section of the Service Design Manual. Networking is a useful way of learning. You can also find local meetups and events close to you at meetup.com or you could start your own. There are hundreds of blogs about agile and lean techniques with lots to learn. Subscribe to ones that you think are interesting. GDS also has an agile delivery community links list that’s updated regularly, which you can subscribe to. It’s also worth having a look to see if there are any relevant collections on the links collection service paper.li Sharing ideas and support (community, blogging, public speaking) Once you’re confident, teaching others is a good way to learn. Try engaging in communities like those on Linkedin, blogging on your own and others’ blogs about agile or speaking about your experiences internally, at meetups or events. Think about how you can enable teams to perform to the best of their abilities. This can come through: Understanding the GDS governance principles GDS have provided some principles for governance on an agile project: It’s important that you understand the detail behind these principles so they can help create the right culture and support for agile projects (and learning) to flourish. Formal training As someone who needs to support delivery and have a broader understanding of agile, you should look for courses that give you an introduction to agile and its benefits, understanding user research, agile estimation and planning. You can use the Civil Service Learning portal to book formal classroom training as a civil servant. Learning from others Try to invite external speakers in to share their own experiences and talk to them about how they compare to yours. See how other people are doing agile within your organisation. Part of the delivery manager / scrum master role is that of a coach and they can help you understand the benefits of working in an agile way. There’s also a job description for an agile coach in the service design manual if you need to bring someone in. Self-initiated learning (networking, reading blogs) You can teach yourself through self-initiated learning, networking and reading books and blogs.  The first thing you should read is the agile section of the Service Design Manual. Networking is a useful way of learning. You can also find local meetups and events close to you at meetup.com or you could start your own. There are hundreds of blogs about agile and lean techniques with lots to learn. Subscribe to ones that you think are interesting. GDS also has an agile delivery community links list that’s updated regularly, which you can subscribe to. It’s also worth having a look to see if there are any relevant collections on the links collection service paper.li. Questioning and retrospecting You’ll learn a lot through asking questions and there is an agile tool that helps you to do this – the retrospective. This is an important part of any agile team and a good way of learning – everyone has an input. Regular retrospectives are great way to understand what works and what doesn’t, so you can think about how your learning can be directed. If you are interested in finding out more about agile, but you are not going to be involved in a project, there are a number of things you can do: Go and see / learning from others See how other people are doing agile within your organisation. Part of the delivery manager / scrum master role is that of a coach and they can help you understand the benefits of working in an agile way. Self-initiated learning (networking, reading blogs) You can teach yourself through self-initiated learning, networking and reading books and blogs.  The first thing you should read is the agile section of the Service Design Manual. Networking is a useful way of learning. You can also find local meetups and events close to you at meetup.com or you could start your own. There are hundreds of blogs about agile and lean techniques with lots to learn. Subscribe to ones that you think are interesting. GDS also has an agile delivery community links list that’s updated regularly, which you can subscribe to. It’s also worth having a look to see if there are any relevant collections on the links collection service paper.li. Formal training If you’re interested in finding out more, you could try a course that gives you an introduction to agile and its benefits. Try to make sure that it’s a broad course that covers more than one methodology or framework. You can use the Civil Service Learning portal to book formal classroom training as a civil servant.","description":"When we refer to agile, we are referring to a culture of:","link":"/service-manual/agile/training-and-learning.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: User-centred design in alpha and beta","indexable_content":"Designers and researchers are essential Researching in design iterations Build Measure Learn Fortnightly user research (eg ‘Testing Tuesday’) Guerrilla research Other research methods Analysis Share your findings Communicating Before the session During the session Affinity sorting Actions Prioritisation Video Showcase Consider your more distant stakeholders Publish the design as it stands Retrospectives keep your team concentrating on real user needs help teams design products which are prioritised by user needs help teams iterate products in response to user feedback they’re sharing all the knowledge that’s been gathered about users, so any decisions made on the product design will be influenced by real user insights the researcher can provide more information and more regular feedback — helping the entire team to iterate and prioritise, and create the best possible product they’ll continually create experiments that allow them to test and prove whether or not design approaches create products that users find understandable and desirable build measure learn it’s usually quite quick to create prototypes of design concepts in code doing so will usually gain more helpful insights from users than paper prototypes allows time for making tweaks and small changes reduces the risk of technical failures in the research sessions have people interact with the design you have been working on (usability testing) explore any specific issues you‘d like to gather more information on through a depth interview (one-on-one interviews that produce deep insights on user needs and behaviours) your own offices a research laboratory (get one from a commercial supplier if there’s not one readily available) define your research questions — what do you want to learn from the round of user testing? prepare some theories about your design, eg changing the words on the button will encourage people to read the page more carefully decide how you’ll know when these theories have been proved or disproved, eg you’ll know this is true because you’ll observe people taking more time to read the page decide the type/demographic of users you want to talk to            age       location       suitability to task       other factors         age location suitability to task other factors start recruitment at least a week before testing (GDS recommend that you find participants through a recruitment agency) prepare a discussion guide that explains how the sessions should be run and the important research outcomes create a participant release form that authorises the recording of the session send a schedule for the day to project team members, inviting and strongly encouraging them to attend sessions arrange for live streaming of the sessions online via a service such as GoToMeeting, so team members who can’t attend in person can still watch it ready to be seen by users ready to be tested end-to-end usable and accessible from the testing venue (eg firewall restrictions, screen resolutions, compatible browsers) make sure the session is being recorded make sure that the live stream of the session is available for external viewers keep research theories at the top of your mind so you don’t forget any important topics write down notes on post-its of important observations            use yellow, super-sticky post-it notes for observations       only 1 observation per post it note       use a marker pen and write in capital letters (easier to read when writing quickly)       if you’re not sure if it’s important, still write it down         use yellow, super-sticky post-it notes for observations only 1 observation per post it note use a marker pen and write in capital letters (easier to read when writing quickly) if you’re not sure if it’s important, still write it down get a written transcript of the session (either during the session or have it transcribed at a later time) analysing sharing communicating have a clear understanding of what you have learned know what the implications on design will be be able to make sure that important observations are not lost gather all the post-it note observations created during the testing sessions and stick them up on a big wall group the observations into similar themes create findings — a statement that summarises the observations — for each group, and write it down on a post-it note, sticking it on top of the group try to address the theories and decide if you have enough information to state a theory is now proven or disproven a theory needs further qualitative/quantitative testing gather findings confirm things that people have seen form a consensus on the findings remember what you discovered see how a particular theme or feature has developed over the course of the design iterations shared document blog story wall (eg Trello) research catalogue showcasing/presenting regular updates always publishing the design having retrospectives the research and design work being done what has been learnt from research how research is changing the design of the service inviting them to your showcases publishing a blog maintaining a shared document sending out regular update emails, using screenshots from the prototype to help people see what has been tested Carry out user research during every stage of your project. Do it continuously through each stage — don’t leave it as something that happens at the beginning and end of phases. Doing user research continuously will: Each team needs to have a designer and a researcher working together, both having an active interest in the design and user insight. This isn’t about a user researcher ‘testing’ the work of the designer — having designers and researchers work together means that: Make sure that you have a team member leading on research that sits with the team. They may also do other work (design, write content etc), but are also responsible for running research at least every 2 weeks. Don’t spend more than 2 weeks working on design without testing it with real end users. Each 2-week iteration should comprise of 3 stages:  With every iteration, create materials to be included in the next round of research. In the alpha and beta phases, these will typically be prototypes that vary in accuracy — from paper prototypes through to working prototypes in code. Use existing GDS code and other open source frameworks as: Accept that you’ll design and build a number of prototypes to test with end users, and that these prototypes will be thrown away. This gives the team freedom to explore a range of different concepts and gain a better understanding of what works best for end users. Allow time for these experiments particularly in the early stages of the project. In the early stages of your project the prototype will probably look unfinished and the design will still require a lot of work. Stick to the ‘test every 2 weeks’ rule, instead of waiting until you’re completely happy with the design or have something that feels finished. Try to make sure that the prototype is completed at least 24 hours before your testing sessions begin, as it: There are many ways to create ‘experiments’ that will help you to answer questions about your product, your end users and your design ideas. Your researcher will help choose the best research methods for answering each question. With their help, you’ll probably end up using a number of different techniques on your project. Whatever your project, be sure to do fortnightly user research. In these sessions you’re able to: Carry out research sessions with users every 2 weeks on the same day. Doing user testing at predictable and regular intervals means that your team members will be able to schedule time to watch sessions. A good approach is to schedule 5 one-on-one interviews for that day, each interview being 1 hour in duration. (Schedule a 6th participant as a ‘spare’ in case some participants don’t attend earlier in the day or you have a poorly recruited participant). Conduct these sessions in either: Make sure that you have both facilities booked and participants recruited as early in your project as possible for the expected duration of the alpha and beta phases. To prepare for these sessions, you need to: Prepare your prototype for the sessions by making sure that it’s: In each user testing session: A day of research should be followed by a period of analysis before making any significant design changes. The analysis stage is described in detail below. You can use the time between more formal testing to conduct some guerrilla-style testing, getting some initial feedback on your revised designs. Guerrilla testing normally involves taking your prototype into a coffee shop or other public location and finding volunteers who’ll give you some quick feedback. Guerrilla testing participants aren’t necessarily representative of your target audience, but they can provide a quick sense-check in between formal testing sessions. There are many other research methods you can use to supplement your ‘in person’ qualitative research and to address specific research questions. Using a different technique can provide better insight into a particular research question, or validate insights from fortnightly research with a larger audience. Here‘s a list of some techniques you can use and information on how and when you should use them. Now it’s time to see what you can learn from your user testing and research. Make the best possible use of this information by: It’s important to properly debrief after spending a day doing user testing as you’ll have seen a lot of people and a lot of different reactions. Although it takes time to analyse sessions in detail, doing so means that you’ll: To do this correctly, you need to: At this stage you’re aiming to create a full set of insights (things that you’ve observed and learned), so don’t start designing solutions just yet. You should allow several hours for this analysis process. Work with anyone that’s seen the user testing session to analyse observations, as it will help to:  Decide what you’re going to do about each finding, if anything, and when you’ve proven a theory, make sure that it’s recorded and shared with the project team. If a finding needs further action, determine what is required to address it in the next design iteration. Write this on an orange post-it note and stick it on top of the finding. When you’ve decided on what actions to take, use a prioritisation method (like dot voting) to decide how you’ll spend your efforts for the next iteration. Put user testing findings and actions in a place that’s easily accessible to the project team, as it’s likely that you’ll come back to the findings to: You can document your findings for others to see in many different ways, like using a: It’s also useful to keep a record of what you tested with users, eg if you’ve prototyped in code, keep a folder in your repository for each round. You may also like to keep screenshots of the prototype. GDS intend to create a format for sharing research findings between projects across government. While GDS work on the best way to do this, keep your research findings in an accessible shared format so findings can be shared and learned from in the future. The findings should, wherever possible, be easily understood with no need for an explanation. Move any actions onto your story wall so that the whole team can see what you’re working on and who’s working on it. Use videos to demonstrate findings and store the session videos in a safe place, ideally where they can be found and watched by team members. If you store the videos on a public service (like Vimeo), make sure they are adequately protected. You need to discuss the project and its progress with others. You can do this by: Just as a product team often presents a showcase at the end of each iteration, so should the research and design team. This will make sure the wider project team has an understanding of: This is an opportunity for members of the project team to catch up if they haven’t been able to attend user testing sessions, as well as an opportunity to ask questions. Depending on how your team works, you might put your presentation into the project team’s showcase or you may have a showcase just for research and design. There may be other people beyond your more immediate project team who are interested in the outcomes of your work. You can communicate and/or work together with these people by: Make it possible for your team to always access the latest version of the prototype, but make sure it’s appropriately protected. Team members may need to check something or use the prototype to talk about the project with their own stakeholders. Put a message across the top of the prototype to remind people that it’s a work in progress. It’s important to reflect on your progress at regular intervals. Schedule a retrospective where research and design team members have the chance to discuss the process openly and can suggest changes to the process. Assign owners and due dates to the actions to make sure that change occurs.","description":"Carry out user research during every stage of your project. Do it continuously through each stage — don’t leave it as something that happens at the beginning and end of phases.","link":"/service-manual/user-centred-design/user-centred-design-alpha-beta.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: User research surveys","indexable_content":"How surveys work Weaknesses and when not to use Participants Online web surveys Email survey Face to face interviews Telephone interviews don’t allow much time or context for considered replies leave room for questions to be misinterpreted have a completion bias – one common criticism is that they are only completed by people who are either very satisfied or very dissatisfied Surveys contain closed ended questions with fixed responses such as ‘yes’, ‘no’, ‘very satisfied’ to ‘very dissatisfied’, ‘excellent’ to ‘poor’ etc, and open ended questions that allow respondents to provide written responses. Surveys can be conducted either face to face by a trained moderator or through self-completion (online, postal etc) in order to quantify thoughts/beliefs, behaviours, ownership etc. Surveys can be a relatively cheap and quick way of gathering and quantifying public opinion and monitoring change over time. Surveys can be used to size a market to find out how many people own a product, or take part in an activity eg how many people use the Internet in the UK. They can be also used to understand usage and attitudes towards a product or activity eg how often people use the Internet, how they access it, why the use it etc. Complex analysis can be also be conducted to understand key drivers of attitude/behaviour, segmentation of audiences, and trade offs between different opinions etc. Surveys provide a snapshot of opinion, and are a useful method for monitoring uptake, usage and attitudes over time. Self-completion surveys used to obtain information from web users. Surveys are activated when a user interacts with a page – this can either be through arriving/leaving a page, or clicking on a link. Software is used to control the  proportion of users who are invited to take a specific survey. In order to increase completion rates, online surveys are normally no longer than 5 to 10 minutes in length (approximately 20 to 25 questions). These are identical in set up and design to online web surveys, except participants are asked to complete them via an email, and not at random when they visit a website. Email surveys are often more targeted as participants can be identified by information already known about the them eg demographic, information usage etc. Online or email surveys are normally the cheapest method as these can be conducted in-house using inexpensive or free survey software, inviting people to take part from their own website, or using a database of email addresses. Like telephone interviews, face to face interviews are structured and conducted by a trained moderator. Unlike other survey methods, respondents may be shown limited stimulus – eg an advert, a leaflet cover or a website design. Face to face interviews also enable researchers to target respondents more easily by demographic, geographic and socio-economic group. Structured interviews that are conducted over the phone by a trained moderator. Telephone and face interviews are the most costly methodology and they are normally conducted by a third party agency. Although surveys can be a quick and cheap manner in which to conduct user research, they often: All surveys, regardless of methodology, should use a robust sample that is nationally representative or representative of an audience of interest. Most nationally representative surveys are 1,000 people or more, but can be significantly larger if there are a number of distinct user groups that are being targeted. It is important that the delivery channel used considers the target audience. For example in the recent Digital Landscape research an online and face-to-face methodology was used as both online and offline audiences needed to be interviewed – an online only methodology would have excluded those who are offline, and the opinions of this audience omitted from the research. It is also important to note that performing research with hard-to-reach groups can be more time consuming and costly (eg black and minority ethnic, people with disabilities).","description":"Surveys contain closed ended questions with fixed responses such as ‘yes’, ‘no’, ‘very satisfied’ to ‘very dissatisfied’, ‘excellent’ to ‘poor’ etc, and open ended questions that allow respondents to provide written responses.","link":"/service-manual/user-centred-design/user-research/user-research-surveys.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: User research tools","indexable_content":"Survey tools Remote user testing tools Card sorting tools Online focus groups Community groups Text analysis tool    Please note: Examples are illustrative, rather than recommendations, and we suggest checking G-Cloud regularly to see if any similar services are available using that framework.  There are a number of good tools that can help user research teams conduct in-house research quickly and effectively. Below is a high level overview of what these enable, for people unfamiliar with user testing. Online tools are a good way in which to conduct in-house research cheaply and effectively. Conducting research in-house enables user research teams get closer to the data, and have a good understanding of the user. This is, however, dependent on teams having adequate resource to enable analysis to be conducted properly and fed back to the relevant teams. Please note: Examples are illustrative, rather than recommendations, and we suggest checking G-Cloud regularly to see if any similar services are available using that framework. These enable the easy creation of online surveys. Features include a scripting tool that allows easy creation of different question types, the ability to add branding, advanced branching, automated reporting, automatic activation/pop script, email links etc. Some good tools are available free, however, these often have restrictions on the number of questions you can ask, the number of completes you can collect, and advance features such as API, and branding. Examples include Survey Monkey, Survey Expression, Smart Survey and Fluid Surveys. These tools enable user research teams to conduct remote usability testing in-house. These tools allow you to script the surveys easily, while also conduct the analysis to be conducted relatively simply and quickly. Most of these tools also enable you to conduct standard online surveys within the same software package. Examples include Keynote Webeffective, and User Zoom. These tools enable user research teams to conduct online card sorting in-house. Card sorting tools usually come as part of a larger survey package. Examples include User Zoom and Optimal Workshop. Enable in-house research teams to conduct focus groups online. Most also enable you to run 1:1 sessions, auto ethnography, diary studies, and micro communities. Examples include LiveMinds and VisionsLive. Enables in-house research team to manage a community of users and/or stakeholders. These normally come part of a larger online research tools package. Examples include LiveMinds and Bloomfire. Enables the analysis of large amounts of written feedback – via email, helpdesk etc. This software enables you to sort user feedback into themes, and make it easier to action. Examples include Feedback Ferret and Atlas.","description":"There are a number of good tools that can help user research teams conduct in-house research quickly and effectively. Below is a high level overview of what these enable, for people unfamiliar with user testing.","link":"/service-manual/user-centred-design/user-research/user-research-tools.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: User research skills","indexable_content":"How user researchers work How to hire researchers Job description templates Further reading Really understanding who your users are, understanding their needs and maintaining a close focus on designing digital services that best meet their needs is an ongoing struggle again our internal processes and domain knowledge. Regular user research helps to make sure that we start and stay aligned with real user needs. User researchers are involved at all phases of the project and work closely and collaboratively with designers and product owners. In the early stages of discovery they assist in developing a clear understanding and empathy for end users, making sense of existing research and commissioning or conducting additional research. Through the Alpha and Beta phases they work closely with the design team to provide guidance based on their understanding of end user needs and behaviour, and working with the team to design methods to answer outstanding questions about the users and the design of the service being created. As a member of the delivery team, you should hire researchers who are comfortable working in an agile team, delivering continuous research in fortnightly iterations. Especially in alpha and beta phases, you will get better results by bringing a researcher into your team rather than outsourcing research to 3rd party. In particular you should look for a researcher who has experience contributing to other disciplines in the team. Capabilities in design or content are particularly helpful. In product development a mixture of experience in both qualitative and quantitative research is useful, although the most essential will be the ability to design and conduct qualitative research. Teams should aim to observe real users interacting with the product they are designing at least every two weeks, most often in the form of one-on-one interviews. When hiring a user researcher there are two main things to consider. Firstly consider whether they have sufficient experience in designing, facilitating and analysing user research, in particular one-on-one interviews and usability testing. A researcher with hours of experience observing end users interacting with digital products can bring value to the design team immediately. Most experienced researchers should be able to provide you with an estimate of their total hours of experience observing users and, on request, provide video clips showing examples of their facilitation techniques for you to review. You are looking for a researcher who has 100+ hours of facilitation experience and who demonstrates an empathetic yet methodical approach to facilitation. Secondly, consider their ability to work well in an agile team. This means that they need to be able to work collaboratively with designers and product owners, to be flexible and responsive in designing research and comfortable with a fast moving work environment where change is constant, and most importantly, they need to have experience and ability in quickly and effectively communicating research insights in an actionable way so that these insights actively shape the design and development of the product. This will usually mean that the researcher has exercised some creativity and experimentation in their method of communicating findings and they should be able to share examples that were more and less successful. See an example of a user researcher job description provided by GDS. Specific guidance for user researchers working on digital by default services. An introduction to user research techniques for each stage of the project.","description":"Really understanding who your users are, understanding their needs and maintaining a close focus on designing digital services that best meet their needs is an ongoing struggle again our internal processes and domain knowledge. Regular user research helps to make sure that we start and stay aligned with real user needs.","link":"/service-manual/the-team/user-researcher.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Usernames","indexable_content":"1. The recommended approach 2. Email addresses as usernames 3. Let people create their own username 4. Create a username for the user 5. Let people change their username On this page: The good: The bad: The good: The bad: Tips: The good: The bad: Tips: The recommended approach Email address as username Let people create their own username Create a username for the user Let people change their username choose email address as the primary option allow people to create their own username if they don’t have or want to use an email address help these people choose a unique username by making suggestions do not use service-generated usernames for repeat-use services they’re unique for many people they’re easy to recall it’s less cognitive load to use an email than create a username it’s often a required bit of information anyway not everyone has or wants an email address some people share an email address (eg older people or carers) some people may not remember which email address they used people can lose access to their email (eg when they leave school or work) they can’t be shared in public people don’t need an email address to use your service people have control – good if the username will be part of their online identity making up a username can be hard for people if you’re asking for email anyway it’s just one more thing for users to do easier to forget than an email address let people use characters like spaces, or let them know if they can’t help people who’s first choice is already taken by suggesting similar alternatives make sure people can retrieve or reset their username if they forget it you can make sure they’re unique less up-front effort for the user computer generated usernames are unlikely to be memorable poor user experience as people have to store and retrieve the username from somewhere send people their username through some other channel or remind them to write it down make sure people can retrieve or reset their username if they forget it Usernames must be unique and easy for people to recall. We recommend that you: Using email addresses as usernames has a number of advantages, but make sure you thoroughly research the users of your service before making it the only option or you may end up excluding people. Find out more about asking for an email address Use this option for services with a social element or in combination with the email option. You can have the service create a username for people from a reference number or similar. Only use this option for infrequent, short term interactions like checking the status of a transaction. Some people will wish to change their username. For example if it’s an email address that’s no longer available for them or it reflects a name that they no longer use. If people will be signing in to your service more than once or twice you should let them change their username or make it easy to migrate to a new account. Make sure you think through the consequences of people doing this. Discuss this page on Hackpad","description":"Usernames must be unique and easy for people to recall.","link":"/service-manual/user-centred-design/resources/patterns/usernames.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Using data","indexable_content":"Understand user needs/decide what to measure Install and configure platforms Establish a baseline Aggregate data Analyse and visualise data Monitor, iterate, and improve Further reading identify the effect of your communications or design initiatives reveal seasonal variations in performance dashboards reports alerts channel used to access service – through which channel(s) did the user find out about and attempt to use the service? new compared with repeat visitors – are first time users behaving differently to those who have used the service before? geographical region – how popular is the digital service by region and how does that compare with online penetration in general? product type – does the user experience vary depending on the type of product or service? value – is performance dependent on the monetary value of the product or service being sought? dashboards – based on objectives and will help inform decisions, often with the help of real-time data reports – providing regular, scheduled snapshots of data, but tend to need extra context and time to absorb alerts – used to inform the users about a change or an event, often using attention-grabbing tools to do so keeping charts plain – don’t use shading, 3D or other distracting effects removing clutter – don’t use trend lines, grid lines, unnecessary labelling not using pie charts – they require more space and are harder to read than bar charts using text where a chart adds nothing Test a range of actions for improving performance and monitor to see which work well. Pilot these on a portion of your users to minimise risk. Implement the best performing solutions widely and then repeat this process continuously as what you measure will change over the course of your product or project’s lifetime. price – change the price eg to attract people to the digital channel product – improve the user experience (eg from user feedback, user testing, A/B testing, multivariate testing) placement – place the digital service URL on printed materials and in voice recordings promotion – increase your use of email and social media to encourage repeated use of your digital service an article on online customer satisfaction scores for retailers, based on the Customer Satisfaction Index, written for The Guardian The role of the data scientist by Mike Loukides Costing customer time, a method for calculating the cost of a transaction for both the service provider and the user (HMRC developed a method for calculating the cost of a users time when interacting with government – this is important because some channels may be quicker to use than others) Designing with Data, an excellent book by Brian Suda which helps you to design beautiful and powerful data visualisations Juice Analytics, a website with lots of useful resources on how to design and develop useful data visualisations and dashboards Edward Tufte’s The Visual Display of Quantitative Information, an original piece of work on data visualisation and introduces the concept of chartjunk the Flowing Data blog by Nathan Yau, a useful source of data visualisation news the D3 Gallery, a stunning collection of data visualisations a nicely presented overview of some of the tools available for data visualisation this article in Wired, which shows how A/B testing was used to good effect in Obama’s election campaign this article in eConsultancy, which shows how multivariate testing was used to improve conversion rates at Lovefilm Simply collecting information about how your service is running isn’t enough to make judgements about how to improve it. A process of continual iteration and close measurement will help you to see what needs improvement and how to investigate ways of improving your service.  Make sure the core key performance indicators (KPIs) established in the service standard, and any other KPIs you choose to measure, accurately reflect the needs of your users and stakeholders. This will allow you to measure the ability of your service to meet user needs. While building your service, make sure that appropriate analytics tools are being used to monitor the service, collecting the data necessary to produce accurate and timely measurements. For more information, read our guidance on choosing analytics tools. Establish a ‘baseline’ based on the current performance trends of each channel. Use this to judge the effectiveness of any changes to your service. This will help you pinpoint the effectiveness of your initiatives, and identify what worked. Look at performance trends over time, rather than taking a snapshot at a particular point. Peaks and dips in performance can then be measured in relation to this base (or trend) line, which helps to: Collect and combine performance information from multiple sources and across multiple channels. Make sure you understand what this will mean in terms of system requirements. Combining data often reveals useful insights, eg into service efficiency (cost per transaction and total cost to serve) or the amount of usage by channel (percentage digital take-up vs post, phone etc). Communicate performance information to your users through the appropriate: Highlight specific segments that you know users are interested in, and make sure that your visualisations are simple, useful and contain minimal amounts of chartjunk. Typical segments include: By making your visualisations easy to understand you significantly increase the likelihood that the information will be used to improve your service. Best practices include: Any service that meets user needs will include an element of user feedback. Monitor this and act on it. Doing so will help to continually improve the service for your users. Many options are available for improving the overall performance of your service. The following examples are based on the 4 Ps of marketing: Taking an iterative approach to service development will increase the speed of improvements and minimises the risk of failure. Don’t wait until the end to do this – do it continuously throughout the process. To find out more about ways to use data, try reading:","description":"Simply collecting information about how your service is running isn’t enough to make judgements about how to improve it. A process of continual iteration and close measurement will help you to see what needs improvement and how to investigate ways of improving your service.","link":"/service-manual/measurement/using-data.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: User stories for web operations","indexable_content":"The problem Intended audience Stories Service managers Delivery managers Process Shared service Policy Design Components Monitoring Logging Configuration management Deployment Access control Provisioning Security controls Testing Operating system This document outlines the typical scope of infrastructure and web operations (sometimes erroneously referred to as hosting) work on a large service redesign project. The sample list of user stories provided is not intended to be a complete list of all areas of interest nor are you likely to need to do all of this for every service. The idea is for this list to be a good starting place from where to you can write additional stories, delete ones you do not require and split stories into smaller ones. Importantly you also need to provide your own acceptance criteria specific to the needs of your service. Remember these stories are a placeholder for a conversation. For some contexts, that conversation will be ‘this does not apply to my service’ – that is fine. But there will almost certainly be other stories not listed here which do apply. An issue we have observed on a number of projects is a lack of understanding early on in a project about the work required to run a large online service. Often this is placed under  hosting and is investigated too late in the process. The hosting of a complex and sensitive software application requires a team of people with specialist skills to design, setup and operate. Because this work is generally not user facing and can be highly technical it is sometimes easy to leave until later – with potentially dire consequences for launching safely and on time. Does your team have people who deeply understand this topic? If you are not an expert then it is important to involve people permanently in the team who are. They can explain the technical trade offs and decisions which may affect your service. As well as understanding the potentially large scope of work, many of the areas discussed here have lead times associated with third parties. The earlier stories related to these topics are brought into project backlogs the sooner estimates can be made and deadlines understood. The following stories are intended to provide a starting point for any project, rather than be a complete set. Individual projects would be expected to take and modify stories as needed and importantly to apply their own acceptance criteria specific to their requirements. The majority of these stories are from the point of view of developers, web operations engineers and the responsible service manager. Although not ideal, for this particular technical topic this works reasonably well. Feel free to change the focus when using them in your backlog. Development process As a developer working on the service So that we can ensure a high level of quality And so we can maximise the integrity of the source code I want a well documented and understood development process Out-of-hours support As the service manager responsible for the service So that we can ensure a suitable level of availability and integrity I want to understand the requirement for Out-of-hours support Disaster recovery As the service manager responsible for the service So that in the event of a disaster everyone doesn’t panic and make things up I want a clear disaster recovery plan in place to deal with different types of catastrophic event Release process As the service manager responsible for the service So that the service can be changed on a very frequent basis And so that changes do not cause problems for users I want a well documented and understood release process Security response As the service manager responsible for the service So that security incidents are handled with extra care And so that the service meets its wider Government obligation to GovCert I want a well documented and understood security incident process Helpdesk As the service manager responsible for the service So that communication with users is done in a joined up way I want a central helpdesk function to deal with events, incidents and requests Request Management As the service manager responsible for the service So that questions from users can be dealt with efficiently I want a clear information request management policy Event Management As the service manager responsible for the service So that likely events that could affect the running of the service can be dealt with smoothly I want a clear event management policy Incident Management As the service manager responsible for the service So that problems that arise with that service can be dealt with efficiently I want a clear incident management policy Operations manual As the service manager responsible for the service So that information about the running of the service is not kept in individuals’ heads And so information is readily available to people running the service I want a single place to store content for a service operations manual Source code hosting As a developer working on the service So we have somewhere to securely store our source code I want access to a central source code hosting service or repository Continuous Integration As a developer working on the service So we can ensure a high level of quality in the code And so we can minimise the time needed for regression testings I want a Continuous Integration environment which automatically runs tests against every commit External DNS As a web operations engineer So that visitors to the service don’t need to remember an IP address that will change I want a process and supplier relationship to manage external DNS addresses Sensitivity of source code As a developer working on the service So that I understand the controls that need to be in place And so that I know who and how I may share it I want a clear policy around the sensitivity of source code Third party code As a developer working on the service I want a clear policy around use of third party source code libraries So that I do not introduce unknown security problems Change evaluation As the service manager responsible for the service So that I can release changes to production quickly And so that we can meet our obligation to the Digital by Default Service Standard I want a documented process for evaluating and deciding on a change to the production service Access control As the service manager responsible for the service So that the confidentiality, integrity and availability of the service isn’t compromised And so that suitable technical controls can be put in place to enforce it I want a clear policy on who has access to what on the production system Separation of duties As the service manager responsible for the service So that we can ensure the service has enough people in the right roles I want to understand any required separation of duties (whether driven by legislation or security concerns) Clearances As the service manager responsible for the service So that security clearances can be arranged early in the project to avoid access restrictions later on I want to know what level of clearances are required for different roles (including third parties) Releasing open source As a developer working on the service So that I do not introduce unknown security problems And so that we can meet our obligation to the Digital by Default Service Standard I want a clear policy around releasing code as open source Government networks As a technical architect So that the right suppliers are contracted And so that long lead times are factored into the project plan early I want to know whether the service requires access to a Government network like the PSN or GSI Multiple infrastructure providers As the service manager for this service So that I understand the intended availability constraints I want to know whether multiple suppliers of Infrastructure are required Capacity planning As a web operations engineer So that we can estimate the number and size of infrastructure components (instances, firewalls, load balancers etc.) And so that resource based costs can be estimated I want to carry out some capacity planning activities Network architecture As a technical architect So that I can build out a production environment to an agreed specification I want a network architecture design Web servers As a web operations engineer working on the service So that we can serve HTTP request And so we can proxy requests to application servers I want to install and configure a web server Databases As a web operations engineer working on the service So that data can be stored in a manner befitting its structure And so the stored data can be queried as quickly as required I want to install and configure a suitable database server As a web operations engineer working on the service So that data can still be read even during a failure of a single database server I want to configure some failover or other redundancy mechanism for the database As a web operations engineer working on the service So that data can still be written even during a failure of a single database server I want to configure some failover or other redundancy mechanism for the database Load balancers As a web operations engineer working on the service So that web requests can still be served even with the failure of one or more web servers I want to install and/or configure a load balancer Internal DNS As a web operations engineer working on the service So that we can easily address our services and instances I want to install and/or configure a mechanism to manage internal DNS Database backups As the service manager for the service So that we can recover from a large failure of our database infrastructure I want regular automated backups to be taken of the data stored in the database As the service manager for the service So that we can recover from a large failure of a single suppliers infrastructure I want regular automated backups to be stored off site HTTP cache As a web operations engineer working on the service So that the service remains fast when serving identical content And so load is minimised on the application servers I want to install an HTTP cache Email gateway As a developer working on the service So that the service can send email to administrators or end users I want to setup and configure a suitable email gateway Application servers As a developer working on the service So that the code I write can be run on server instances I want to install and configure a suitable application server Internal package repository As a web operations engineer working on the service So that we can use software not available in our operating system repositories And so that we can use the security, dependency management and versioning features I want to install and configure an internal package repository Artifact repository As a developer working on the service So that we can share and version individual code components that need it I want to install and configure an artifact repository Message queue As a developer working on the service So that I can easily and efficiently process work asynchronously I want to install and configure a suitable message queue or work queue system Search server As a developer working on the service So that I can quickly and efficiently search through large amounts of data I want to install and configure a suitable search engine Object cache As a developer working on the service So that I can minimise the number of queries to the database And so that I can keep the service fast and responsive to users I want to install and configure a object caching system Metric collection service As a web operations engineer working on the service So that we can collect large numbers of time series metrics from the running service I want to install and configure a metric collection system Application running monitoring checks As a web operations engineer working on the service So that we can run checks against metrics from the metrics system And so that we can run active checks based on arbitrary code I want to install and configure a monitoring system Smoke tests As a developer working on the service So that I know that I haven’t broken anything when deploying my application I want a series of smoke tests to be run after all deployments Application metrics As a developer working on the service So that I can gain visibility of how my application is running in production And so we can find and fix problems with it quickly I want a simple way of instrumenting my application to feed metrics to the metrics system System metrics As a web operations engineer working on the service So that we can identify and fix problems with the system, ideally before they occur I want to set up collection of low level system metrics like load, disk, network io, etc. Security monitoring As a web operations engineer working on the service So that we notice quickly and are alerted to any incidents with a security flavour I want to configure suitable security monitoring tools Notifications As a web operations engineer or developer supporting the service So that I know about any issues as they happen I want to set up suitable notifications from the monitoring system Transactional monitoring As a developer working on a transactional service So that we can block fraudulent or otherwise suspect transactions I want to install and configure a transactional monitoring system with suitable rules External monitoring As the service manager for the service So that in the event of a failure of the monitoring system And so that the service is monitoring from outside our local network I want an external monitoring capability with basic checks to monitoring service uptime Monitoring data feed from infrastructure provider As a web operations engineer working on the service So that I am aware of problems in the hypervisor, physical or network infrastructure I want a feed of monitoring data from the Infrastructure supplier Log collection As a web operations engineer working on the service So that I can easily see everything that is happening in specific applications I want to collect all the logs from applications running on the same host in one place Log aggregation As a web operations engineer working on the service So that I don’t have to go to an individual machine to view its logs I want all logs from all machines to be aggregated together Log storage As a web operations engineer working on the service So that logs can be kept for a suitable period of time I want to provision enough storage for log archiving Log viewing As a web operations engineer working on the service So that I can see what is happening across the infrastructure I want a mechanism for viewing and searching logs in as near real time as possible As a developer working on the service So that I can extract information from logs to aid with improving the service I want a mechanism to run queries across the aggregated logs Configuration management client As a web operations engineer working on the service So that changes to server configuration can be made safely and quickly I want to install software to manage configuration management Configuration management database As a web operations engineer working on the service So that configuration changes are tracked over time And so that current state of available to query I want to install software to manage a configuration management database Configuration management server As a web operations engineer working on the service So that all nodes do not have all configuration information I want to install software to allow centralised management of Configuration management code Configuration management code deployment mechanism As a web operations engineer working on the service So that configuration changes can be made safely and in an auditable manner I want a deployment process and tooling for configuration management code Application deployment mechanism As a developer working on the service So that changes to applications can be made available to users And so that changes are made in a safe and auditable manner I want a deployment process and tooling for application code Release tracking As the service manager for the service So that we have an auditable log of what was changed when by whom I want an up-to-date list of releases to be maintained Packaging As a web operations engineer working on the service So that we don’t have to compile customised applications from source before using them And so we can take advantage of dependency and version management capabilities of the OS I want a process and tooling for creating our own system packages Orchestration As a web operations engineer working on the service So that I can run commands across multiple instances quickly I want tooling in place which allows some orchestration based on the current instances Database migrations As a web operations engineer working on the service So that I can have confidence that database migration scripts will work when applied to production I want database migrations to be deployed through the same sequence of environments as code changes   Management of secrets As a web operations engineer working on the service So that I can ensure confidential communication between particular parts of the system I want a process or tool for managing secrets such as keys and passwords End user devices As the service manager responsible for the service So that management access to the infrastructure can be locked down to prevent unauthorised access I want to know what kind of protection the management end user devices require User directory As a web operations engineer So that we do not have to maintain multiple lists of privileged users And so that users can be added and removed once in a central fashion I want to install and configure something to provide a single user directory Key based authentication As a web operations engineer So that we are not vulnerable to password based login attempts to individual servers I want to set-up public key based authentication Single sign-on As a web operations engineer So that any third party web interfaces we use can be accessed via a single login I want to install and configure a single sign-on systems Network/VPN configuration As a web operations engineer So that management functions can not be accessed via the public internet And so that we reduce the surface area for attack I want to restrict management access to a VPN and/or non-public restricted network Other environments As the service manage for the service So that I can see the very latest working version of the service at any time And so I can share that with people in and outside the team I want a preview environment to be provisioned which is similar to production As a web operations engineer working on the service So that the we have a clean environment in which to test production deployments And so that we have a secure environment to test with production-like data I want to provision a staging environment which mimics production as closely as possible Production environment As a web operations engineer working on the service So that the service can launch to the public I want to provision a production environment Base image(s) As a web operations engineer working on the service So that all server instances start out with sensible security settings I want to create a base image running the chosen operating system with hardened configuration Public network interfaces As a web operations engineer working on the service So that the application only receives wanted traffic from the internet And so that we don’t accidentally expose sensitive or insecure components of the system I want to configure and test the public network interfaces for the system Private network configuration As a web operations engineer working on the service So that individual internal components can only talk with known parts of the system And so we limit the extent of any security breach I want to configure and test the private network interfaces for the system Network codes of connection As a web operations engineer working on the service Given I need to communicate with a system only available on a Government network So that the two systems can talk with each other I want to meet the code of connection requirements and configure access to the network Management network As a web operations engineer working on the service So that network traffic used to manage the infrastructure is separate from public traffic And so we can monitor irregularities in network traffic separately I want to configure a separate management network Platform load balancers As a web operations engineer working on the service So that we can reduce the number of single points of failure And so that we can scale out to deal with a large amount of traffic I want to provision load balancers to distribute traffic between multiple instances Platform firewalls As a web operations engineer working on the service So that unwanted traffic can be filtered before it enters our virtual infrastructure I want to configure the external facing IaaS firewalls to only allow certain traffic Dynamic environments As a web operations engineer working on the service So that we are not constrained by a fixed number of environments And so we can easy run full stack tests or experiments I want to be able to easily provision an environment running the full service Elastic scaling As a web operations engineer working on the service So that the service can automatically deal with unexpected increases in traffic I want to configure tooling to automatically scale the number of instances based on load Operating system hardening As a web operations engineer So that we are making use of built-in operating system security controls I want to automate a default set of hardening rules for our chosen operating system Malware detection As a web operations engineer So that instances which may be compromised can be dealt with quickly I want to automate the detection of potential malware Intrusion detection As a web operations engineer So that instances which are being attacked or probed can defend themselves I want to configure an intrusion detection and prevention system Virus scanning As a web operations engineer So we can be sure that files in the system don’t have viruses I want to install virus scanning for files passing a network boundary Host firewalls As a web operations engineer So that the surface area for attack is limited And so that services which should only be available locally aren’t exposed on the internet I want to install and configure a local firewall On instance event auditing As a web operations engineer So that I know when things like logins or other sensitive events happen on instances I want to set-up some auditing of events Rate/connection limiting As a web operations engineer So that large spikes in traffic from a single source don’t overwhelm application I want to configure some level of rate and connection limiting for web requests Secure storage of key material As a web operations engineer So that any highly sensitive cryptographic keys are not lost, resulting in a compromise I want to have a mechanism in place to securely store key material Third party DDoS protection As a web operations engineer So that a the site does not go down under a denial of service attack I want to purchase and/or configure a level of DDoS protection Performance testing As the service manager responsible for the service So that we know the service will be fast and responsive under realistic traffic I want to be able to run a comprehensive performance test suite against the service As a developer working on the service So that we know changes to the code do not negatively affect performance I want the performance test suite to run as part of the continuous integration system Load testing As the service manager responsible for the service So that we know the service will still be working under larger amounts of traffic than are expected I want to be able to run a comprehensive load test suite against the service Application penetration testing As the service manager responsible for the service So that the service does not get compromised due to a vulnerability And so we meet our accreditation obligations I want to run a suitable number of penetration tests against the applications under development As the service manager responsible for the service So that the service does not get compromised due to a vulnerability And so we meet our accreditation obligations I want to run a suitable number of penetration tests against third party installed applications used as part of the service Infrastructure penetration testing As the service manager responsible for the service So that the service does not get compromised due to a vulnerability And so we meet our accreditation obligations I want to run a suitable number of penetration tests against the infrastructure configuration Operation system selection As a web operations engineer working on the service So that we have a clear path to receiving security updates And so we can more easily find support for our systems I want to select and install a suitable default operating system for the service File systems As a web operations engineer working on the service So that we get the best possible performance and reliability from the disk I want to select a suitable file system and partition layout Resource isolation As a web operations engineer working on the service So that noisy applications cannot affect other applications on the instance I want to be able to isolate running applications from each other in terms of memory and CPU Read-only file systems As a web operations engineer working on the service So that I can protect against files being changed due to compromises in the application I want to be able to configure a read-only file system if appropriate.","description":"This document outlines the typical scope of infrastructure and web operations\n(sometimes erroneously referred to as hosting) work on a large service\nredesign project.","link":"/service-manual/operations/web-operations-stories.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Web operations skills","indexable_content":"The importance of web operations Skills Guidance Job Description Further reading work with developers to optimise existing applications and to design new ones participate in stand-ups, planning sessions and retrospectives design, build and run systems for application deployment, systems orchestration and configuration management encourage everyone (developers, delivery managers, product owners) to think about how new applications will be run and maintained contribute to designing internal processes needed in the running of a high performance development and operations organisation help everyone understand constraints around security, performance, cost and resulting tradeoffs a deep understanding of the target operating system experience of multiple programming languages common deployment patterns continuous integration capacity planning load and performance testing techniques highly-available systems design administration and tuning of production database systems installation and usage of monitoring tools; for instance Nagios, Ganglia, Riemann, Graphite etc knowledge of configuration, deployment and management of web application stacks configuration management tools like Puppet, Chef, CFEngine EC2 or similar dynamic provisioning compliance, auditing and security open source development experience in a product centric environment Presentation given to GDS about web operations Article explaining the web operations role Book all about web operations topics Web operations engineers (sometimes called systems administrators, operations engineers or site reliability engineers) run the production systems and help the development team build software that is easy to operate, scale and secure. This involves expertise in areas such as infrastructure, configuration management, monitoring, deployment and operating systems. Web operations people help run the eventual production systems, but also to help the development team build software that is easy to operate. Thinking about how the eventual system will be run at the very start of the project is important if you want to smoothly move from prototypes to production systems. At a high level they will: Web operations engineers will have specific skills, such as: And ideally have an interest in or some experience with: Read guidance in the manual of particular interest to web ops. Click either of the options below to download a template Web Ops job description.  Download as OpenDocument Format / Download as MS Word doc  Cabinet Office will help departments to recruit suitably skilled individuals through the Recruitment Hub.","description":"Web operations engineers (sometimes called systems administrators, operations\nengineers or site reliability engineers) run the production systems and help the development team build software that is easy to operate, scale and secure. This involves expertise in areas such as infrastructure, configuration management, monitoring, deployment and operating systems.","link":"/service-manual/the-team/web-operations.html"},{"_type":"manual_section","organisations":["government-digital-service"],"manual":"/service-manual","title":"Government Service Design Manual: Working with prototypes","indexable_content":"Building prototypes Ideas can be ugly understanding your users and their needs developing a sense of how you might serve those needs estimating the effort involved in building and running a service to do so The best way to understand a product is to try to build it. Prototyping is an essential process to get a feel for the shape and edges of a product, to begin to estimate the work involved and to provide something you can quickly test with real users. This is a vital part of a process often known as ‘product discovery’: We built alpha.gov.uk as a prototype of what would later become the single domain www.gov.uk. It was built quickly without much concern to scalability, resilience, or any of the other considerations of a ‘real’ product, because none of those matter unless the core concept is sound. That allowed us to get feedback early and also understand some of the trickier concepts we would have to grapple with, like the fuzzy lines between different audiences, the operational processes that would be required, and so on. Prototyping can start on paper with sketches. Hand-drawn sketches of what a service might involve are a good way to begin thinking things through. We encourage everyone to get to running code as quickly as possible. It’s only when you start working in the same medium your users will be using (for online services that’s generally a web browser, but it may also be via an API (application programming interface)) that you can really understand the experience you need to provide. The smart answer format for GOV.UK began as a series of paper sketches refined over a week by a small team. That process gave them a good sense of the boundaries of the problem they were trying to solve. As quickly as possible we prototyped the format using HTML and JavaScript so that it could be experienced in a web browser. That revealed more constraints, such as the fact that users might expect to be able to go back and amend an answer without realising that would change their whole journey through the format. This allowed us to quickly adjust the user interface to be clearer for its users before we started the work of building out the full system. Running code also forces you to think about your integrations with other services and how they might work – do you need to send email? integrate with an existing database? and so on. A prototype will rarely actually include these integrations but having a clear picture of them is vital if you’re going to understand the real effort involved in building and operating the service.","description":"The best way to understand a product is to try to build it. Prototyping is an essential process to get a feel for the shape and edges of a product, to begin to estimate the work involved and to provide something you can quickly test with real users.","link":"/service-manual/user-centred-design/working-with-prototypes.html"},{"_type":"manual","organisations":["government-digital-service"],"title":"Government Service Design Manual","description":"All new digital services from the government must meet the Digital by Default Service Standard","link":"/service-manual"}]
